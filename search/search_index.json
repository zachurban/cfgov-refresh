{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to cfgov-refresh \u00b6 This is the documentation for the cfgov-refresh project, a redesign of the www.consumerfinance.gov website. It is organized thematically in order to create a central repository for all information pertaining to cfgov-refresh. Disclaimer \u00b6 This project is a work in progress. Nothing presented in this repo\u2014whether in the source code, issue tracker, or wiki\u2014is a final product unless it is marked as such or appears on www.consumerfinance.gov . In-progress updates may appear on beta.consumerfinance.gov . Technology stack \u00b6 The standard technology stack for development of cfgov-refresh within the CFPB consists of the following base: macOS Homebrew - package manager for installing system software on OSX Python 2.7 and pip (Python package manager) Jinja2 templates for front-end rendering. See requirements/libraries.txt for version. Wagtail CMS for content administration. See requirements/wagtail.txt for version. PostgreSQL 10.5 is the database we use in production and locally. Psycopg is the Python library that lets Python talk to Postgres. See requirements/postgres.txt for current version. Additional dependencies, listed below Additional dependencies \u00b6 Elasticsearch : Used for full-text search capabilities and content indexing. Node 8 and yarn : Used for downloading and managing front-end dependencies and assets. Front-end dependencies are listed in the project's package.json file. Gulp 4 for running tasks, including compiling front-end assets and running acceptance and unit tests. virtualenv virtualenvwrapper Versions \u00b6 Versions for most front-end packages are kept updated in the project's package.json file. Versions for back-end software including Django, Wagtail, Jinja, etc. are kept in the project's requirements files: https://github.com/cfpb/cfgov-refresh/tree/master/requirements base.txt : shortcut for django.txt + wagtail.txt + libraries.txt deployment.txt : requirements for deployment, includes base.txt and postgres.txt and a New Relic library which we don't install anywhere else. django.txt : specifies the Django version. The file is used when running the site, but by having it separate we can test against other versions of Django by excluding this file. libraries.txt : Python libraries. local.txt : includes base.txt and postgres.txt and some useful libraries when developing locally. docs.txt : requirements to build the cfgov-refresh docs. optional-public.txt : cfgov-refresh satellite apps. Should/could be moved into libraries.txt . postgres.txt : requirements to connect Django to Postgres. scripts.txt : Requirements for running certain jobs on Jenkins, so scripts can run in Jenkins without having to install all the other requirements. test.txt : requirements for running Python tests. travis.txt : extra requirements for Travis. Should/could be moved to explicitly listed in the .travis.yml file? wagtail.txt : specifies Wagtail version. In its own file to make it easier to test multiple versions, same as with django.txt .","title":"Introduction"},{"location":"#introduction-to-cfgov-refresh","text":"This is the documentation for the cfgov-refresh project, a redesign of the www.consumerfinance.gov website. It is organized thematically in order to create a central repository for all information pertaining to cfgov-refresh.","title":"Introduction to cfgov-refresh"},{"location":"#disclaimer","text":"This project is a work in progress. Nothing presented in this repo\u2014whether in the source code, issue tracker, or wiki\u2014is a final product unless it is marked as such or appears on www.consumerfinance.gov . In-progress updates may appear on beta.consumerfinance.gov .","title":"Disclaimer"},{"location":"#technology-stack","text":"The standard technology stack for development of cfgov-refresh within the CFPB consists of the following base: macOS Homebrew - package manager for installing system software on OSX Python 2.7 and pip (Python package manager) Jinja2 templates for front-end rendering. See requirements/libraries.txt for version. Wagtail CMS for content administration. See requirements/wagtail.txt for version. PostgreSQL 10.5 is the database we use in production and locally. Psycopg is the Python library that lets Python talk to Postgres. See requirements/postgres.txt for current version. Additional dependencies, listed below","title":"Technology stack"},{"location":"#additional-dependencies","text":"Elasticsearch : Used for full-text search capabilities and content indexing. Node 8 and yarn : Used for downloading and managing front-end dependencies and assets. Front-end dependencies are listed in the project's package.json file. Gulp 4 for running tasks, including compiling front-end assets and running acceptance and unit tests. virtualenv virtualenvwrapper","title":"Additional dependencies"},{"location":"#versions","text":"Versions for most front-end packages are kept updated in the project's package.json file. Versions for back-end software including Django, Wagtail, Jinja, etc. are kept in the project's requirements files: https://github.com/cfpb/cfgov-refresh/tree/master/requirements base.txt : shortcut for django.txt + wagtail.txt + libraries.txt deployment.txt : requirements for deployment, includes base.txt and postgres.txt and a New Relic library which we don't install anywhere else. django.txt : specifies the Django version. The file is used when running the site, but by having it separate we can test against other versions of Django by excluding this file. libraries.txt : Python libraries. local.txt : includes base.txt and postgres.txt and some useful libraries when developing locally. docs.txt : requirements to build the cfgov-refresh docs. optional-public.txt : cfgov-refresh satellite apps. Should/could be moved into libraries.txt . postgres.txt : requirements to connect Django to Postgres. scripts.txt : Requirements for running certain jobs on Jenkins, so scripts can run in Jenkins without having to install all the other requirements. test.txt : requirements for running Python tests. travis.txt : extra requirements for Travis. Should/could be moved to explicitly listed in the .travis.yml file? wagtail.txt : specifies Wagtail version. In its own file to make it easier to test multiple versions, same as with django.txt .","title":"Versions"},{"location":"ask-cfpb/","text":"Ask CFPB API \u00b6 This API provides search access to the English and Spanish content behind Ask CFPB and Obtener respuestas . The financial topics covered include: Auto loans Bank accounts and services Credit cards Credit reports and scores Debt collection Families and money Money transfers Mortgages Payday loans Prepaid cards Student loans Usage \u00b6 The API is a read-only resource that delivers search results in json format. Requests follow this pattern: https://www.consumerfinance.gov/ask-cfpb/search/json/?q=[SEARCH TERMS] The json response will includes a list of results, each with a question, an answer, and a URL for the related CFPB page. If no results are found, the \"suggestion\" field will offer a more promising search term if one can be found. The payload for the search term \"tuition\" would look like this, but with more result entries: { query: tuition , suggestion: null, result_query: tuition , results: [ { url: https://www.consumerfinance.gov/ask-cfpb/what-is-a-tuition-payment-plan-en-563/ , text: Tuition payment plans, also called tuition installment plans, are short-term (12 months or less) payment plans that split your college bills into equal monthly payments. Tuition installment plans can be an alternative to student loans if you can afford to pay tuition, just not in a lump sum at the start of the semester or quarter. These payment plans do not generally charge interest, but they may have up-front fees. What is a tuition payment plan? , question: What is a tuition payment plan? } ] } Spanish content \u00b6 For questions and answers in Spanish, requests should follow this pattern: https://www.consumerfinance.gov/es/obtener-respuestas/buscar/json/?q=[SPANISH SEARCH TERMS] The payload for the Spanish search term \"vehiculo\" would look like this, but with more result entries: { query: vehiculo , suggestion: null, result_query: vehiculo , results: [ { url: https://www.consumerfinance.gov/es/obtener-respuestas/como-puedo-averiguar-el-significado-de-los-terminos-de-mi-contrato-de-leasing-es-2047/ , text: Bajo la Ley de Arrendamientos del Consumidor (CLA, por sus siglas en ingl\u00e9s), la persona o compa\u00f1\u00eda de quien usted hace el leasing de un veh\u00edculo, conocida como el arrendador , deber\u00e1 informar por escrito ciertos costos y plazos si el leasing es de m\u00e1s de cuatro meses y si cumple con otros requisitos. La mayor\u00eda de los arrendamientos de veh\u00edculos est\u00e1 sujeta a la CLA. Los siguientes materiales le pueden ayudar a entender los t\u00e9rminos de su contrato de leasing. En el sitio web Comprenda c\u00f3mo funciona la financiaci\u00f3n de veh\u00edculos de la Comisi\u00f3n Federal de Comercio se ofrece la siguiente informaci\u00f3n en espa\u00f1ol: Antes de comprar un veh\u00edculo o hacer un leasing \u00bfDeber\u00eda hacer un leasing para un veh\u00edculo? Glosario de t\u00e9rminos espec\u00edficos M\u00e1s informaci\u00f3n en espa\u00f1ol de GobiernoUSA.gov: Consejos para comprar un auto usado: Arrendamiento con derecho a compra o \u201cleasing\u201d Bajo la Ley de Arrendamientos del Consumidor (CLA, por sus siglas en ingles), la persona o compania de quien usted hace el leasing de un vehiculo, conocida como el arrendador , debera informar por escrito ciertos costos y plazos si el leasing es de mas de cuatro meses y si cumple con otros requisitos. La mayoria de los arrendamientos de vehiculos esta sujeta a la CLA. Los siguientes materiales le pueden ayudar a entender los terminos de su contrato de leasing. En el sitio web Comprenda como funciona la financiacion de vehiculos de la Comision Federal de Comercio se ofrece la siguiente informacion en espanol: Antes de comprar un vehiculo o hacer un leasing Deberia hacer un leasing para un vehiculo? Glosario de terminos especificos Mas informacion en espanol de GobiernoUSA.gov: Consejos para comprar un auto usado: Arrendamiento con derecho a compra o leasing \u00bfC\u00f3mo puedo averiguar el significado de los t\u00e9rminos de mi contrato de leasing? Como puedo averiguar el significado de los terminos de mi contrato de leasing? , question: \u00bfC\u00f3mo puedo averiguar el significado de los t\u00e9rminos de mi contrato de leasing? } ] }","title":"Ask CFPB"},{"location":"ask-cfpb/#ask-cfpb-api","text":"This API provides search access to the English and Spanish content behind Ask CFPB and Obtener respuestas . The financial topics covered include: Auto loans Bank accounts and services Credit cards Credit reports and scores Debt collection Families and money Money transfers Mortgages Payday loans Prepaid cards Student loans","title":"Ask CFPB API"},{"location":"ask-cfpb/#usage","text":"The API is a read-only resource that delivers search results in json format. Requests follow this pattern: https://www.consumerfinance.gov/ask-cfpb/search/json/?q=[SEARCH TERMS] The json response will includes a list of results, each with a question, an answer, and a URL for the related CFPB page. If no results are found, the \"suggestion\" field will offer a more promising search term if one can be found. The payload for the search term \"tuition\" would look like this, but with more result entries: { query: tuition , suggestion: null, result_query: tuition , results: [ { url: https://www.consumerfinance.gov/ask-cfpb/what-is-a-tuition-payment-plan-en-563/ , text: Tuition payment plans, also called tuition installment plans, are short-term (12 months or less) payment plans that split your college bills into equal monthly payments. Tuition installment plans can be an alternative to student loans if you can afford to pay tuition, just not in a lump sum at the start of the semester or quarter. These payment plans do not generally charge interest, but they may have up-front fees. What is a tuition payment plan? , question: What is a tuition payment plan? } ] }","title":"Usage"},{"location":"ask-cfpb/#spanish-content","text":"For questions and answers in Spanish, requests should follow this pattern: https://www.consumerfinance.gov/es/obtener-respuestas/buscar/json/?q=[SPANISH SEARCH TERMS] The payload for the Spanish search term \"vehiculo\" would look like this, but with more result entries: { query: vehiculo , suggestion: null, result_query: vehiculo , results: [ { url: https://www.consumerfinance.gov/es/obtener-respuestas/como-puedo-averiguar-el-significado-de-los-terminos-de-mi-contrato-de-leasing-es-2047/ , text: Bajo la Ley de Arrendamientos del Consumidor (CLA, por sus siglas en ingl\u00e9s), la persona o compa\u00f1\u00eda de quien usted hace el leasing de un veh\u00edculo, conocida como el arrendador , deber\u00e1 informar por escrito ciertos costos y plazos si el leasing es de m\u00e1s de cuatro meses y si cumple con otros requisitos. La mayor\u00eda de los arrendamientos de veh\u00edculos est\u00e1 sujeta a la CLA. Los siguientes materiales le pueden ayudar a entender los t\u00e9rminos de su contrato de leasing. En el sitio web Comprenda c\u00f3mo funciona la financiaci\u00f3n de veh\u00edculos de la Comisi\u00f3n Federal de Comercio se ofrece la siguiente informaci\u00f3n en espa\u00f1ol: Antes de comprar un veh\u00edculo o hacer un leasing \u00bfDeber\u00eda hacer un leasing para un veh\u00edculo? Glosario de t\u00e9rminos espec\u00edficos M\u00e1s informaci\u00f3n en espa\u00f1ol de GobiernoUSA.gov: Consejos para comprar un auto usado: Arrendamiento con derecho a compra o \u201cleasing\u201d Bajo la Ley de Arrendamientos del Consumidor (CLA, por sus siglas en ingles), la persona o compania de quien usted hace el leasing de un vehiculo, conocida como el arrendador , debera informar por escrito ciertos costos y plazos si el leasing es de mas de cuatro meses y si cumple con otros requisitos. La mayoria de los arrendamientos de vehiculos esta sujeta a la CLA. Los siguientes materiales le pueden ayudar a entender los terminos de su contrato de leasing. En el sitio web Comprenda como funciona la financiacion de vehiculos de la Comision Federal de Comercio se ofrece la siguiente informacion en espanol: Antes de comprar un vehiculo o hacer un leasing Deberia hacer un leasing para un vehiculo? Glosario de terminos especificos Mas informacion en espanol de GobiernoUSA.gov: Consejos para comprar un auto usado: Arrendamiento con derecho a compra o leasing \u00bfC\u00f3mo puedo averiguar el significado de los t\u00e9rminos de mi contrato de leasing? Como puedo averiguar el significado de los terminos de mi contrato de leasing? , question: \u00bfC\u00f3mo puedo averiguar el significado de los t\u00e9rminos de mi contrato de leasing? } ] }","title":"Spanish content"},{"location":"atomic-structure/","text":"Notes on Atomic Design \u00b6 Our components employ the concept of atomic design, meaning that we break them down into atoms, molecules, and organisms, each successive level being more complex than the previous. (We do not currently use the template or page concepts as described in Brad Frost's seminal article introducing atomic design ). Our components are composed (on the front-end) of HTML, Less, and JavaScript. If a component doesn\u2019t have user interactions or require styling, then it won\u2019t have an associated JS and/or Less file. Components that are available for adding to a Wagtail page also require some Python programming\u2014see the creating and editing components page for details. We compose our atomic components as follows: Atoms \u00b6 The smallest kind of component. May not contain any other components. Prefixed with a- in class names. HTML \u00b6 div class= a-tag Tag label {{ svg_icon('remove') }} /div Less \u00b6 .a-tag { cursor: default; display: inline-block; padding: 5px 10px; \u2026 } JavaScript \u00b6 None of our atoms require any JavaScript at this time. Molecules \u00b6 The medium-sized component. May contain atoms. Prefixed with m- in class names. HTML \u00b6 div class= m-notification m-notification__visible m-notification__error data-js-hook= state_atomic_init {{ svg_icon('error') }} div class= m-notification_content role= alert div class= h4 m-notification_message Page not found. /div /div /div Less \u00b6 .m-notification { display: none; position: relative; padding: @notification-padding__px; \u2026 } JavaScript \u00b6 function Notification( element ) { const BASE_CLASS = 'm-notification'; // Constants for the state of this Notification. const SUCCESS = 'success'; const WARNING = 'warning'; const ERROR = 'error'; // Constants for the Notification modifiers. const MODIFIER_VISIBLE = BASE_CLASS + '__visible'; const _dom = atomicHelpers.checkDom( element, BASE_CLASS ); const _contentDom = _dom.querySelector( '.' + BASE_CLASS + '_content' ); \u2026 } The Notification molecule can be instantiated by adding the following to your project's JavaScript code: const notification = new Notification( _dom ); notification.init(); Organisms \u00b6 The largest component. May contain atoms, molecules, or (if no other solution is viable) other organisms. Prefixed with o- in class names. HTML \u00b6 div class= o-expandable o-expandable__borders o-expandable__midtone o-expandable__expanded data-js-hook= state_atomic_init button class= o-expandable_target aria-pressed= true div class= o-expandable_header \u2026 Less \u00b6 .o-expandable { position: relative; _target { padding: 0; border: 0; \u2026 } \u2026 } JavaScript \u00b6 function Expandable( element ) { const BASE_CLASS = 'o-expandable'; // Bitwise flags for the state of this Expandable. const COLLAPSED = 0; const COLLAPSING = 1; const EXPANDING = 2; const EXPANDED = 3; // The Expandable element will directly be the Expandable // when used in an ExpandableGroup, otherwise it can be the parent container. const _dom = atomicHelpers.checkDom( element, BASE_CLASS ); const _target = _dom.querySelector( '.' + BASE_CLASS + '_target' ); const _content = _dom.querySelector( '.' + BASE_CLASS + '_content' ); \u2026 } The Expandable organism can be instantiated by adding the following to your project's JavaScript code: const expandable = new Expandable( _dom.querySelector( '.o-expandable' ) ); expandable.init( _expandable.EXPANDED ); or const atomicHelpers = require( '../../modules/util/atomic-helpers' ); const Expandable = require( '../../organisms/Expandable' ); atomicHelpers.instantiateAll( '.o-expandable', Expandable ); Folder structure \u00b6 Our atomic components are separated and named based on asset type. HTML, Less, and JavaScript for each component are in separate directories. HTML \u00b6 cfgov-refresh/cfgov/jinja2/v1/_includes/atoms/ cfgov-refresh/cfgov/jinja2/v1/_includes/molecules/ cfgov-refresh/cfgov/jinja2/v1/_includes/organisms/ Note Some of our foundational components get their Less and JavaScript from Capital Framework , but the HTML for their Wagtail block templates is stored in the above folders. CSS \u00b6 cfgov-refresh/cfgov/unprocessed/css/atoms/ cfgov-refresh/cfgov/unprocessed/css/molecules/ cfgov-refresh/cfgov/unprocessed/css/organisms/ JavaScript \u00b6 cfgov-refresh/cfgov/unprocessed/js/molecules/ cfgov-refresh/cfgov/unprocessed/js/organisms/ Tests \u00b6 cfgov-refresh/test/unit_tests/js/molecules/ cfgov-refresh/test/unit_tests/js/organisms/ JavaScript architecture \u00b6 JavaScript components are built to be rendered on the server and then enhanced via JavaScript on the client. The basic interface for the components is as follows: function AtomicComponent( domElement ) { // Ensure the passed in Element is in the DOM. // Query and store references to sub-elements. // Instantiate child atomic components. // Bind necessary events for referenced DOM elements. // Perform other initialization related tasks. this.init = function init(){} // General teardown function // We don't remove the element from the DOM so // we need to unbind the events. this.destroy = function destroy(){} } We generally favor composition over inheritance. You can get more information by reading the following: A Simple Challenge to Classical Inheritance Fans Composition over Inheritance (YouTube) Component build pipeline \u00b6 Gulp \u00b6 Gulp is used as a task automation tool. Tasks include compiling CSS, creating a standard Webpack workflow for bundling scripts, minifying code, linting, running unit tests, and more . Webpack \u00b6 Wepback is used as a module bundler, although it's capable of more. We create page, global, and component-specific bundles. The configuration for the bundles is contained in config/webpack-config.js . An explanation for the usage of each bundle is contained in gulp/tasks/scripts.js . Routes \u00b6 Routes are used to serve JavaScript bundles to the browser based on the requested URL or Wagtail page's Media definition. This happens via code contained in base.html . This file serves as the base HTML template for serving Wagtail pages. Wagtail page Media class \u00b6 Each atomic component has a Media class that lists the JavaScript files that should be loaded via base.html . When a page is requested via the browser, code contained in base.html will loop all atomic components for the requested page and load the appropriate atomic JavaScript bundles. Here is an example of the Media class on a component, the EmailSignUp organism : class Media: js = ['email-signup.js'] This will load the email-signup.js script on any page that includes the EmailSignUp organism in one of its StreamFields.","title":"Atomic Structure and Design"},{"location":"atomic-structure/#notes-on-atomic-design","text":"Our components employ the concept of atomic design, meaning that we break them down into atoms, molecules, and organisms, each successive level being more complex than the previous. (We do not currently use the template or page concepts as described in Brad Frost's seminal article introducing atomic design ). Our components are composed (on the front-end) of HTML, Less, and JavaScript. If a component doesn\u2019t have user interactions or require styling, then it won\u2019t have an associated JS and/or Less file. Components that are available for adding to a Wagtail page also require some Python programming\u2014see the creating and editing components page for details. We compose our atomic components as follows:","title":"Notes on Atomic Design"},{"location":"atomic-structure/#atoms","text":"The smallest kind of component. May not contain any other components. Prefixed with a- in class names.","title":"Atoms"},{"location":"atomic-structure/#html","text":"div class= a-tag Tag label {{ svg_icon('remove') }} /div","title":"HTML"},{"location":"atomic-structure/#less","text":".a-tag { cursor: default; display: inline-block; padding: 5px 10px; \u2026 }","title":"Less"},{"location":"atomic-structure/#javascript","text":"None of our atoms require any JavaScript at this time.","title":"JavaScript"},{"location":"atomic-structure/#molecules","text":"The medium-sized component. May contain atoms. Prefixed with m- in class names.","title":"Molecules"},{"location":"atomic-structure/#html_1","text":"div class= m-notification m-notification__visible m-notification__error data-js-hook= state_atomic_init {{ svg_icon('error') }} div class= m-notification_content role= alert div class= h4 m-notification_message Page not found. /div /div /div","title":"HTML"},{"location":"atomic-structure/#less_1","text":".m-notification { display: none; position: relative; padding: @notification-padding__px; \u2026 }","title":"Less"},{"location":"atomic-structure/#javascript_1","text":"function Notification( element ) { const BASE_CLASS = 'm-notification'; // Constants for the state of this Notification. const SUCCESS = 'success'; const WARNING = 'warning'; const ERROR = 'error'; // Constants for the Notification modifiers. const MODIFIER_VISIBLE = BASE_CLASS + '__visible'; const _dom = atomicHelpers.checkDom( element, BASE_CLASS ); const _contentDom = _dom.querySelector( '.' + BASE_CLASS + '_content' ); \u2026 } The Notification molecule can be instantiated by adding the following to your project's JavaScript code: const notification = new Notification( _dom ); notification.init();","title":"JavaScript"},{"location":"atomic-structure/#organisms","text":"The largest component. May contain atoms, molecules, or (if no other solution is viable) other organisms. Prefixed with o- in class names.","title":"Organisms"},{"location":"atomic-structure/#html_2","text":"div class= o-expandable o-expandable__borders o-expandable__midtone o-expandable__expanded data-js-hook= state_atomic_init button class= o-expandable_target aria-pressed= true div class= o-expandable_header \u2026","title":"HTML"},{"location":"atomic-structure/#less_2","text":".o-expandable { position: relative; _target { padding: 0; border: 0; \u2026 } \u2026 }","title":"Less"},{"location":"atomic-structure/#javascript_2","text":"function Expandable( element ) { const BASE_CLASS = 'o-expandable'; // Bitwise flags for the state of this Expandable. const COLLAPSED = 0; const COLLAPSING = 1; const EXPANDING = 2; const EXPANDED = 3; // The Expandable element will directly be the Expandable // when used in an ExpandableGroup, otherwise it can be the parent container. const _dom = atomicHelpers.checkDom( element, BASE_CLASS ); const _target = _dom.querySelector( '.' + BASE_CLASS + '_target' ); const _content = _dom.querySelector( '.' + BASE_CLASS + '_content' ); \u2026 } The Expandable organism can be instantiated by adding the following to your project's JavaScript code: const expandable = new Expandable( _dom.querySelector( '.o-expandable' ) ); expandable.init( _expandable.EXPANDED ); or const atomicHelpers = require( '../../modules/util/atomic-helpers' ); const Expandable = require( '../../organisms/Expandable' ); atomicHelpers.instantiateAll( '.o-expandable', Expandable );","title":"JavaScript"},{"location":"atomic-structure/#folder-structure","text":"Our atomic components are separated and named based on asset type. HTML, Less, and JavaScript for each component are in separate directories.","title":"Folder structure"},{"location":"atomic-structure/#html_3","text":"cfgov-refresh/cfgov/jinja2/v1/_includes/atoms/ cfgov-refresh/cfgov/jinja2/v1/_includes/molecules/ cfgov-refresh/cfgov/jinja2/v1/_includes/organisms/ Note Some of our foundational components get their Less and JavaScript from Capital Framework , but the HTML for their Wagtail block templates is stored in the above folders.","title":"HTML"},{"location":"atomic-structure/#css","text":"cfgov-refresh/cfgov/unprocessed/css/atoms/ cfgov-refresh/cfgov/unprocessed/css/molecules/ cfgov-refresh/cfgov/unprocessed/css/organisms/","title":"CSS"},{"location":"atomic-structure/#javascript_3","text":"cfgov-refresh/cfgov/unprocessed/js/molecules/ cfgov-refresh/cfgov/unprocessed/js/organisms/","title":"JavaScript"},{"location":"atomic-structure/#tests","text":"cfgov-refresh/test/unit_tests/js/molecules/ cfgov-refresh/test/unit_tests/js/organisms/","title":"Tests"},{"location":"atomic-structure/#javascript-architecture","text":"JavaScript components are built to be rendered on the server and then enhanced via JavaScript on the client. The basic interface for the components is as follows: function AtomicComponent( domElement ) { // Ensure the passed in Element is in the DOM. // Query and store references to sub-elements. // Instantiate child atomic components. // Bind necessary events for referenced DOM elements. // Perform other initialization related tasks. this.init = function init(){} // General teardown function // We don't remove the element from the DOM so // we need to unbind the events. this.destroy = function destroy(){} } We generally favor composition over inheritance. You can get more information by reading the following: A Simple Challenge to Classical Inheritance Fans Composition over Inheritance (YouTube)","title":"JavaScript architecture"},{"location":"atomic-structure/#component-build-pipeline","text":"","title":"Component build pipeline"},{"location":"atomic-structure/#gulp","text":"Gulp is used as a task automation tool. Tasks include compiling CSS, creating a standard Webpack workflow for bundling scripts, minifying code, linting, running unit tests, and more .","title":"Gulp"},{"location":"atomic-structure/#webpack","text":"Wepback is used as a module bundler, although it's capable of more. We create page, global, and component-specific bundles. The configuration for the bundles is contained in config/webpack-config.js . An explanation for the usage of each bundle is contained in gulp/tasks/scripts.js .","title":"Webpack"},{"location":"atomic-structure/#routes","text":"Routes are used to serve JavaScript bundles to the browser based on the requested URL or Wagtail page's Media definition. This happens via code contained in base.html . This file serves as the base HTML template for serving Wagtail pages.","title":"Routes"},{"location":"atomic-structure/#wagtail-page-media-class","text":"Each atomic component has a Media class that lists the JavaScript files that should be loaded via base.html . When a page is requested via the browser, code contained in base.html will loop all atomic components for the requested page and load the appropriate atomic JavaScript bundles. Here is an example of the Media class on a component, the EmailSignUp organism : class Media: js = ['email-signup.js'] This will load the email-signup.js script on any page that includes the EmailSignUp organism in one of its StreamFields.","title":"Wagtail page Media class"},{"location":"branching-merging/","text":"Branching and merging \u00b6 Branches should be named descriptively, preferably in some way that indicates whether they are short-lived feature branches or longer-lived development branches. Short-lived feature branches should be deleted once they are merged into master. All pull requests to merge into master must be reviewed by at least one member of the cf.gov platform team. The cf.gov platform team will ensure that these reviews happen in a timely manner. To ensure timely code reviews, please tag all PRs to master with @cfpb/cfgov-backends and @cfpb/cfgov-frontends as appropriate. When reviewing pull requests, it is important to distinguish between explicit blockers and things that can be addressed in the future or would be nice to have. The latter two can be indicated with 'TODO'. This is best as a simple top-level post after review to summarize the review. The cfgov-refresh repository makes use of automated testing and linting to ensure the quality, consistency, and readability of the codebase. All pull requests to master must pass all automated tests and must not reduce the code coverage of the codebase. It is the responsibility of the submitter to ensure that the tests pass. Pull requests that are not to master must use GitHub labels in such a way that individuals who are responsible for reviewing those pull requests can easily find them. Pull requests that are works-in-progress must be clearly labeled as such. Generally, teams working on cf.gov projects should create and collaborate on feature branches, with frequent merges back to master. Teams are responsible for governing their own branches and forks, these guidelines apply to master.","title":"Branching and Merging"},{"location":"branching-merging/#branching-and-merging","text":"Branches should be named descriptively, preferably in some way that indicates whether they are short-lived feature branches or longer-lived development branches. Short-lived feature branches should be deleted once they are merged into master. All pull requests to merge into master must be reviewed by at least one member of the cf.gov platform team. The cf.gov platform team will ensure that these reviews happen in a timely manner. To ensure timely code reviews, please tag all PRs to master with @cfpb/cfgov-backends and @cfpb/cfgov-frontends as appropriate. When reviewing pull requests, it is important to distinguish between explicit blockers and things that can be addressed in the future or would be nice to have. The latter two can be indicated with 'TODO'. This is best as a simple top-level post after review to summarize the review. The cfgov-refresh repository makes use of automated testing and linting to ensure the quality, consistency, and readability of the codebase. All pull requests to master must pass all automated tests and must not reduce the code coverage of the codebase. It is the responsibility of the submitter to ensure that the tests pass. Pull requests that are not to master must use GitHub labels in such a way that individuals who are responsible for reviewing those pull requests can easily find them. Pull requests that are works-in-progress must be clearly labeled as such. Generally, teams working on cf.gov projects should create and collaborate on feature branches, with frequent merges back to master. Teams are responsible for governing their own branches and forks, these guidelines apply to master.","title":"Branching and merging"},{"location":"browser-acceptance-tests/","text":"Browser/Acceptance Tests \u00b6 Quick start: \u00b6 To run browser tests, open a new Terminal window or tab and change to the project directory, then tell gulp to start the tests: gulp build gulp test:acceptance ( tox -e acceptance can be run as well ) There are several options you can pass to run a particular suite of tests, to run a particular list of features, and/or to run it in \"fast\" mode: gulp test:acceptance --suite=wagtail-admin ( runs just the wagtail-admin suite ) gulp test:acceptance --specs=multiselect.feature ( runs just the multiselect feature ) gulp test:acceptance --tags=@mobile ( runs all scenarios tagged with @mobile ) gulp test:acceptance --recreate ( runs the tests and recreates the virtual environment ) The same options can be used with tox (--omitted): tox -e acceptance suite=wagtail-admin tox -e acceptance specs=multiselect.feature tox -e acceptance tags=@mobile These tests will run on their own server; you do not need to be running your development server. Cucumber - tool for running automated tests written in plain language \u00b6 Below are some suggested standards for Cucumber Feature files: Table copied from https://saucelabs.com/blog/write-great-cucumber-tests by Greg Sypolt, with moderate modifications Feature Files Every *.feature file consists in a single feature, focused on the business value. Gherkin Feature:Title (one line describing the story) Narrative Description: As a [role], I want [feature], so that I [benefit] Scenario: Title (acceptance criteria of user story) Given [context] And [some more context]... When [event] Then [outcome] And [another outcome]... Scenario:... Given, When, and Then Statements There might be some confusion surrounding where to put the verification step in the Given, When, Then sequence. Each statement has a purpose. Given is the pre-condition to put the system into a known state before the user starts interacting with the application When describes the key action the user performs Then is observing the expected outcome Just remember the \u2018then\u2019 step is an acceptance criteria of the story. Background The background needs to be used wisely. If you use the same steps at the beginning of all scenarios of a feature, put them into the feature\u2019s background scenario. The background steps are run before each scenario. Background: Given I am logged into Wagtail as an admin And I create a Wagtail Sublanding Page And I open the content menu Scenarios Keep each scenario independent. The scenarios should run independently, without any dependencies on other scenarios. Scenarios should be between 3 to 6 statements, if possible. Scenario Outline If you identify the need to use a scenario outline, take a step back and ask the following question: Is it necessary to repeat this scenario \u2018x\u2019 amount of times just to exercise the different combination of data? In most cases, one time is enough for UI level testing. Declarative Vs Imperative Scenarios The declarative style describes behavior at a higher level, which improves the readability of the feature by abstracting out the implementation details of the application. The imperative style is more verbose but better describes the expected behavior. Either style is acceptable. Example: Declarative Scenario:User logs in Given I am on the homepage When I log in Then I should see a login notification Example: Imperative Scenario: User logs in Given I am on the homepage When I click on the \"Login\" button And I fill in the \"Email\" field with \" \" And I fill in the \"Password\" field with \"secret\" And I click on \"Submit\" Then I should see \"Welcome to the app, John Doe\" Sauce Connect - send tests to the cloud \u00b6 Sauce Labs can be used to run tests remotely in the cloud. Log into https://saucelabs.com/account . Update and uncomment the SAUCE_USERNAME , SAUCE_ACCESS_KEY , and SAUCE_SELENIUM_URL values in your .env file. The access key can be found on the Sauce Labs user settings page . Reload the settings with source .env . Run the tests with gulp test:acceptance --sauce . Monitor progress of the tests on the Sauce Labs dashboard Automated Tests tab. Note If you get the error Error: ENOTFOUND getaddrinfo ENOTFOUND while running a test, it likely means that Sauce Connect is not running. Manual test configuration \u00b6 A number of command-line arguments can be set to test particular configurations: --suite : Choose a particular suite or suites to run. For example, gulp test:acceptance --suite=content or gulp test:acceptance --suite=content,functional . --specs : Choose a particular spec or specs to run. For example, gulp test:acceptance --specs=header.feature , gulp test:acceptance --specs=header.feature,pagination.feature , or gulp test:acceptance --specs=filterable*.feature . If --suite is specified, this argument will be ignored. If neither --suite nor --specs are specified, all specs will be run. --windowSize : Set the window size in pixels in w,h format. For example, gulp test:acceptance --windowSize=900,400 . --browserName : Set the browser to run. For example, gulp test:acceptance --browserName=firefox . --version : Set the browser version to run. For example, gulp test:acceptance --version='44.0' . --platform : Set the OS platform to run. For example, gulp test:acceptance --platform='osx 10.10' . --sauce : Whether to run on Sauce Labs or not. For example, gulp test:acceptance --sauce=false . Tests \u00b6 Tests are organized into suites under the test/browser_tests/cucumber/features directory. Any new tests should be added to an existing suite (e.g. \"default\"), or placed into a new suite directory. All tests start with writing a .feature spec in one of these suites, and then adding corresponding step definitions, found in test/browser_tests/cucumber/step_definitions . Further reading \u00b6 Cucumber features Protractor Select elements on a page Writing Jasmin expectations . Understanding Page Objects Performance testing \u00b6 To audit if the site complies with performance best practices and guidelines, run gulp audit:perf . The audit will run against Google's Lighthouse . Accessibility Testing \u00b6 Run the acceptance tests with an --a11y flag (i.e. gulp test:acceptance --a11y ) to check every webpage for WCAG and Section 508 compliancy using Protractor's accessibility plugin . If you'd like to audit a specific page, use gulp audit:a11y : Enable the environment variable ACHECKER_ID in your .env file. Get a free AChecker API ID for the value. Reload your .env with source ./.env while in the project root directory. Run gulp audit:a11y to run an audit on the homepage. To test a page aside from the homepage, add the --u= path_to_test flag. For example, gulp audit:a11y --u=contact-us or gulp audit:a11y --u=the-bureau/bureau-structure/ . Source code linting \u00b6 The default test task includes linting of the JavaScript source, build, and test files. Use the gulp lint command from the command-line to run the ESLint linter, which checks the JavaScript against the rules configured in .eslintrc . See the ESLint docs for detailed rule descriptions. There are a number of options to the command: gulp lint:build : Lint only the gulp build scripts. gulp lint:test : Lint only the test scripts. gulp lint:scripts : Lint only the project source scripts. --fix : Add this flag (like gulp lint --fix or gulp lint:build --fix ) to auto-fix some errors, where ESLint has support to do so.","title":"Browser/Acceptance Testing"},{"location":"browser-acceptance-tests/#browseracceptance-tests","text":"","title":"Browser/Acceptance Tests"},{"location":"browser-acceptance-tests/#quick-start","text":"To run browser tests, open a new Terminal window or tab and change to the project directory, then tell gulp to start the tests: gulp build gulp test:acceptance ( tox -e acceptance can be run as well ) There are several options you can pass to run a particular suite of tests, to run a particular list of features, and/or to run it in \"fast\" mode: gulp test:acceptance --suite=wagtail-admin ( runs just the wagtail-admin suite ) gulp test:acceptance --specs=multiselect.feature ( runs just the multiselect feature ) gulp test:acceptance --tags=@mobile ( runs all scenarios tagged with @mobile ) gulp test:acceptance --recreate ( runs the tests and recreates the virtual environment ) The same options can be used with tox (--omitted): tox -e acceptance suite=wagtail-admin tox -e acceptance specs=multiselect.feature tox -e acceptance tags=@mobile These tests will run on their own server; you do not need to be running your development server.","title":"Quick start:"},{"location":"browser-acceptance-tests/#cucumber-tool-for-running-automated-tests-written-in-plain-language","text":"Below are some suggested standards for Cucumber Feature files: Table copied from https://saucelabs.com/blog/write-great-cucumber-tests by Greg Sypolt, with moderate modifications Feature Files Every *.feature file consists in a single feature, focused on the business value. Gherkin Feature:Title (one line describing the story) Narrative Description: As a [role], I want [feature], so that I [benefit] Scenario: Title (acceptance criteria of user story) Given [context] And [some more context]... When [event] Then [outcome] And [another outcome]... Scenario:... Given, When, and Then Statements There might be some confusion surrounding where to put the verification step in the Given, When, Then sequence. Each statement has a purpose. Given is the pre-condition to put the system into a known state before the user starts interacting with the application When describes the key action the user performs Then is observing the expected outcome Just remember the \u2018then\u2019 step is an acceptance criteria of the story. Background The background needs to be used wisely. If you use the same steps at the beginning of all scenarios of a feature, put them into the feature\u2019s background scenario. The background steps are run before each scenario. Background: Given I am logged into Wagtail as an admin And I create a Wagtail Sublanding Page And I open the content menu Scenarios Keep each scenario independent. The scenarios should run independently, without any dependencies on other scenarios. Scenarios should be between 3 to 6 statements, if possible. Scenario Outline If you identify the need to use a scenario outline, take a step back and ask the following question: Is it necessary to repeat this scenario \u2018x\u2019 amount of times just to exercise the different combination of data? In most cases, one time is enough for UI level testing. Declarative Vs Imperative Scenarios The declarative style describes behavior at a higher level, which improves the readability of the feature by abstracting out the implementation details of the application. The imperative style is more verbose but better describes the expected behavior. Either style is acceptable. Example: Declarative Scenario:User logs in Given I am on the homepage When I log in Then I should see a login notification Example: Imperative Scenario: User logs in Given I am on the homepage When I click on the \"Login\" button And I fill in the \"Email\" field with \" \" And I fill in the \"Password\" field with \"secret\" And I click on \"Submit\" Then I should see \"Welcome to the app, John Doe\"","title":"Cucumber - tool for running automated tests written in plain language"},{"location":"browser-acceptance-tests/#sauce-connect-send-tests-to-the-cloud","text":"Sauce Labs can be used to run tests remotely in the cloud. Log into https://saucelabs.com/account . Update and uncomment the SAUCE_USERNAME , SAUCE_ACCESS_KEY , and SAUCE_SELENIUM_URL values in your .env file. The access key can be found on the Sauce Labs user settings page . Reload the settings with source .env . Run the tests with gulp test:acceptance --sauce . Monitor progress of the tests on the Sauce Labs dashboard Automated Tests tab. Note If you get the error Error: ENOTFOUND getaddrinfo ENOTFOUND while running a test, it likely means that Sauce Connect is not running.","title":"Sauce Connect - send tests to the cloud"},{"location":"browser-acceptance-tests/#manual-test-configuration","text":"A number of command-line arguments can be set to test particular configurations: --suite : Choose a particular suite or suites to run. For example, gulp test:acceptance --suite=content or gulp test:acceptance --suite=content,functional . --specs : Choose a particular spec or specs to run. For example, gulp test:acceptance --specs=header.feature , gulp test:acceptance --specs=header.feature,pagination.feature , or gulp test:acceptance --specs=filterable*.feature . If --suite is specified, this argument will be ignored. If neither --suite nor --specs are specified, all specs will be run. --windowSize : Set the window size in pixels in w,h format. For example, gulp test:acceptance --windowSize=900,400 . --browserName : Set the browser to run. For example, gulp test:acceptance --browserName=firefox . --version : Set the browser version to run. For example, gulp test:acceptance --version='44.0' . --platform : Set the OS platform to run. For example, gulp test:acceptance --platform='osx 10.10' . --sauce : Whether to run on Sauce Labs or not. For example, gulp test:acceptance --sauce=false .","title":"Manual test configuration"},{"location":"browser-acceptance-tests/#tests","text":"Tests are organized into suites under the test/browser_tests/cucumber/features directory. Any new tests should be added to an existing suite (e.g. \"default\"), or placed into a new suite directory. All tests start with writing a .feature spec in one of these suites, and then adding corresponding step definitions, found in test/browser_tests/cucumber/step_definitions .","title":"Tests"},{"location":"browser-acceptance-tests/#further-reading","text":"Cucumber features Protractor Select elements on a page Writing Jasmin expectations . Understanding Page Objects","title":"Further reading"},{"location":"browser-acceptance-tests/#performance-testing","text":"To audit if the site complies with performance best practices and guidelines, run gulp audit:perf . The audit will run against Google's Lighthouse .","title":"Performance testing"},{"location":"browser-acceptance-tests/#accessibility-testing","text":"Run the acceptance tests with an --a11y flag (i.e. gulp test:acceptance --a11y ) to check every webpage for WCAG and Section 508 compliancy using Protractor's accessibility plugin . If you'd like to audit a specific page, use gulp audit:a11y : Enable the environment variable ACHECKER_ID in your .env file. Get a free AChecker API ID for the value. Reload your .env with source ./.env while in the project root directory. Run gulp audit:a11y to run an audit on the homepage. To test a page aside from the homepage, add the --u= path_to_test flag. For example, gulp audit:a11y --u=contact-us or gulp audit:a11y --u=the-bureau/bureau-structure/ .","title":"Accessibility Testing"},{"location":"browser-acceptance-tests/#source-code-linting","text":"The default test task includes linting of the JavaScript source, build, and test files. Use the gulp lint command from the command-line to run the ESLint linter, which checks the JavaScript against the rules configured in .eslintrc . See the ESLint docs for detailed rule descriptions. There are a number of options to the command: gulp lint:build : Lint only the gulp build scripts. gulp lint:test : Lint only the test scripts. gulp lint:scripts : Lint only the project source scripts. --fix : Add this flag (like gulp lint --fix or gulp lint:build --fix ) to auto-fix some errors, where ESLint has support to do so.","title":"Source code linting"},{"location":"caching/","text":"Caching \u00b6 Akamai \u00b6 We use Akamai , a content delivery network, to cache the entirety of www.consumerfinance.gov (but not our development servers). We invalidate any given page in Wagtail when it is published or unpublished (by hooking up the custom class AkamaiBackend to Wagtail's frontend cache invalidator . By default, we clear the Akamai cache any time we deploy. There are certain pages that do not live in Wagtail or are impacted by changes on another page (imagine our newsroom page that lists titles of other pages) or another process (imagine data from Socrata gets updated) and thus will display outdated content until the page's time to live (TTL) has expired, a deploy has happened, or if someone manually invalidates that page. Our default TTL is 24 hours. Checking the cache state of a URL \u00b6 To get the current cache state of a URL (perhaps to see if that URL has been invalidated), you can use the following curl command to check the X-Cache header: curl -sI -H Pragma: akamai-x-cache-on https://beta.consumerfinance.gov | grep X-Cache It will return something like: X-Cache: TCP_HIT from a23-46-239-53.deploy.akamaitechnologies.com (AkamaiGHost/9.5.4-24580776) (-) The possible cache state values are: TCP_HIT : The object was fresh in cache and object from disk cache. TCP_MISS : The object was not in cache, server fetched object from origin. TCP_REFRESH_HIT : The object was stale in cache and we successfully refreshed with the origin on an If-modified-Since request. TCP_REFRESH_MISS : Object was stale in cache and refresh obtained a new object from origin in response to our IF-Modified-Since request. TCP_REFRESH_FAIL_HIT : Object was stale in cache and we failed on refresh (couldn't reach origin) so we served the stale object. TCP_IMS_HIT : IF-Modified-Since request from client and object was fresh in cache and served. TCP_NEGATIVE_HIT : Object previously returned a \"not found\" (or any other negatively cacheable response) and that cached response was a hit for this new request. TCP_MEM_HIT : Object was on disk and in the memory cache. Server served it without hitting the disk. TCP_DENIED : Denied access to the client for whatever reason. TCP_COOKIE_DENY : Denied access on cookie authentication (if centralized or decentralized authorization feature is being used in config). Django caching \u00b6 Starting in December 2017, we use template fragment caching to cache all or part of a template. It is enabled on our \"post previews\", snippets of a page that we display on results of filterable pages (e.g. our blog page research reports ). It can easily be enabled on other templates. See this PR as an example of the code that would need to be introduced to cache a new fragment. When a page gets published, it will update the post preview cache for that particular page. However, if there are code changes that impact the page's content, or the post preview template itself gets updated, the entire post preview cache will need to be manually cleared. Clearing this particular cache could be an option when deploying, as it is with Akamai, but should not be a default since most deploys wouldn't impact the code in question. Currently, the manual way to do this would be to run the following from a production server's django shell: from django.core.cache import caches caches['post_preview'].clear() To run the application locally with caching for post previews enabled, run ENABLE_POST_PREVIEW_CACHE=1 ./runserver.sh Alternatively, add this variable to your .env if you generally want it enabled locally. Due to the impossibility/difficulty/complexity of caching individual Wagtail blocks (they are not serializable) and invalidating content that does not have some type of post_save hook (e.g. Taggit models), we have started with caching segments that are tied to a Wagtail page (which can be easily invalidated using the page_published Wagtail signal), hence the post previews. With more research or improvements to these third-party libraries, it is possible we could expand Django-level caching to more content.","title":"Caching"},{"location":"caching/#caching","text":"","title":"Caching"},{"location":"caching/#akamai","text":"We use Akamai , a content delivery network, to cache the entirety of www.consumerfinance.gov (but not our development servers). We invalidate any given page in Wagtail when it is published or unpublished (by hooking up the custom class AkamaiBackend to Wagtail's frontend cache invalidator . By default, we clear the Akamai cache any time we deploy. There are certain pages that do not live in Wagtail or are impacted by changes on another page (imagine our newsroom page that lists titles of other pages) or another process (imagine data from Socrata gets updated) and thus will display outdated content until the page's time to live (TTL) has expired, a deploy has happened, or if someone manually invalidates that page. Our default TTL is 24 hours.","title":"Akamai"},{"location":"caching/#checking-the-cache-state-of-a-url","text":"To get the current cache state of a URL (perhaps to see if that URL has been invalidated), you can use the following curl command to check the X-Cache header: curl -sI -H Pragma: akamai-x-cache-on https://beta.consumerfinance.gov | grep X-Cache It will return something like: X-Cache: TCP_HIT from a23-46-239-53.deploy.akamaitechnologies.com (AkamaiGHost/9.5.4-24580776) (-) The possible cache state values are: TCP_HIT : The object was fresh in cache and object from disk cache. TCP_MISS : The object was not in cache, server fetched object from origin. TCP_REFRESH_HIT : The object was stale in cache and we successfully refreshed with the origin on an If-modified-Since request. TCP_REFRESH_MISS : Object was stale in cache and refresh obtained a new object from origin in response to our IF-Modified-Since request. TCP_REFRESH_FAIL_HIT : Object was stale in cache and we failed on refresh (couldn't reach origin) so we served the stale object. TCP_IMS_HIT : IF-Modified-Since request from client and object was fresh in cache and served. TCP_NEGATIVE_HIT : Object previously returned a \"not found\" (or any other negatively cacheable response) and that cached response was a hit for this new request. TCP_MEM_HIT : Object was on disk and in the memory cache. Server served it without hitting the disk. TCP_DENIED : Denied access to the client for whatever reason. TCP_COOKIE_DENY : Denied access on cookie authentication (if centralized or decentralized authorization feature is being used in config).","title":"Checking the cache state of a URL"},{"location":"caching/#django-caching","text":"Starting in December 2017, we use template fragment caching to cache all or part of a template. It is enabled on our \"post previews\", snippets of a page that we display on results of filterable pages (e.g. our blog page research reports ). It can easily be enabled on other templates. See this PR as an example of the code that would need to be introduced to cache a new fragment. When a page gets published, it will update the post preview cache for that particular page. However, if there are code changes that impact the page's content, or the post preview template itself gets updated, the entire post preview cache will need to be manually cleared. Clearing this particular cache could be an option when deploying, as it is with Akamai, but should not be a default since most deploys wouldn't impact the code in question. Currently, the manual way to do this would be to run the following from a production server's django shell: from django.core.cache import caches caches['post_preview'].clear() To run the application locally with caching for post previews enabled, run ENABLE_POST_PREVIEW_CACHE=1 ./runserver.sh Alternatively, add this variable to your .env if you generally want it enabled locally. Due to the impossibility/difficulty/complexity of caching individual Wagtail blocks (they are not serializable) and invalidating content that does not have some type of post_save hook (e.g. Taggit models), we have started with caching segments that are tied to a Wagtail page (which can be easily invalidated using the page_published Wagtail signal), hence the post previews. With more research or improvements to these third-party libraries, it is possible we could expand Django-level caching to more content.","title":"Django caching"},{"location":"consumer-complaint-database/","text":"Consumer Complaint Database API \u00b6 A resource for searching complaints submitted to the CFPB \u00b6 Each week the CFPB sends thousands of consumers\u2019 complaints about financial products and services to companies for response. Those complaints are published on our website after the company responds or after 15 days, whichever comes first. The API allows automation of the same filtering and searching functions offered to website visitors. Detailed documentation for the search API can be found here . Notes \u00b6 The database generally updates daily, and contains certain information for each complaint, including the source of the complaint, the date of submission, and the company the complaint was sent to for response. The database also includes information about the actions taken by the company in response to the complaint, such as, whether the company\u2019s response was timely and how the company responded. If the consumer opts to share it and after we take steps to remove personal information, we publish the consumer\u2019s description of what happened. Companies also have the option to select a public response. Company level information should be considered in context of company size and/or market share. Complaints referred to other regulators, such as complaints about depository institutions with less than $10 billion in assets, are not published in the Consumer Complaint Database.","title":"Consumer Complaints"},{"location":"consumer-complaint-database/#consumer-complaint-database-api","text":"","title":"Consumer Complaint Database API"},{"location":"consumer-complaint-database/#a-resource-for-searching-complaints-submitted-to-the-cfpb","text":"Each week the CFPB sends thousands of consumers\u2019 complaints about financial products and services to companies for response. Those complaints are published on our website after the company responds or after 15 days, whichever comes first. The API allows automation of the same filtering and searching functions offered to website visitors. Detailed documentation for the search API can be found here .","title":"A resource for searching complaints submitted to the CFPB"},{"location":"consumer-complaint-database/#notes","text":"The database generally updates daily, and contains certain information for each complaint, including the source of the complaint, the date of submission, and the company the complaint was sent to for response. The database also includes information about the actions taken by the company in response to the complaint, such as, whether the company\u2019s response was timely and how the company responded. If the consumer opts to share it and after we take steps to remove personal information, we publish the consumer\u2019s description of what happened. Companies also have the option to select a public response. Company level information should be considered in context of company size and/or market share. Complaints referred to other regulators, such as complaints about depository institutions with less than $10 billion in assets, are not published in the Consumer Complaint Database.","title":"Notes"},{"location":"contributing-docs/","text":"Contributing to the Docs \u00b6 Our documentation is written as Markdown files and served via GitHub Pages by MkDocs . Writing the docs \u00b6 As our documentation is written in Markdown, the base Markdown specification is a useful reference. MkDocs also includes some documentation to get you started writing in Markdown . In addition to standard Markdown, our documentation supports the following extensions: Admonitions adds specially called-out text anywhere within the document as notes, warnings, and other types. BetterEm improves the handling of bold and italics. MagicLink provides automatic linking for URLs in the Markdown text. SuperFences makes a number of improvements to standard Markdown code fences. Tilde adds support for creating del /del tags with ~~ . Tables adds support for tables to standard Markdown. When creating new documents, they should be added to the mkdocs.yml file in the appropriate place under nav: to get them to appear in the sidebar navigation. For example: nav: - Introduction: index.md Running the docs locally \u00b6 With Docker \u00b6 When running cfgov-refresh using Docker-compose , this documentation is running by default at http://localhost:8888 . Manually \u00b6 When using the stand-alone installation of cfgov-refresh, you can run these docs with: workon cfgov-refresh pip install -r requirements/docs.txt mkdocs serve -a :8888 Once running, they are accessible at http://localhost:8888 . Deploying the docs to GitHub Pages \u00b6 Every time a PR is merged to master, Travis will build and deploy the documentation to https://cfpb.github.io/cfgov-refresh . See How we use Travis CI for more info. If you would like to deploy to a fork of cfgov-refresh owned by another user you can provide the -r argument: mkdocs gh-deploy -r USER Where USER is the GitHub user. The docs will then be available at https://USER.github.io/cfgov-refresh/ after a short period of time. See the the MkDocs documentation for more information.","title":"Contributing to the Docs"},{"location":"contributing-docs/#contributing-to-the-docs","text":"Our documentation is written as Markdown files and served via GitHub Pages by MkDocs .","title":"Contributing to the Docs"},{"location":"contributing-docs/#writing-the-docs","text":"As our documentation is written in Markdown, the base Markdown specification is a useful reference. MkDocs also includes some documentation to get you started writing in Markdown . In addition to standard Markdown, our documentation supports the following extensions: Admonitions adds specially called-out text anywhere within the document as notes, warnings, and other types. BetterEm improves the handling of bold and italics. MagicLink provides automatic linking for URLs in the Markdown text. SuperFences makes a number of improvements to standard Markdown code fences. Tilde adds support for creating del /del tags with ~~ . Tables adds support for tables to standard Markdown. When creating new documents, they should be added to the mkdocs.yml file in the appropriate place under nav: to get them to appear in the sidebar navigation. For example: nav: - Introduction: index.md","title":"Writing the docs"},{"location":"contributing-docs/#running-the-docs-locally","text":"","title":"Running the docs locally"},{"location":"contributing-docs/#with-docker","text":"When running cfgov-refresh using Docker-compose , this documentation is running by default at http://localhost:8888 .","title":"With Docker"},{"location":"contributing-docs/#manually","text":"When using the stand-alone installation of cfgov-refresh, you can run these docs with: workon cfgov-refresh pip install -r requirements/docs.txt mkdocs serve -a :8888 Once running, they are accessible at http://localhost:8888 .","title":"Manually"},{"location":"contributing-docs/#deploying-the-docs-to-github-pages","text":"Every time a PR is merged to master, Travis will build and deploy the documentation to https://cfpb.github.io/cfgov-refresh . See How we use Travis CI for more info. If you would like to deploy to a fork of cfgov-refresh owned by another user you can provide the -r argument: mkdocs gh-deploy -r USER Where USER is the GitHub user. The docs will then be available at https://USER.github.io/cfgov-refresh/ after a short period of time. See the the MkDocs documentation for more information.","title":"Deploying the docs to GitHub Pages"},{"location":"development-tips/","text":"Development tips \u00b6 TIP: Developing on nested satellite apps \u00b6 Some projects can sit inside cfgov-refresh, but manage their own asset dependencies. These projects have their own package.json and base templates. The structure looks like this: npm modules \u00b6 App's own dependency list is in cfgov/unprocessed/apps/[project namespace]/package.json App's node_modules path is listed in the Travis config https://github.com/cfpb/cfgov-refresh/blob/master/.travis.yml#L10 so that their dependencies will be available when Travis runs. Webpack \u00b6 Apps may include their own webpack-config.js configuration that adjusts how their app-specific assets should be built. This configuration appears in cfgov/unprocessed/apps/[project namespace]/webpack-config.js Browserlist \u00b6 Apps may include a browserlist config file, which is automatically picked up by @babel/preset-env inside the webpack config, if no browsers option is supplied. Adding Images \u00b6 Images should be compressed and optimized before being committed to the repo In order to keep builds fast and reduce dependencies, the front-end build does not contain an image optimization step A suggested workflow for those with Adobe Creative Suite is as follows: Export a full-quality PNG from Adobe Illustrator Reexport that PNG from Adobe Fireworks as an 8-bit PNG Run the 8-bit PNG through ImageOptim Templates \u00b6 Apps use a jinja template that extends the base.html template used by the rest of the site. This template would reside in cfgov/jinja2/v1/[project namespace]/index.html or similar (for example, owning-a-home ). Note A template may support a non-standard browser, like an older IE version, by including the required dependencies, polyfills, etc. in its template's {% block css %} or {% block javascript scoped %} blocks. TIP: Loading satellite apps \u00b6 Some projects fit within the cfgov-refresh architecture, but are not fully incorporated into the project. These are known as \"satellite apps.\" Satellite apps are listed in the optional-public.txt requirements file. In addition to the aforementioned list, HMDA Explorer and Rural or Underserved , have their own installation requirements. If using Docker, follow these guidelines . Otherwise, if not using Docker, follow these guidelines: Build the third-party projects per their directions Stop the web server and return to cfgov-refresh Run pip install -e ../ sibling to load the projects' dependencies Note Do not install the projects directly into the cfgov-refresh directory. Clone and install the projects as siblings to cfgov-refresh , so that they share the same parent directory ( ~/Projects or similar). TIP: Working with the templates \u00b6 Front-End Template/Asset Locations \u00b6 Templates that are served by the Django server: cfgov\\jinja2\\v1 Static assets prior to processing (minifying etc.): cfgov\\unprocessed . Note After running gulp build the site's assets are copied over to cfgov\\static_built , ready to be served by Django. TIP: Debugging site performance \u00b6 When running locally it is possible to enable the Django Debug Toolbar by defining the ENABLE_DEBUG_TOOLBAR environment variable: $ ENABLE_DEBUG_TOOLBAR=1 ./runserver.sh This tool exposes various useful pieces of information about things like HTTP headers, Django settings, SQL queries, and template variables. Note that running with the toolbar on may have an impact on local server performance.","title":"Development Tips"},{"location":"development-tips/#development-tips","text":"","title":"Development tips"},{"location":"development-tips/#tip-developing-on-nested-satellite-apps","text":"Some projects can sit inside cfgov-refresh, but manage their own asset dependencies. These projects have their own package.json and base templates. The structure looks like this:","title":"TIP: Developing on nested satellite apps"},{"location":"development-tips/#npm-modules","text":"App's own dependency list is in cfgov/unprocessed/apps/[project namespace]/package.json App's node_modules path is listed in the Travis config https://github.com/cfpb/cfgov-refresh/blob/master/.travis.yml#L10 so that their dependencies will be available when Travis runs.","title":"npm modules"},{"location":"development-tips/#webpack","text":"Apps may include their own webpack-config.js configuration that adjusts how their app-specific assets should be built. This configuration appears in cfgov/unprocessed/apps/[project namespace]/webpack-config.js","title":"Webpack"},{"location":"development-tips/#browserlist","text":"Apps may include a browserlist config file, which is automatically picked up by @babel/preset-env inside the webpack config, if no browsers option is supplied.","title":"Browserlist"},{"location":"development-tips/#adding-images","text":"Images should be compressed and optimized before being committed to the repo In order to keep builds fast and reduce dependencies, the front-end build does not contain an image optimization step A suggested workflow for those with Adobe Creative Suite is as follows: Export a full-quality PNG from Adobe Illustrator Reexport that PNG from Adobe Fireworks as an 8-bit PNG Run the 8-bit PNG through ImageOptim","title":"Adding Images"},{"location":"development-tips/#templates","text":"Apps use a jinja template that extends the base.html template used by the rest of the site. This template would reside in cfgov/jinja2/v1/[project namespace]/index.html or similar (for example, owning-a-home ). Note A template may support a non-standard browser, like an older IE version, by including the required dependencies, polyfills, etc. in its template's {% block css %} or {% block javascript scoped %} blocks.","title":"Templates"},{"location":"development-tips/#tip-loading-satellite-apps","text":"Some projects fit within the cfgov-refresh architecture, but are not fully incorporated into the project. These are known as \"satellite apps.\" Satellite apps are listed in the optional-public.txt requirements file. In addition to the aforementioned list, HMDA Explorer and Rural or Underserved , have their own installation requirements. If using Docker, follow these guidelines . Otherwise, if not using Docker, follow these guidelines: Build the third-party projects per their directions Stop the web server and return to cfgov-refresh Run pip install -e ../ sibling to load the projects' dependencies Note Do not install the projects directly into the cfgov-refresh directory. Clone and install the projects as siblings to cfgov-refresh , so that they share the same parent directory ( ~/Projects or similar).","title":"TIP: Loading satellite apps"},{"location":"development-tips/#tip-working-with-the-templates","text":"","title":"TIP: Working with the templates"},{"location":"development-tips/#front-end-templateasset-locations","text":"Templates that are served by the Django server: cfgov\\jinja2\\v1 Static assets prior to processing (minifying etc.): cfgov\\unprocessed . Note After running gulp build the site's assets are copied over to cfgov\\static_built , ready to be served by Django.","title":"Front-End Template/Asset Locations"},{"location":"development-tips/#tip-debugging-site-performance","text":"When running locally it is possible to enable the Django Debug Toolbar by defining the ENABLE_DEBUG_TOOLBAR environment variable: $ ENABLE_DEBUG_TOOLBAR=1 ./runserver.sh This tool exposes various useful pieces of information about things like HTTP headers, Django settings, SQL queries, and template variables. Note that running with the toolbar on may have an impact on local server performance.","title":"TIP: Debugging site performance"},{"location":"editing-components/","text":"Creating and Editing Wagtail Components \u00b6 cfgov-refresh implements a number of components that editors can choose from when building a page, for example: Heroes, Expandable Groups, or Info Unit Groups. The CFPB Design Manual describes the design and intended usage of many of these components. In Wagtail parlance, these are called \"StreamField blocks\" * (or just \"blocks\"). We sometimes also refer to them as \"modules\", because we think that the terms \"component\" and \"module\" may be more obvious to non-developers. This page will use the terms somewhat interchangeably. One other important thing to note before we begin: blocks can be nested within other blocks. * If you're going to be doing anything more than making minor updates to existing components, this is highly recommended reading. Table of contents \u00b6 The parts of a Wagtail block The back end The Python class Adding it to a StreamField The front end The HTML template Adding CSS Adding JavaScript How-to guides Creating a new component Adding a field to an existing component Editing a field on an existing component Removing a field from an existing component Creating migrations for StreamField blocks The parts of a Wagtail block \u00b6 Blocks are implemented via several different bits of code: Defining a block's fields and other properties in a Python class Adding the class to a page's StreamField block options Creating an HTML template for rendering the block on a page (Optionally) adding some CSS for styling the block (Optionally) adding some JavaScript for adding advanced behavior Before you dive in further, check out the Notes on Atomic Design page and familiarize yourself with our basic concepts of atoms, molecules, and organisms. The back end \u00b6 The Python class \u00b6 A component's fields and other properties are defined in a Python class, typically a subclass of Wagtail's StructBlock . These classes are located in a number of different files across the repository, but there are two major categories they fall into: Files corresponding to a general-purpose, site-wide atomic component. These files\u2014 atoms.py , molecules.py , and organisms.py \u2014are located in cfgov/v1/atomic_elements . Files that are specific to a particular sub-app, such as regulations3k's blocks.py . There are other places where StreamField block classes are defined (particularly blocks that are only ever used as fields within another block), but these are the two most common locations where top-level Wagtail modules are stored. A simple component class looks like this: class RelatedContent(blocks.StructBlock): # 1 heading = blocks.CharBlock(required=False) # 2 paragraph = blocks.RichTextBlock(required=False) # 3 links = blocks.ListBlock(atoms.Hyperlink()) # 4 class Meta: # 5 icon = 'grip' # 6 label = 'Related content' # 7 template = '_includes/molecules/related-content.html' # 8 There are a few things happening here: The RelatedContent class is a subclass of Wagtail's StructBlock , which allows for the combination of a fixed number of other sub-blocks (see previous comment about blocks being nested within other blocks) into a single unit (what we'd think of as a \"module\" in the Wagtail editor). This one has three sub-blocks (lines 2, 3, and 4). The heading field uses the basic Wagtail CharBlock , which results in a field with a basic single-line text input. The paragraph field uses the basic Wagtail RichTextBlock , which results in a field with a multiline WYSIWYG text input. The links field uses another basic Wagtail block, ListBlock , which is a special type of block that can hold a variable number of some other block (the Hyperlink atom block, in this case). The Meta class defines some properties on the RelatedContent block that are used by the Wagtail admin or in rendering the block. The icon property tells Wagtail what icon to use in the editor for the button you use to add a RelatedContent block to a StreamField. Icon options can be found in the Wagtail style guide when running locally: http://localhost:8000/admin/styleguide/#icons The optional label property overrides the text of that same button; if label is not set, Wagtail will generate one from the name of the block. The template property is a pointer to the HTML template used to render this component. See below for more on templates. This results in a module that looks like this in the Wagtail editor: Note again that what we think of as fields are also blocks, and what we think of as components or modules are a special kind of block, StructBlock , that comprise the sub-blocks that are our fields. There are two common optional things that are also used in component classes: Overriding the default get_context method to pass additional data to the template Adding component-specific JavaScript via the Media class Adding it to a StreamField \u00b6 Components are made available in the page editing interface by adding them to one of a page types's StreamFields. These are usually the first things in a page's class definition. For example, see this snippet from blog_page.py : class BlogPage(AbstractFilterPage): content = StreamField([ ('full_width_text', organisms.FullWidthText()), ('info_unit_group', organisms.InfoUnitGroup()), ('expandable', organisms.Expandable()), ('well', organisms.Well()), ('email_signup', organisms.EmailSignUp()), ('feedback', v1_blocks.Feedback()), ]) \u2026 This sets up a StreamField named content that allows for the insertion of any of those seven listed blocks into it. To make the RelatedContent module (shown above) available to this StreamField, we'd add a new entry to this list following the same format: ('related_content', molecules.RelatedContent()), . Most page types have two StreamFields ( header and content ) in the general content area (the first tab on an editing screen), and most also share a common sidefoot StreamField (so named for the fact that it appears on the right side on some page types, but in the footer on others) on the sidebar tab. Don't forget the migrations! Adding or changing fields on either Python class will always require a new Django schema migration ; additionally, changing field names or types on an existing block will require a Django data migration . See the guide on creating migrations for StreamField blocks for more details. The front end \u00b6 Before diving into the front-end code, a reminder to visit the Notes on Atomic Design page to learn about how we conceive of components in a hierarchy of atoms, molecules, and organisms. The HTML template \u00b6 Frontend rendering of Wagtail StreamField blocks to HTML can be controlled by writing a Django template and associating it with the block type. A custom block definition can specify the template path in its Meta class: from wagtail.core import blocks class PersonBlock(blocks.StructBlock): name = blocks.CharBlock() email = blocks.EmailBlock() class Meta: template = 'myapp/blocks/person_block.html' StreamField block templates are loaded by the Django template loader in the same way that Django page templates are. The specified template path must be loadable by one of the Django template engines configured in settings.TEMPLATES . (This project supports both the standard Django templates backend and the Jinja2 backend , but Jinja2 is more commonly used.) See the Django templates documentation for more details on the search algorithm used to locate a template. Returning to the RelatedContent example, this is what its Jinja2 template looks like (comments excluded): div class= m-related-content {% if value.heading %} header class= m-slug-header h2 class= a-heading {{ value.heading }} /h2 /header {% endif %} {{ value.paragraph | safe }} {% if value.links %} ul class= m-list m-list__links {% for link in value.links %} li class= m-list_item a href= {{ link.url }} class= m-list_link {{ link.text }} /a /li {% endfor %} /ul {% endif %} /div When Wagtail renders a block, it includes the values of its fields in an object named value . Above, you can see where the heading and paragraph fields are output with Jinja2 expression tags . And note how the links field (a ListBlock ) is iterated over, and the values of its Hyperlink child blocks are output. That's about as simple an example as it gets, but block templates can get much more complex when they have lots of child blocks and grandchild blocks. Also, if a block definition has overridden get_context to pass other data into the template (as described at the end of the Python class section above), those context variables can also be output with simple Jinja2 expression tags: {{ context_var }} . Adding CSS \u00b6 If a component needs any custom styling not already provided by Capital Framework or cfgov-refresh, you can add it by creating a new Less file for the component. Note Please be sure that you actually need new Less before creating it. We have a wide array of styles already available in Capital Framework components and here in cfgov-refresh , some of which could perhaps be combined to achieve your desired result. Also be sure that new component designs have gone through our internal approval process before adding them to the project. If you're working on a general-purpose atomic component for site-wide use, this file should live in cfgov/unprocessed/css/ atoms|molecules|organisms / . (Choose the deepest folder according to the atomic rank of the component.) Continuing the RelatedContent example, if it needed its own styles, it would live at cfgov/unprocessed/css/molecules/related-content.less . Newly-created Less files need to be imported into the project's master main.less file, located at cfgov/unprocessed/css/main.less . Please place them in the appropriate section for their atomic rank. Because cfgov-refresh uses main.less to build a single CSS file for almost the entire project, it is not necessary to tell the Python model anything about a component-specific stylesheet (for general-purpose, site-wide components). That is not the case with JavaScript, as we will see in the next section. Note If you're working on a component that belongs to a particular sub-app, its Less file should live in cfgov/unprocessed/ app-name /css/ . Adding JavaScript \u00b6 Each atomic component may optionally be given a Media class that can list one or more JavaScript files that should be loaded when using it. When a page is requested via the browser, code contained in base.html will loop all atomic components for the requested page and load the appropriate atomic JavaScript bundles. Here is how one would add the Media class to our RelatedContent example: class RelatedContent(blocks.StructBlock): \u2026 # see first example on this page class Media: js = ['related-content.js'] (The related-content.js file would need to be placed in cfgov/unprocessed/js/molecules/ ; see Notes on Atomic Design .) This will load the related-content.js script on any page that includes the RelatedContent molecule in one of its StreamFields. How-to guides \u00b6 Creating a new component \u00b6 Review the Notes on Atomic Design page. Add each of the parts mentioned above: Create the Python class Add the class to a StreamField Create an HTML template for the component (Optionally) add some CSS (Optionally) add some JavaScript Note Before creating a new component, please consider whether one of our existing components can meet your needs. Talk to the consumerfinance.gov product owners if your content has specific display requirements that aren't served by an existing component, or if a specific maintenance efficiency will be gained from a new component. Adding a field to an existing component \u00b6 Locate the Python class of the component you want to add a field to. Add the field by inserting a snippet like this in the list of fields, in the order in which you want it to appear in the editor: field_name = blocks.BlockName() . Replace field_name with a succinct name for what data the field contains Replace BlockName with one of the basic Wagtail block types . Sometimes we create our own custom blocks that can be used, as well. See, for example, the HeadingBlock , used in InfoUnitGroup , among other places. Add any desired parameters: required=False if you do not want the field to be required (it usually is, by default) label='Some label' if you would like the editor to show a label more meaningful than the sentence-case transformation of the field name help_text='Some text' if the field needs a more verbose explanation to be shown in the editor to make it clear to users how it should work default= some appropriate value if you want the field to have a specific default value, e.g., True to have a BooleanBlock checkbox default to checked. Certain blocks may take other arguments, as described in the basic Wagtail blocks documentation . Edit the component template to do something with the field's data \u2013 output it, use it to trigger a CSS class, etc. Create a schema migration. Editing a field on an existing component \u00b6 Determine if the change you want to make will need a data migration. If the answer is no : make your changes, create a schema migration , and be on your merry way. If the answer is yes : continue on. Add the new version of the field. Create a schema migration for adding the new field. Create a data migration to copy data from the old field into the new field. Edit the component template to use the new field's data instead of the old field's data. Remove the old field. Create a schema migration for removing the old field. Removing a field from an existing component \u00b6 These instructions presume that you do not care about any data stored in the field you are deleting. If that is not the case, please go up to the instructions for editing a field and come back here when instructed. Locate the field you want to remove in the block's Python class. Delete the field definition. Create a schema migration. Creating migrations for StreamField blocks \u00b6 To automatically generate a schema migration , run ./cfgov/manage.py makemigrations -n description_of_changes from the root of the repository. You may also need a data migration \u00b6 Some field edits (like changing the default , label , help_text , and required properties, or changing the order of fields on a block) do not require a data migration. A schema migration is sufficient. Data migrations are required any time you: rename an existing field change the type of an existing field delete an existing field rename a block within a StreamField delete a block if you do not want to lose any data already stored in that field or block. In other words, if an existing field or block is changing, any data stored in that field or block has to be migrated to a different place, unless you're OK with jettisoning it. Not sure if there's actually any data that could potentially be lost? \u00b6 You may not know off the top of your head if a component you are modifying actually has any data stored that could be lost. One way that you can manually check to see if this is the case is to use the Block Inventory feature that we added to Wagtail . This feature lets you search all Wagtail pages on the site for the presence of a component. Here's how: With a current database dump, visit http://localhost:8000/admin/inventory/ . (This is also available from the admin menu by going to Settings Block Inventory .) Using the three pairs of dropdown menus at the top of the page, choose to look for pages that either include or exclude particular components. If multiple components are selected, resulting pages must match all of the conditions. Click Find matching pages to execute the search. If no results are found, lucky you! You're in the clear to make whatever changes you desire to that component without worrying about data loss. If, more likely, there are results, you should open each result, look through the page for the instance(s) of the component in question, and see if the changes you want to make would cause the loss of important data. If there is only a small amount of potential data loss, it may be more practical to forego the data migration and manually replace that data once the code changes have been deployed. If you think this is the preferable route, consult with the appropriate stakeholders to confirm that they are OK with the window of time in which users may experience a gap in the data that is being manually replaced. For more details on both kinds of migrations, see the Wagtail Migrations page .","title":"Creating and Editing Components"},{"location":"editing-components/#creating-and-editing-wagtail-components","text":"cfgov-refresh implements a number of components that editors can choose from when building a page, for example: Heroes, Expandable Groups, or Info Unit Groups. The CFPB Design Manual describes the design and intended usage of many of these components. In Wagtail parlance, these are called \"StreamField blocks\" * (or just \"blocks\"). We sometimes also refer to them as \"modules\", because we think that the terms \"component\" and \"module\" may be more obvious to non-developers. This page will use the terms somewhat interchangeably. One other important thing to note before we begin: blocks can be nested within other blocks. * If you're going to be doing anything more than making minor updates to existing components, this is highly recommended reading.","title":"Creating and Editing Wagtail Components"},{"location":"editing-components/#table-of-contents","text":"The parts of a Wagtail block The back end The Python class Adding it to a StreamField The front end The HTML template Adding CSS Adding JavaScript How-to guides Creating a new component Adding a field to an existing component Editing a field on an existing component Removing a field from an existing component Creating migrations for StreamField blocks","title":"Table of contents"},{"location":"editing-components/#the-parts-of-a-wagtail-block","text":"Blocks are implemented via several different bits of code: Defining a block's fields and other properties in a Python class Adding the class to a page's StreamField block options Creating an HTML template for rendering the block on a page (Optionally) adding some CSS for styling the block (Optionally) adding some JavaScript for adding advanced behavior Before you dive in further, check out the Notes on Atomic Design page and familiarize yourself with our basic concepts of atoms, molecules, and organisms.","title":"The parts of a Wagtail block"},{"location":"editing-components/#the-back-end","text":"","title":"The back end"},{"location":"editing-components/#the-python-class","text":"A component's fields and other properties are defined in a Python class, typically a subclass of Wagtail's StructBlock . These classes are located in a number of different files across the repository, but there are two major categories they fall into: Files corresponding to a general-purpose, site-wide atomic component. These files\u2014 atoms.py , molecules.py , and organisms.py \u2014are located in cfgov/v1/atomic_elements . Files that are specific to a particular sub-app, such as regulations3k's blocks.py . There are other places where StreamField block classes are defined (particularly blocks that are only ever used as fields within another block), but these are the two most common locations where top-level Wagtail modules are stored. A simple component class looks like this: class RelatedContent(blocks.StructBlock): # 1 heading = blocks.CharBlock(required=False) # 2 paragraph = blocks.RichTextBlock(required=False) # 3 links = blocks.ListBlock(atoms.Hyperlink()) # 4 class Meta: # 5 icon = 'grip' # 6 label = 'Related content' # 7 template = '_includes/molecules/related-content.html' # 8 There are a few things happening here: The RelatedContent class is a subclass of Wagtail's StructBlock , which allows for the combination of a fixed number of other sub-blocks (see previous comment about blocks being nested within other blocks) into a single unit (what we'd think of as a \"module\" in the Wagtail editor). This one has three sub-blocks (lines 2, 3, and 4). The heading field uses the basic Wagtail CharBlock , which results in a field with a basic single-line text input. The paragraph field uses the basic Wagtail RichTextBlock , which results in a field with a multiline WYSIWYG text input. The links field uses another basic Wagtail block, ListBlock , which is a special type of block that can hold a variable number of some other block (the Hyperlink atom block, in this case). The Meta class defines some properties on the RelatedContent block that are used by the Wagtail admin or in rendering the block. The icon property tells Wagtail what icon to use in the editor for the button you use to add a RelatedContent block to a StreamField. Icon options can be found in the Wagtail style guide when running locally: http://localhost:8000/admin/styleguide/#icons The optional label property overrides the text of that same button; if label is not set, Wagtail will generate one from the name of the block. The template property is a pointer to the HTML template used to render this component. See below for more on templates. This results in a module that looks like this in the Wagtail editor: Note again that what we think of as fields are also blocks, and what we think of as components or modules are a special kind of block, StructBlock , that comprise the sub-blocks that are our fields. There are two common optional things that are also used in component classes: Overriding the default get_context method to pass additional data to the template Adding component-specific JavaScript via the Media class","title":"The Python class"},{"location":"editing-components/#adding-it-to-a-streamfield","text":"Components are made available in the page editing interface by adding them to one of a page types's StreamFields. These are usually the first things in a page's class definition. For example, see this snippet from blog_page.py : class BlogPage(AbstractFilterPage): content = StreamField([ ('full_width_text', organisms.FullWidthText()), ('info_unit_group', organisms.InfoUnitGroup()), ('expandable', organisms.Expandable()), ('well', organisms.Well()), ('email_signup', organisms.EmailSignUp()), ('feedback', v1_blocks.Feedback()), ]) \u2026 This sets up a StreamField named content that allows for the insertion of any of those seven listed blocks into it. To make the RelatedContent module (shown above) available to this StreamField, we'd add a new entry to this list following the same format: ('related_content', molecules.RelatedContent()), . Most page types have two StreamFields ( header and content ) in the general content area (the first tab on an editing screen), and most also share a common sidefoot StreamField (so named for the fact that it appears on the right side on some page types, but in the footer on others) on the sidebar tab. Don't forget the migrations! Adding or changing fields on either Python class will always require a new Django schema migration ; additionally, changing field names or types on an existing block will require a Django data migration . See the guide on creating migrations for StreamField blocks for more details.","title":"Adding it to a StreamField"},{"location":"editing-components/#the-front-end","text":"Before diving into the front-end code, a reminder to visit the Notes on Atomic Design page to learn about how we conceive of components in a hierarchy of atoms, molecules, and organisms.","title":"The front end"},{"location":"editing-components/#the-html-template","text":"Frontend rendering of Wagtail StreamField blocks to HTML can be controlled by writing a Django template and associating it with the block type. A custom block definition can specify the template path in its Meta class: from wagtail.core import blocks class PersonBlock(blocks.StructBlock): name = blocks.CharBlock() email = blocks.EmailBlock() class Meta: template = 'myapp/blocks/person_block.html' StreamField block templates are loaded by the Django template loader in the same way that Django page templates are. The specified template path must be loadable by one of the Django template engines configured in settings.TEMPLATES . (This project supports both the standard Django templates backend and the Jinja2 backend , but Jinja2 is more commonly used.) See the Django templates documentation for more details on the search algorithm used to locate a template. Returning to the RelatedContent example, this is what its Jinja2 template looks like (comments excluded): div class= m-related-content {% if value.heading %} header class= m-slug-header h2 class= a-heading {{ value.heading }} /h2 /header {% endif %} {{ value.paragraph | safe }} {% if value.links %} ul class= m-list m-list__links {% for link in value.links %} li class= m-list_item a href= {{ link.url }} class= m-list_link {{ link.text }} /a /li {% endfor %} /ul {% endif %} /div When Wagtail renders a block, it includes the values of its fields in an object named value . Above, you can see where the heading and paragraph fields are output with Jinja2 expression tags . And note how the links field (a ListBlock ) is iterated over, and the values of its Hyperlink child blocks are output. That's about as simple an example as it gets, but block templates can get much more complex when they have lots of child blocks and grandchild blocks. Also, if a block definition has overridden get_context to pass other data into the template (as described at the end of the Python class section above), those context variables can also be output with simple Jinja2 expression tags: {{ context_var }} .","title":"The HTML template"},{"location":"editing-components/#adding-css","text":"If a component needs any custom styling not already provided by Capital Framework or cfgov-refresh, you can add it by creating a new Less file for the component. Note Please be sure that you actually need new Less before creating it. We have a wide array of styles already available in Capital Framework components and here in cfgov-refresh , some of which could perhaps be combined to achieve your desired result. Also be sure that new component designs have gone through our internal approval process before adding them to the project. If you're working on a general-purpose atomic component for site-wide use, this file should live in cfgov/unprocessed/css/ atoms|molecules|organisms / . (Choose the deepest folder according to the atomic rank of the component.) Continuing the RelatedContent example, if it needed its own styles, it would live at cfgov/unprocessed/css/molecules/related-content.less . Newly-created Less files need to be imported into the project's master main.less file, located at cfgov/unprocessed/css/main.less . Please place them in the appropriate section for their atomic rank. Because cfgov-refresh uses main.less to build a single CSS file for almost the entire project, it is not necessary to tell the Python model anything about a component-specific stylesheet (for general-purpose, site-wide components). That is not the case with JavaScript, as we will see in the next section. Note If you're working on a component that belongs to a particular sub-app, its Less file should live in cfgov/unprocessed/ app-name /css/ .","title":"Adding CSS"},{"location":"editing-components/#adding-javascript","text":"Each atomic component may optionally be given a Media class that can list one or more JavaScript files that should be loaded when using it. When a page is requested via the browser, code contained in base.html will loop all atomic components for the requested page and load the appropriate atomic JavaScript bundles. Here is how one would add the Media class to our RelatedContent example: class RelatedContent(blocks.StructBlock): \u2026 # see first example on this page class Media: js = ['related-content.js'] (The related-content.js file would need to be placed in cfgov/unprocessed/js/molecules/ ; see Notes on Atomic Design .) This will load the related-content.js script on any page that includes the RelatedContent molecule in one of its StreamFields.","title":"Adding JavaScript"},{"location":"editing-components/#how-to-guides","text":"","title":"How-to guides"},{"location":"editing-components/#creating-a-new-component","text":"Review the Notes on Atomic Design page. Add each of the parts mentioned above: Create the Python class Add the class to a StreamField Create an HTML template for the component (Optionally) add some CSS (Optionally) add some JavaScript Note Before creating a new component, please consider whether one of our existing components can meet your needs. Talk to the consumerfinance.gov product owners if your content has specific display requirements that aren't served by an existing component, or if a specific maintenance efficiency will be gained from a new component.","title":"Creating a new component"},{"location":"editing-components/#adding-a-field-to-an-existing-component","text":"Locate the Python class of the component you want to add a field to. Add the field by inserting a snippet like this in the list of fields, in the order in which you want it to appear in the editor: field_name = blocks.BlockName() . Replace field_name with a succinct name for what data the field contains Replace BlockName with one of the basic Wagtail block types . Sometimes we create our own custom blocks that can be used, as well. See, for example, the HeadingBlock , used in InfoUnitGroup , among other places. Add any desired parameters: required=False if you do not want the field to be required (it usually is, by default) label='Some label' if you would like the editor to show a label more meaningful than the sentence-case transformation of the field name help_text='Some text' if the field needs a more verbose explanation to be shown in the editor to make it clear to users how it should work default= some appropriate value if you want the field to have a specific default value, e.g., True to have a BooleanBlock checkbox default to checked. Certain blocks may take other arguments, as described in the basic Wagtail blocks documentation . Edit the component template to do something with the field's data \u2013 output it, use it to trigger a CSS class, etc. Create a schema migration.","title":"Adding a field to an existing component"},{"location":"editing-components/#editing-a-field-on-an-existing-component","text":"Determine if the change you want to make will need a data migration. If the answer is no : make your changes, create a schema migration , and be on your merry way. If the answer is yes : continue on. Add the new version of the field. Create a schema migration for adding the new field. Create a data migration to copy data from the old field into the new field. Edit the component template to use the new field's data instead of the old field's data. Remove the old field. Create a schema migration for removing the old field.","title":"Editing a field on an existing component"},{"location":"editing-components/#removing-a-field-from-an-existing-component","text":"These instructions presume that you do not care about any data stored in the field you are deleting. If that is not the case, please go up to the instructions for editing a field and come back here when instructed. Locate the field you want to remove in the block's Python class. Delete the field definition. Create a schema migration.","title":"Removing a field from an existing component"},{"location":"editing-components/#creating-migrations-for-streamfield-blocks","text":"To automatically generate a schema migration , run ./cfgov/manage.py makemigrations -n description_of_changes from the root of the repository.","title":"Creating migrations for StreamField blocks"},{"location":"editing-components/#you-may-also-need-a-data-migration","text":"Some field edits (like changing the default , label , help_text , and required properties, or changing the order of fields on a block) do not require a data migration. A schema migration is sufficient. Data migrations are required any time you: rename an existing field change the type of an existing field delete an existing field rename a block within a StreamField delete a block if you do not want to lose any data already stored in that field or block. In other words, if an existing field or block is changing, any data stored in that field or block has to be migrated to a different place, unless you're OK with jettisoning it.","title":"You may also need a data migration"},{"location":"editing-components/#not-sure-if-theres-actually-any-data-that-could-potentially-be-lost","text":"You may not know off the top of your head if a component you are modifying actually has any data stored that could be lost. One way that you can manually check to see if this is the case is to use the Block Inventory feature that we added to Wagtail . This feature lets you search all Wagtail pages on the site for the presence of a component. Here's how: With a current database dump, visit http://localhost:8000/admin/inventory/ . (This is also available from the admin menu by going to Settings Block Inventory .) Using the three pairs of dropdown menus at the top of the page, choose to look for pages that either include or exclude particular components. If multiple components are selected, resulting pages must match all of the conditions. Click Find matching pages to execute the search. If no results are found, lucky you! You're in the clear to make whatever changes you desire to that component without worrying about data loss. If, more likely, there are results, you should open each result, look through the page for the instance(s) of the component in question, and see if the changes you want to make would cause the loss of important data. If there is only a small amount of potential data loss, it may be more practical to forego the data migration and manually replace that data once the code changes have been deployed. If you think this is the preferable route, consult with the appropriate stakeholders to confirm that they are OK with the window of time in which users may experience a gap in the data that is being manually replaced. For more details on both kinds of migrations, see the Wagtail Migrations page .","title":"Not sure if there's actually any data that could potentially be lost?"},{"location":"feature-flags/","text":"Feature flags \u00b6 Feature flags are implemented using our Django-Flags and Wagtail-Flags apps. The Django-Flags documentation contains an overview of feature flags and how to use them and the Wagtail-Flags README describes how to add feature flag conditions in the Wagtail admin. This document covers how to add and use feature flags with cfgov-refresh and the conventions we have around their use. Adding a flag Checking a flag In templates Jinja2 Django In code In URLs Enabling a flag Hard-coded conditions Database conditions Satellite apps Hygiene Adding a flag \u00b6 Feature flags are defined in code in the cfgov/settings/base.py file as part of the FLAGS setting. Each flag consists of a single string and a Python list of its hard-coded conditions (see Enabling a flag below). FLAGS = { # Beta banner, seen on beta.consumerfinance.gov # When enabled, a banner appears across the top of the site proclaiming # This beta site is a work in progress. 'BETA_NOTICE': [], } By convention our flag names are all uppercase, with underscores instead of whitespace. A comment is expected above each flag with a short description fo what happens when it is enabled. Checking a flag \u00b6 Flags can be checked either in Python code or in Django or Jinja2 template files. See the full Wagtail Flags API is documented for more information. In templates \u00b6 Jinja2 \u00b6 Most of cfgov-refresh's templates are Jinja2. In these templates, two template functions are provided, flag_enabled and flag_disabled . Each takes a flag name as its first argument and request` object as the second. flag_enabled('MY_FLAG') will return True if the conditions under which MY_FLAG is enabled are met. flag_disabled('MY_FLAG') will return True if the conditions under which MY_FLAG is enabled are not met. See Enabling a flag below for more on flag conditions. An example is the BETA_NOTICE flag as implemented in header.html : {% if flag_enabled('BETA_NOTICE') and show_banner %} div class= m-global-banner div class= wrapper wrapper__match-content o-expandable o-expandable__expanded div class= m-global-banner_head span class= cf-icon cf-icon-error-round m-global-banner_icon /span This beta site is a work in progress. /div \u2026 /div /div {% endif %} Django \u00b6 In Django templates (used in Satellite apps and the Wagtail admin), two template functions are provided flag_enabled and flag_disabled once the feature_flags template tag library is loaded. flag_enabled 'MY_FLAG' will return True if the conditions under which MY_FLAG is enabled are met. flag_disabled 'MY_FLAG' will return True if the conditions under which MY_FLAG is enabled are not met. See Enabling a flag below for more on flag conditions. The BETA_NOTICE Jinja2 example above when implemented with Django templates would look like this: {% load feature_flags %} {% flag_enabled 'BETA_NOTICE' as beta_flag %} {% if beta_flag and show_banner %} div class= m-global-banner div class= wrapper wrapper__match-content o-expandable o-expandable__expanded div class= m-global-banner_head span class= cf-icon cf-icon-error-round m-global-banner_icon /span This beta site is a work in progress. /div \u2026 /div /div {% endif %} In code \u00b6 In Python code three functions are available for checking feature flags, flag_state , flag_enabled , and flag_disabled . The Python API is slightly different from the Jinja2 or Django template API, in that flag conditions can take more potential arguments than requests, and thus flags are more flexible when checked in Python (in and outside a request cycle). See the Django-Flags flag state API documentation for more . Additionally two decorators, flag_check and flag_required , are provided for wrapping views (and another functions) in a feature flag check. See the Django-Flags flag decorators API documentation for more . In URLs \u00b6 There are two ways to flag Django URL patterns in urls.py : with flagged_url() in place of url() for a single pattern, or with the flagged_urls() context manager for multiple URLs. flagged_url(flag_name, regex, view, kwargs=None, name=None, state=True, fallback=None) works exactly like url() except it takes a flag name as its first argument. If the flag's state matches the given state , the URL pattern will be served from the given view ; if not, and fallback is given, the fallback will be used. An example is our WAGTAIL_ABOUT_US flag : flagged_url('WAGTAIL_ABOUT_US', r'^about-us/$', lambda req: ServeView.as_view()(req, req.path), fallback=SheerTemplateView.as_view( template_name='about-us/index.html'), name='about-us'), Ignoring the view being a lambda for now (see Flagging Wagtail URLs below ), this URL will be served via Wagtail if WAGTAIL_ABOUT_US 's conditions are True , and from a TemplateView if its conditions are False . If you need to flag multiple URLs with the same flag, you can use the flagged_urls() context manager. with flagged_urls(flag_name, state=True, fallback=None) as url provides a context in which the returned url() function can be used in place of the Django url() function in patterns and those patterns will share the same feature flag, state, and fallback. An example is our WAGTAIL_ASK_CFPB flag : with flagged_urls('WAGTAIL_ASK_CFPB') as url: ask_patterns = [ url(r'^(?i)ask-cfpb/([-\\w]{1,244})-(en)-(\\d{1,6})/?$', view_answer, name='ask-english-answer'), url(r'^(?i)obtener-respuestas/([-\\w]{1,244})-(es)-(\\d{1,6})/?$', view_answer, name='ask-spanish-answer'), \u2026 ] urlpatterns += ask_patterns Warning Do not attempt to use flag_check or any flag state-checking functions in urls.py . Because they will be evaluated on import of urls.py they will attempt to access the Django FlagState model before it is ready and will error. Flagging Wagtail URLs \u00b6 Wagtail views in flagged_url with a Django view as fallback (or vice-versa) can be a bit awkward. Django views are typically called with request as the first argument, and Wagtail's serve view takes both the request and the path. To get around this, in flagged_url we typically use a lambda for the view: lambda req: ServeView.as_view()(req, req.path) This lambda takes the request and calls the Wagtail-Sharing ServeView (which we're using in place of wagtail.wagtailcore.views.serve ). Enabling a flag \u00b6 Feature flags are enabled based on a set of conditions that are given either in the Django settings files (in cfgov/cfgov/settings/ ) or in the Django or Wagtail admin. Multiple conditions can be given, both in settings and in the admin, and if any condition is satisfied a flag is enabled. A list of available conditions and how to use them is available in the Django-Flags documentation . Hard-coded conditions \u00b6 Conditions that are defined in the Django settings are hard-coded, and require a change to files in cfgov-refresh, a new tagged release, and new deployment to change. These conditions should be used for flags that are relatively long-lasting and that can require a round-trip through the release and deployment process to change. When adding a flag to the Django settings the flag's dictionary of conditions can contain a condition name and value that must be satisfied for the flag to be enabled. The nature of that value changes depending on the condition type. See the Django-Flags conditions documentation for more on individual conditions. There is a simple boolean condition that is either True or False , and if it is True the flag is enabled and if it is False the flag is disabled. If we want to always turn the BETA_NOTICE flag on in settings with a boolean condition, that would look like this: FLAGS = { # Beta banner, seen on beta.consumerfinance.gov # When enabled, a banner appears across the top of the site proclaiming # This beta site is a work in progress. 'BETA_NOTICE': [ { 'condition': 'boolean', 'value': True, }, ], } Database conditions \u00b6 Conditions that are managed via the Wagtail or Django admin are stored in the database. These conditions can be changed in real-time and do not require any code changes or release and deployment to change (presuming the code that uses the feature flag is in place). To view, delete, and add database conditions, navigate to \"Settings Flags\" in the Wagtail admin. Once in the flag settings, you'll have a list of all flags and their conditions.. Database conditions can be deleted with the trash can button on the right. To create a new database condition, select \"Add a condition\". As with hard-coded conditions , to create a database condition you must select which condition type you would like to use and give it a value that must be satisfied for the flag to be enabled. Database conditions can only be set for flags that exist in the Django settings. Satellite apps \u00b6 Feature flags can be used in satellite apps in exactly the same way they are used in cfgov-refresh. An example is the use of a feature flagged template choice in the complaintdatabase app . Hygiene \u00b6 Feature flags should be rare and ephemeral. Changes should be small and frequent, and not big-bang releases, and flags that are no longer used and their conditions should be cleaned up and removed from code and the database.","title":"Feature Flags"},{"location":"feature-flags/#feature-flags","text":"Feature flags are implemented using our Django-Flags and Wagtail-Flags apps. The Django-Flags documentation contains an overview of feature flags and how to use them and the Wagtail-Flags README describes how to add feature flag conditions in the Wagtail admin. This document covers how to add and use feature flags with cfgov-refresh and the conventions we have around their use. Adding a flag Checking a flag In templates Jinja2 Django In code In URLs Enabling a flag Hard-coded conditions Database conditions Satellite apps Hygiene","title":"Feature flags"},{"location":"feature-flags/#adding-a-flag","text":"Feature flags are defined in code in the cfgov/settings/base.py file as part of the FLAGS setting. Each flag consists of a single string and a Python list of its hard-coded conditions (see Enabling a flag below). FLAGS = { # Beta banner, seen on beta.consumerfinance.gov # When enabled, a banner appears across the top of the site proclaiming # This beta site is a work in progress. 'BETA_NOTICE': [], } By convention our flag names are all uppercase, with underscores instead of whitespace. A comment is expected above each flag with a short description fo what happens when it is enabled.","title":"Adding a flag"},{"location":"feature-flags/#checking-a-flag","text":"Flags can be checked either in Python code or in Django or Jinja2 template files. See the full Wagtail Flags API is documented for more information.","title":"Checking a flag"},{"location":"feature-flags/#in-templates","text":"","title":"In templates"},{"location":"feature-flags/#jinja2","text":"Most of cfgov-refresh's templates are Jinja2. In these templates, two template functions are provided, flag_enabled and flag_disabled . Each takes a flag name as its first argument and request` object as the second. flag_enabled('MY_FLAG') will return True if the conditions under which MY_FLAG is enabled are met. flag_disabled('MY_FLAG') will return True if the conditions under which MY_FLAG is enabled are not met. See Enabling a flag below for more on flag conditions. An example is the BETA_NOTICE flag as implemented in header.html : {% if flag_enabled('BETA_NOTICE') and show_banner %} div class= m-global-banner div class= wrapper wrapper__match-content o-expandable o-expandable__expanded div class= m-global-banner_head span class= cf-icon cf-icon-error-round m-global-banner_icon /span This beta site is a work in progress. /div \u2026 /div /div {% endif %}","title":"Jinja2"},{"location":"feature-flags/#django","text":"In Django templates (used in Satellite apps and the Wagtail admin), two template functions are provided flag_enabled and flag_disabled once the feature_flags template tag library is loaded. flag_enabled 'MY_FLAG' will return True if the conditions under which MY_FLAG is enabled are met. flag_disabled 'MY_FLAG' will return True if the conditions under which MY_FLAG is enabled are not met. See Enabling a flag below for more on flag conditions. The BETA_NOTICE Jinja2 example above when implemented with Django templates would look like this: {% load feature_flags %} {% flag_enabled 'BETA_NOTICE' as beta_flag %} {% if beta_flag and show_banner %} div class= m-global-banner div class= wrapper wrapper__match-content o-expandable o-expandable__expanded div class= m-global-banner_head span class= cf-icon cf-icon-error-round m-global-banner_icon /span This beta site is a work in progress. /div \u2026 /div /div {% endif %}","title":"Django"},{"location":"feature-flags/#in-code","text":"In Python code three functions are available for checking feature flags, flag_state , flag_enabled , and flag_disabled . The Python API is slightly different from the Jinja2 or Django template API, in that flag conditions can take more potential arguments than requests, and thus flags are more flexible when checked in Python (in and outside a request cycle). See the Django-Flags flag state API documentation for more . Additionally two decorators, flag_check and flag_required , are provided for wrapping views (and another functions) in a feature flag check. See the Django-Flags flag decorators API documentation for more .","title":"In code"},{"location":"feature-flags/#in-urls","text":"There are two ways to flag Django URL patterns in urls.py : with flagged_url() in place of url() for a single pattern, or with the flagged_urls() context manager for multiple URLs. flagged_url(flag_name, regex, view, kwargs=None, name=None, state=True, fallback=None) works exactly like url() except it takes a flag name as its first argument. If the flag's state matches the given state , the URL pattern will be served from the given view ; if not, and fallback is given, the fallback will be used. An example is our WAGTAIL_ABOUT_US flag : flagged_url('WAGTAIL_ABOUT_US', r'^about-us/$', lambda req: ServeView.as_view()(req, req.path), fallback=SheerTemplateView.as_view( template_name='about-us/index.html'), name='about-us'), Ignoring the view being a lambda for now (see Flagging Wagtail URLs below ), this URL will be served via Wagtail if WAGTAIL_ABOUT_US 's conditions are True , and from a TemplateView if its conditions are False . If you need to flag multiple URLs with the same flag, you can use the flagged_urls() context manager. with flagged_urls(flag_name, state=True, fallback=None) as url provides a context in which the returned url() function can be used in place of the Django url() function in patterns and those patterns will share the same feature flag, state, and fallback. An example is our WAGTAIL_ASK_CFPB flag : with flagged_urls('WAGTAIL_ASK_CFPB') as url: ask_patterns = [ url(r'^(?i)ask-cfpb/([-\\w]{1,244})-(en)-(\\d{1,6})/?$', view_answer, name='ask-english-answer'), url(r'^(?i)obtener-respuestas/([-\\w]{1,244})-(es)-(\\d{1,6})/?$', view_answer, name='ask-spanish-answer'), \u2026 ] urlpatterns += ask_patterns Warning Do not attempt to use flag_check or any flag state-checking functions in urls.py . Because they will be evaluated on import of urls.py they will attempt to access the Django FlagState model before it is ready and will error.","title":"In URLs"},{"location":"feature-flags/#flagging-wagtail-urls","text":"Wagtail views in flagged_url with a Django view as fallback (or vice-versa) can be a bit awkward. Django views are typically called with request as the first argument, and Wagtail's serve view takes both the request and the path. To get around this, in flagged_url we typically use a lambda for the view: lambda req: ServeView.as_view()(req, req.path) This lambda takes the request and calls the Wagtail-Sharing ServeView (which we're using in place of wagtail.wagtailcore.views.serve ).","title":"Flagging Wagtail URLs"},{"location":"feature-flags/#enabling-a-flag","text":"Feature flags are enabled based on a set of conditions that are given either in the Django settings files (in cfgov/cfgov/settings/ ) or in the Django or Wagtail admin. Multiple conditions can be given, both in settings and in the admin, and if any condition is satisfied a flag is enabled. A list of available conditions and how to use them is available in the Django-Flags documentation .","title":"Enabling a flag"},{"location":"feature-flags/#hard-coded-conditions","text":"Conditions that are defined in the Django settings are hard-coded, and require a change to files in cfgov-refresh, a new tagged release, and new deployment to change. These conditions should be used for flags that are relatively long-lasting and that can require a round-trip through the release and deployment process to change. When adding a flag to the Django settings the flag's dictionary of conditions can contain a condition name and value that must be satisfied for the flag to be enabled. The nature of that value changes depending on the condition type. See the Django-Flags conditions documentation for more on individual conditions. There is a simple boolean condition that is either True or False , and if it is True the flag is enabled and if it is False the flag is disabled. If we want to always turn the BETA_NOTICE flag on in settings with a boolean condition, that would look like this: FLAGS = { # Beta banner, seen on beta.consumerfinance.gov # When enabled, a banner appears across the top of the site proclaiming # This beta site is a work in progress. 'BETA_NOTICE': [ { 'condition': 'boolean', 'value': True, }, ], }","title":"Hard-coded conditions"},{"location":"feature-flags/#database-conditions","text":"Conditions that are managed via the Wagtail or Django admin are stored in the database. These conditions can be changed in real-time and do not require any code changes or release and deployment to change (presuming the code that uses the feature flag is in place). To view, delete, and add database conditions, navigate to \"Settings Flags\" in the Wagtail admin. Once in the flag settings, you'll have a list of all flags and their conditions.. Database conditions can be deleted with the trash can button on the right. To create a new database condition, select \"Add a condition\". As with hard-coded conditions , to create a database condition you must select which condition type you would like to use and give it a value that must be satisfied for the flag to be enabled. Database conditions can only be set for flags that exist in the Django settings.","title":"Database conditions"},{"location":"feature-flags/#satellite-apps","text":"Feature flags can be used in satellite apps in exactly the same way they are used in cfgov-refresh. An example is the use of a feature flagged template choice in the complaintdatabase app .","title":"Satellite apps"},{"location":"feature-flags/#hygiene","text":"Feature flags should be rare and ephemeral. Changes should be small and frequent, and not big-bang releases, and flags that are no longer used and their conditions should be cleaned up and removed from code and the database.","title":"Hygiene"},{"location":"forms-in-wagtail/","text":"Including forms into Wagtail page context \u00b6 Wagtail provides a FormBuilder module, but it cannot be used with subclasses of Page, like our CFGOVPage. For our purposes there is AbstractFormBlock , a subclass of StructBlock that implements methods to process a request. If a developer wishes to add a module that includes a form, they only need to follow a few steps in order to get it handled properly: Create the form. Create handler class that implements a method named process . The process method should take in a boolean parameter named is_submitted that flags whether or not that particular module has been the source of the request. The process method should return a dictionary that will be included in the context of the page and a JSONResponse for AJAX requests. If a context is returned, this is where the form would go. Create a subclass of AbstractFormBlock with any other blocks that are required. Add the path to the handler class to the block class' Meta handler attribute. Create a template in which to render the form. Here's an example of a form's block class: ... class FormBlock(AbstractFormBlock): heading = blocks.CharBlock() class Meta: handler = 'app_name.handlers.handler_class' # defaults method = 'POST' icon = 'form' ... And an example of a handler class: ... class ConferenceRegistrationHandler(Handler): def process(self, is_submitted): if is_submitted: form = Form(self.request.POST) if form.is_valid(): return success else: return fail return {'form': Form()} ...","title":"Forms in Wagtail Page Context"},{"location":"forms-in-wagtail/#including-forms-into-wagtail-page-context","text":"Wagtail provides a FormBuilder module, but it cannot be used with subclasses of Page, like our CFGOVPage. For our purposes there is AbstractFormBlock , a subclass of StructBlock that implements methods to process a request. If a developer wishes to add a module that includes a form, they only need to follow a few steps in order to get it handled properly: Create the form. Create handler class that implements a method named process . The process method should take in a boolean parameter named is_submitted that flags whether or not that particular module has been the source of the request. The process method should return a dictionary that will be included in the context of the page and a JSONResponse for AJAX requests. If a context is returned, this is where the form would go. Create a subclass of AbstractFormBlock with any other blocks that are required. Add the path to the handler class to the block class' Meta handler attribute. Create a template in which to render the form. Here's an example of a form's block class: ... class FormBlock(AbstractFormBlock): heading = blocks.CharBlock() class Meta: handler = 'app_name.handlers.handler_class' # defaults method = 'POST' icon = 'form' ... And an example of a handler class: ... class ConferenceRegistrationHandler(Handler): def process(self, is_submitted): if is_submitted: form = Form(self.request.POST) if form.is_valid(): return success else: return fail return {'form': Form()} ...","title":"Including forms into Wagtail page context"},{"location":"hmda/","text":"HMDA API docs \u00b6 A resource for data required by the Home Mortgage Disclosure Act \u00b6 The CFPB publishes data gleaned from mortgage loan applications going back to 2007. The API enables automated access to this rich data set. Full details on how to use it starts here .","title":"HMDA Data"},{"location":"hmda/#hmda-api-docs","text":"","title":"HMDA API docs"},{"location":"hmda/#a-resource-for-data-required-by-the-home-mortgage-disclosure-act","text":"The CFPB publishes data gleaned from mortgage loan applications going back to 2007. The API enables automated access to this rich data set. Full details on how to use it starts here .","title":"A resource for data required by the Home Mortgage Disclosure Act"},{"location":"housing-counselor-tool/","text":"Find a Housing Counselor \u00b6 The Find a Housing Counselor tool allows users to search for HUD-approved housing counselors. Users enter a U.S. ZIP code, and the tool returns a list of the ten housing counselors nearest to that ZIP code location. The page also displays a map of the results using Mapbox. When the user enters a ZIP code in the page's search box, the Django view fetches a JSON file of the results from our Amazon S3 bucket. The results are inserted into the page template and the Mapbox map . The page also contains a link to a PDF version of the results, which is also stored in S3. The files are publicly accessible, so the tool can run on localhost or in a container without any change in behavior. This page documents the process we use to generate the JSON and PDF files the Find a Housing Counselor tool relies on. Housing Counselor Data Processing \u00b6 The tool gets its data from the U.S. Department of Housing and Urban Development (HUD). HUD provides an API to their list of approved counseling agencies. A daily job, cf.gov-housing-counselor-data on our external Jenkins server, queries HUD data and produces the JSON and PDF files we use for the Find a Housing Counselor tool. It performs the following steps, each of which is optional and configured using parameters before starting the job. Geocode ZIP codes ( GEOCODE_ZIPCODES ) Generate JSON files ( MAKE_JSON ) Generate HTML files ( MAKE_HTML ) Generate PDFs ( MAKE_PDF ) Upload to S3 ( UPLOAD_TO_S3 ) Geocode ZIP codes \u00b6 The GEOCODE_ZIPCODES step generates a file of all ZIP codes in the United States and their location latitude and longitude and saves it in the Jenkins workspace. By default, this step is not enabled. Since ZIP code geographical information rarely changes, we run this step rarely by manually enabling the option in the Jenkins job. If enabled, this step calls the hud_geocode_zipcodes management command. The management command in turn calls the BulkZipCodeGeocoder in geocoder.py . BulkZipCodeGeocoder uses the Mapbox geocoding API to determine which 5-digit number sequences are ZIP codes and fetch their latitude and longitude values. The management command inserts this data into a CSV and saves it to ./zipcodes.csv on the Jenkins job workspace. The generated file looks like this: 12305,42.81,-73.94 12306,42.77,-73.96 12307,42.81,-73.93 12308,42.82,-73.93 12309,42.81,-73.91 12325,42.88779,-73.99597 12345,42.80856,-74.02737 Generate JSON files \u00b6 When enabled, the MAKE_JSON step generates a JSON file of housing counselor data for each ZIP code in the U.S. Each file contains the ten results geographically nearest to the ZIP code's latitude and longitude. This step is enabled by default. This step calls the hud_generate_json management command, which performs the following steps: fetch agency listings from HUD clean the results fill in any missing latitude and longitude values create files of the 10 nearest results for each U.S. ZIP code Fetch agency listings \u00b6 ( in fetcher.py ) Request every housing counselor in HUD's database with a request to https://data.hud.gov/Housing_Counselor/searchByLocation?Lat=38.8951 Long=-77.0367 Distance=5000 . It returns thousands of results like this: { services : DFC,FBC,PPC,RHC , languages : ENG,SPA , agc_STATUS : A , agc_SRC_CD : HUD , counslg_METHOD : Face to Face Counseling,Group Counseling,Phone Counseling , agcid : 80790 , adr1 : 1234 N. Example St. , adr2 : , city : SPRINGFIELD , email : counselor@example.org , fax : 999-888-7777 , nme : EXAMPLE COMMUNITY HOUSING SERVICES , phone1 : 111-222-3333 , statecd : MI , weburl : www.example.org , zipcd : 48219-8888 , agc_ADDR_LATITUDE : 42.442658 , agc_ADDR_LONGITUDE : -83.28329 , parentid : 81228 , county_NME : , phone2 : , mailingadr1 : 1234 N. Example St. , mailingadr2 : , mailingcity : SPRINGFIELD , mailingzipcd : 48219-8888 , mailingstatecd : MI , state_NME : Michigan , state_FIPS_CODE : null, faithbased : N , colonias_IND : Y , migrantwkrs_IND : N }, Clean the results \u00b6 ( in fetcher.py ) We replace the languages value with the full language names. We replace the services value with fully spelled out service descriptions. Both of these mappings come from the HUD API. ( in cleaner.py ) Clean the data: Convert latitude and longitude values to float values. Convert the the city and organization name to title case. Ensure email appears to be an email (otherwise leave it blank). If a URL value is present, ensure it begins with http:// . Backfill missing latitude and longitude values \u00b6 ( in geocoder.py ) If any counselor records are missing their latitude or longitude data, we fill in those values with the lat/long location of the agency's ZIP code. This uses the data file created in the Geocode ZIP Codes stage of the Jenkins job ( above ). Create collections of results by ZIP code \u00b6 ( in generator.py ) Create an in-memory SQLite database and define a distance_in_miles function in it. Fill the database with a three-column table: For each counselor in the list, include its latitude and longitude (both in radians) and all of its information from the HUD API as a JSON text string. For each ZIP code in the U.S., query the database to find the 10 closest housing counselors to the lat/long of that ZIP. Put the information in a JSON structure like this (but with ten results instead of one): { zip : { lat : 42.80856, lng : -74.02737, zipcode : 12345 }, counseling_agencies : [{ adr1 : 1234 N. Example St. , state_FIPS_CODE : null, adr2 : , zipcd : 48219-8888 , mailingcity : Springfield , weburl : http://www.example.org , agc_STATUS : A , city : Springfield , languages : [ English , Spanish ], faithbased : N , mailingstatecd : MI , email : counselor@example.org , fax : 999-888-7777 , phone1 : 111-222-3333 , distance : 5.430720023569205, phone2 : , agc_ADDR_LATITUDE : 42.442658, agcid : 80790 , agc_SRC_CD : HUD , nme : Example Community Housing Services , migrantwkrs_IND : N , parentid : 82772 , services : [ Mortgage Delinquency and Default Resolution Counse , Home Improvement and Rehabilitation Counseling , Pre-purchase Counseling , Pre-purchase Homebuyer Education Workshops , Rental Housing Counseling ], counslg_METHOD : Face to Face Counseling,Group Counseling,Phone Counseling , county_NME : , mailingadr1 : 1234 N. Example St. , statecd : MI , mailingadr2 : , mailingzipcd : 48219-8888 , state_NME : Michigan , agc_ADDR_LONGITUDE : -83.28329, colonias_IND : Y }] } Save the resulting JSON files on the Jenkins job workspace, in a jsons directory, e.g. jsons/12345.json . Generate HTML files \u00b6 When enabled, the MAKE_HTML step generates an HTML page of housing counselor results, including styles, for each file created in the previous step. It saves them in an htmls directory on the Jenkins job workspace. This step is enabled by default. This step calls the hud_generate_html management command, which calls HTML generation code in generator.py . Django renders the housing_counselor/pdf_selfcontained.html template with the housing counselor data from each JSON file. We save the resulting HTML files on the Jenkins job workspace, in a htmls directory, e.g. htmls/12345.html . Generate PDFs \u00b6 When enabled, the MAKE_PDF step generates a PDF of each file created in the previous step. It saves them in a pdfs directory on the Jenkins job workspace. This step is enabled by default. This step uses a HTML to PDF conversion command line tool. We run the conversion on each file in the htmls directory, generating a PDF version of each, e.g. pdfs/12345.pdf . Upload to S3 \u00b6 When enabled, the UPLOAD_TO_S3 step uploads the contents of the jsons and pdfs directories to an Amazon S3 bucket where it can be accessed by consumerfinance.gov. This step is enabled by default. The files are publicly accessible, e.g.: https://files.consumerfinance.gov/a/assets/hud/jsons/12345.json https://files.consumerfinance.gov/a/assets/hud/pdfs/12345.pdf","title":"Find a Housing Counselor Tool"},{"location":"housing-counselor-tool/#find-a-housing-counselor","text":"The Find a Housing Counselor tool allows users to search for HUD-approved housing counselors. Users enter a U.S. ZIP code, and the tool returns a list of the ten housing counselors nearest to that ZIP code location. The page also displays a map of the results using Mapbox. When the user enters a ZIP code in the page's search box, the Django view fetches a JSON file of the results from our Amazon S3 bucket. The results are inserted into the page template and the Mapbox map . The page also contains a link to a PDF version of the results, which is also stored in S3. The files are publicly accessible, so the tool can run on localhost or in a container without any change in behavior. This page documents the process we use to generate the JSON and PDF files the Find a Housing Counselor tool relies on.","title":"Find a Housing Counselor"},{"location":"housing-counselor-tool/#housing-counselor-data-processing","text":"The tool gets its data from the U.S. Department of Housing and Urban Development (HUD). HUD provides an API to their list of approved counseling agencies. A daily job, cf.gov-housing-counselor-data on our external Jenkins server, queries HUD data and produces the JSON and PDF files we use for the Find a Housing Counselor tool. It performs the following steps, each of which is optional and configured using parameters before starting the job. Geocode ZIP codes ( GEOCODE_ZIPCODES ) Generate JSON files ( MAKE_JSON ) Generate HTML files ( MAKE_HTML ) Generate PDFs ( MAKE_PDF ) Upload to S3 ( UPLOAD_TO_S3 )","title":"Housing Counselor Data Processing"},{"location":"housing-counselor-tool/#geocode-zip-codes","text":"The GEOCODE_ZIPCODES step generates a file of all ZIP codes in the United States and their location latitude and longitude and saves it in the Jenkins workspace. By default, this step is not enabled. Since ZIP code geographical information rarely changes, we run this step rarely by manually enabling the option in the Jenkins job. If enabled, this step calls the hud_geocode_zipcodes management command. The management command in turn calls the BulkZipCodeGeocoder in geocoder.py . BulkZipCodeGeocoder uses the Mapbox geocoding API to determine which 5-digit number sequences are ZIP codes and fetch their latitude and longitude values. The management command inserts this data into a CSV and saves it to ./zipcodes.csv on the Jenkins job workspace. The generated file looks like this: 12305,42.81,-73.94 12306,42.77,-73.96 12307,42.81,-73.93 12308,42.82,-73.93 12309,42.81,-73.91 12325,42.88779,-73.99597 12345,42.80856,-74.02737","title":"Geocode ZIP codes"},{"location":"housing-counselor-tool/#generate-json-files","text":"When enabled, the MAKE_JSON step generates a JSON file of housing counselor data for each ZIP code in the U.S. Each file contains the ten results geographically nearest to the ZIP code's latitude and longitude. This step is enabled by default. This step calls the hud_generate_json management command, which performs the following steps: fetch agency listings from HUD clean the results fill in any missing latitude and longitude values create files of the 10 nearest results for each U.S. ZIP code","title":"Generate JSON files"},{"location":"housing-counselor-tool/#fetch-agency-listings","text":"( in fetcher.py ) Request every housing counselor in HUD's database with a request to https://data.hud.gov/Housing_Counselor/searchByLocation?Lat=38.8951 Long=-77.0367 Distance=5000 . It returns thousands of results like this: { services : DFC,FBC,PPC,RHC , languages : ENG,SPA , agc_STATUS : A , agc_SRC_CD : HUD , counslg_METHOD : Face to Face Counseling,Group Counseling,Phone Counseling , agcid : 80790 , adr1 : 1234 N. Example St. , adr2 : , city : SPRINGFIELD , email : counselor@example.org , fax : 999-888-7777 , nme : EXAMPLE COMMUNITY HOUSING SERVICES , phone1 : 111-222-3333 , statecd : MI , weburl : www.example.org , zipcd : 48219-8888 , agc_ADDR_LATITUDE : 42.442658 , agc_ADDR_LONGITUDE : -83.28329 , parentid : 81228 , county_NME : , phone2 : , mailingadr1 : 1234 N. Example St. , mailingadr2 : , mailingcity : SPRINGFIELD , mailingzipcd : 48219-8888 , mailingstatecd : MI , state_NME : Michigan , state_FIPS_CODE : null, faithbased : N , colonias_IND : Y , migrantwkrs_IND : N },","title":"Fetch agency listings"},{"location":"housing-counselor-tool/#clean-the-results","text":"( in fetcher.py ) We replace the languages value with the full language names. We replace the services value with fully spelled out service descriptions. Both of these mappings come from the HUD API. ( in cleaner.py ) Clean the data: Convert latitude and longitude values to float values. Convert the the city and organization name to title case. Ensure email appears to be an email (otherwise leave it blank). If a URL value is present, ensure it begins with http:// .","title":"Clean the results"},{"location":"housing-counselor-tool/#backfill-missing-latitude-and-longitude-values","text":"( in geocoder.py ) If any counselor records are missing their latitude or longitude data, we fill in those values with the lat/long location of the agency's ZIP code. This uses the data file created in the Geocode ZIP Codes stage of the Jenkins job ( above ).","title":"Backfill missing latitude and longitude values"},{"location":"housing-counselor-tool/#create-collections-of-results-by-zip-code","text":"( in generator.py ) Create an in-memory SQLite database and define a distance_in_miles function in it. Fill the database with a three-column table: For each counselor in the list, include its latitude and longitude (both in radians) and all of its information from the HUD API as a JSON text string. For each ZIP code in the U.S., query the database to find the 10 closest housing counselors to the lat/long of that ZIP. Put the information in a JSON structure like this (but with ten results instead of one): { zip : { lat : 42.80856, lng : -74.02737, zipcode : 12345 }, counseling_agencies : [{ adr1 : 1234 N. Example St. , state_FIPS_CODE : null, adr2 : , zipcd : 48219-8888 , mailingcity : Springfield , weburl : http://www.example.org , agc_STATUS : A , city : Springfield , languages : [ English , Spanish ], faithbased : N , mailingstatecd : MI , email : counselor@example.org , fax : 999-888-7777 , phone1 : 111-222-3333 , distance : 5.430720023569205, phone2 : , agc_ADDR_LATITUDE : 42.442658, agcid : 80790 , agc_SRC_CD : HUD , nme : Example Community Housing Services , migrantwkrs_IND : N , parentid : 82772 , services : [ Mortgage Delinquency and Default Resolution Counse , Home Improvement and Rehabilitation Counseling , Pre-purchase Counseling , Pre-purchase Homebuyer Education Workshops , Rental Housing Counseling ], counslg_METHOD : Face to Face Counseling,Group Counseling,Phone Counseling , county_NME : , mailingadr1 : 1234 N. Example St. , statecd : MI , mailingadr2 : , mailingzipcd : 48219-8888 , state_NME : Michigan , agc_ADDR_LONGITUDE : -83.28329, colonias_IND : Y }] } Save the resulting JSON files on the Jenkins job workspace, in a jsons directory, e.g. jsons/12345.json .","title":"Create collections of results by ZIP code"},{"location":"housing-counselor-tool/#generate-html-files","text":"When enabled, the MAKE_HTML step generates an HTML page of housing counselor results, including styles, for each file created in the previous step. It saves them in an htmls directory on the Jenkins job workspace. This step is enabled by default. This step calls the hud_generate_html management command, which calls HTML generation code in generator.py . Django renders the housing_counselor/pdf_selfcontained.html template with the housing counselor data from each JSON file. We save the resulting HTML files on the Jenkins job workspace, in a htmls directory, e.g. htmls/12345.html .","title":"Generate HTML files"},{"location":"housing-counselor-tool/#generate-pdfs","text":"When enabled, the MAKE_PDF step generates a PDF of each file created in the previous step. It saves them in a pdfs directory on the Jenkins job workspace. This step is enabled by default. This step uses a HTML to PDF conversion command line tool. We run the conversion on each file in the htmls directory, generating a PDF version of each, e.g. pdfs/12345.pdf .","title":"Generate PDFs"},{"location":"housing-counselor-tool/#upload-to-s3","text":"When enabled, the UPLOAD_TO_S3 step uploads the contents of the jsons and pdfs directories to an Amazon S3 bucket where it can be accessed by consumerfinance.gov. This step is enabled by default. The files are publicly accessible, e.g.: https://files.consumerfinance.gov/a/assets/hud/jsons/12345.json https://files.consumerfinance.gov/a/assets/hud/pdfs/12345.pdf","title":"Upload to S3"},{"location":"installation/","text":"Installation and configuration for cfgov-refresh \u00b6 Clone the repository \u00b6 Using the console, navigate to the root directory in which your projects live and clone this project's repository: git clone git@github.com:cfpb/cfgov-refresh.git cd cfgov-refresh You may also wish to fork the repository on GitHub and clone the resultant personal fork. This is advised if you are going to be doing development on cfgov-refresh and contributing to the project. There are two ways to install cfgov-refresh: Stand-alone installation Docker-compose installation Stand-alone installation \u00b6 These instructions are somewhat specific to developing on macOS, but if you're familiar with other Unix-based systems, it should be fairly easy to adapt them to your needs. Install system-level requirements \u00b6 virtualenv virtualenvwrapper Python modules \u00b6 Install virtualenv and virtualenvwrapper to be able to create a local environment for your server: pip install virtualenv virtualenvwrapper Autoenv module \u00b6 This project uses a large number of environment variables. To automatically define environment variables and launch the virtualenv upon cd ing to the project folder, install Autoenv . We recommend using Homebrew : brew install autoenv After installation, Homebrew will output instructions similar to: To finish the installation, source activate.sh in your shell: source /Users/[YOUR USERNAME]/homebrew/opt/autoenv/activate.sh Run that now for your initial setup. Any time you run the project you\u2019ll need to run that last line, so if you\u2019ll be working with the project consistently, we suggest adding it to your Bash profile by running: echo 'source /Users/[YOUR USERNAME]/homebrew/opt/autoenv/activate.sh' ~/.bash_profile If you need to find this info again later, you can run: brew info autoenv Note If you use Zsh you\u2019ll need to use zsh-autoenv , but we can\u2019t provide support for issues that may arise. Front-end dependencies \u00b6 The cfgov-refresh front end currently uses the following frameworks / tools: Gulp : task management for pulling in assets, linting and concatenating code, etc. Less : CSS pre-processor. Capital Framework : User interface pattern-library produced by the CFPB. Note If you\u2019re new to Capital Framework, we encourage you to start here . Install Node.js however you\u2019d like. We recommend using nvm , though. Note This project requires Node.js v8.5 or higher, and npm v5.2 or higher. Install yarn . We recommend using Homebrew : # Use --ignore-dependencies to use your system installed node version brew install yarn --ignore-dependencies Webfonts \u00b6 The site uses a proprietary licensed font, Avenir. If you want to pull this from a content delivery network (CDN), you can set the @use-font-cdn to true and rebuild the assets with yarn run gulp build . If you want to install self-hosted fonts locally, you can place the font files in static.in/cfgov-fonts/fonts/ and restart the local web server. If you are a CFPB employee, you can perform this step with: cd static.in/ git clone https://[GHE]/CFGOV/cfgov-fonts/ Where [GHE] is our GitHub Enterprise URL. Set up your environment \u00b6 The Django app relies on environment variables defined in a .env file. If this is your first time setting up the project, copy .env_SAMPLE to .env and then source that file: cp -a .env_SAMPLE .env source .env Each time you start a new session for working on this project, you'll need to get those environment variables and get your virtualenv running again. If you setup Autoenv earlier, this will happen for you automatically when you cd into the project directory. If you prefer not to use Autoenv, just be sure to source .env every time you start a new session of work on the project. Install Postgres \u00b6 If you're on a Mac and use Homebrew, you can easily install Postgres: brew install postgresql Once it's installed, you can configure it to run as a service: brew services start postgresql Then create the database and associated user: dropdb --if-exists cfgov dropuser --if-exists cfpb createuser cfpb createdb -O cfpb cfgov If you absolutely need to use SQLite, you'll need to update your .env file to comment out the line that specifies Postgres as the db: # export DATABASE_URL=postgres://cfpb@localhost/cfgov And then uncomment the line that tells Django to use SQLite: export DATABASE_URL=sqlite:///db.sqlite3 Run the setup script \u00b6 At this point, your machine should have everything it needs to automate the rest of the setup process. If you haven't cloned this repo yet, clone it to a local folder. Because related projects will need to be installed as siblings to this project, we recommend putting them all in their own folder, e.g., ~/Projects/cf.gov . Once cloned, from the project root ( ~/Projects/cf.gov/cfgov-refresh/ ), run this command to complete the setup process: ./setup.sh This will take several minutes, going through the steps in these scripts: frontend.sh backend.sh Once complete, you should have a fully functioning local environment, ready for you to develop against! There are some optional setup steps that you may want to perform before continuing. Want to know more about what the setup scripts are doing? Read the detailed rundown. Continue following the usage instructions . Docker-compose installation \u00b6 Tools we use for developing with Docker \u00b6 Docker : You may not need to interact directly with Docker, but you should know that it's a client/server application for managing containers (a way of running software in an isolated environment) and images (a snapshot of all of the files neccessary to run a container). Docker Compose : Compose allows you to configure and run a collection of connected containers (like a web application and its database). Docker Machine : In environments where Docker Engine is not available, Docker Machine can be used to create and manage Docker hosts on virtual machines. For more information on Docker Machine, see the 'How do I use Docker Machine' section in the usage guide . 1. Setup your Docker environment \u00b6 If you have never installed Docker before, follow the instructions here or from your operating system vendor. If you are on a mac and are unable to install the official \"Docker for Mac\" package, the quickstart instructions below might help. If you are on a machine that is already set up to run Linux docker containers, please install Docker Compose . If docker-compose ps runs without error, you can can go to step 2. Copy the .env_SAMPLE file over \u00b6 The Docker Compose setup (see docker-compose.yml ) provides the environment variables defined in .env to the container running the Django app. If this is your first time setting up the project, copy .env_SAMPLE to .env : cp -a .env_SAMPLE .env Mac + Homebrew + Docker Machine + VirtualBox quickstart \u00b6 Starting assumptions : You already have Homebrew and VirtualBox installed. You can run brew search docker without error. Install Docker, Docker Machine, and Docker Compose: brew install docker docker-compose docker-machine Then run source mac-virtualbox-init.sh to initialize your Docker Machine setup. At this point, docker-compose ps should run without error. 2. Setup your frontend environment \u00b6 Refer to the front-end dependencies described above in the standalone installation instructions . 3. Run setup \u00b6 ./setup.sh docker This will install and build the frontend and set up the docker environment. 4. Run Docker Compose for the first time \u00b6 docker-compose up This will download and/or build images, and then start the containers, as described in the docker-compose.yml file. This will take a few minutes, or longer if you are on a slow internet connection. When it's all done, you should be able to load http://localhost:8000 in your browser, and see a database error. 3. Setup the database \u00b6 Run ./shell.sh . This opens a bash shell inside your Python container. You can either load initial data per the instructions below, or load a database dump. You could save some time and effort later (if you have access to the CFPB network), by configuring a URL for database dumps in the .env file. CFGOV_PROD_DB_LOCATION=http://(rest of the URL) You can get that URL at [GHE]/CFGOV/platform/wiki/Database-downloads#resources-available-via-s3 With CFGOV_PROD_DB_LOCATION in .env you should be able to run: ./refresh-data.sh Otherwise, the instructions to load a database dump below should be enough to get you started. Once you have a database loaded, you should have a functioning copy of site working at http://localhost:8000 4. Next Steps \u00b6 See the Docker section of the usage page to continue after that. Optional steps \u00b6 Load initial data into database \u00b6 The initial-data.sh script can be used to initialize a new database to make it easy to get started working on Wagtail. This script first ensures that all migrations are applied to the database, and then does the following: Creates an admin superuser with password admin . If it doesn't already exist, creates a new Wagtail home page named CFGOV , with a slug of cfgov . Updates the default Wagtail site to use the port defined by the DJANGO_HTTP_PORT environment variable, if defined; otherwise this port is set to 80. If it doesn't already exist, creates a new wagtail-sharing SharingSite with a hostname and port defined by the DJANGO_STAGING_HOSTNAME and DJANGO_HTTP_PORT environment variables. Load a database dump \u00b6 If you're installing this fresh, the initial data you receive will not be as extensive as you'd probably like it to be. You can get a database dump by: Going to [GHE]/CFGOV/platform/wiki/Database-downloads Selecting one of the extractions and downloading the production_django.sql.gz file Run: ./refresh-data.sh /path/to/dump.sql.gz The refresh-data.sh script will apply the same changes as the initial-data.sh script described above (including setting up the admin superuser), but will not apply migrations. To apply any unapplied migrations to a database created from a dump, run: python cfgov/manage.py migrate Set variables for working with the GovDelivery API \u00b6 Uncomment and set the GovDelivery environment variables in your .env file. Note GovDelivery is a third-party web service that powers our emails. The API is used by subscribe forms on our website. Users may decide to swap this tool out for another third-party service. Install GNU gettext for Django translation support \u00b6 In order to generate Django translations as documented here , you'll need to install the GNU gettext library. On MacOS, GNU gettext is available via Homebrew: brew install gettext but it gets installed as \"keg-only\" due to conflicts with the default installation of BSD gettext. This means that GNU gettext won't be loaded in your PATH by default. To fix this, you can run brew link --force gettext to force its installation, or see brew info gettext for an alternate solution. If installed locally, you should be able to run this command successfully: $ gettext --version GNU gettext is also required to run our translation-related unit tests locally. Curious about what the setup scripts are doing? \u00b6 Here's a rundown of each of the scripts called by setup.sh and what they do. 1. frontend.sh \u00b6 Initialize project dependency directories ( init ) This script first checks for an argument passed from the command line that can trigger different options for different environments. Since you ran it with no arguments, it will set up the dev environment. It then creates a checksum for package-lock.json (if it exists) and package.json . This will be used later to determine if dependencies need to be installed. It will then set some env vars for the Node dependency directories. 1. Clean and install project dependencies ( clean_and_install ) The script will now compare the checksums to see if it needs to install dependencies, or if they are already up-to-date. If the checksums do not match, the script will empty out all installed dependencies ( clean ) so the new installation can start fresh, then install the latest requested dependencies ( install ). The devDependencies from package.json are not installed if the environment is production, and if it's the dev or test environment, it checks to see if Protractor is globally installed. Finally, it creates a new checksum for future comparisons. 1. Run tasks to build the project for distribution ( build ) Finally, the script runs yarn run gulp build to rebuild the front-end assets. It no longer cleans first, because the gulp-changed plugin prevents rebuilding assets that haven't changed since the last build. If this is the production environment, it also triggers style and script builds for ondemand and nemo , which aren't part of a standard gulp build . 2. backend.sh \u00b6 Note backend.sh is not used for our Docker setup. Confirm environment ( init ) This script first checks for an argument passed from the command line that can trigger different options for different environments. Since you ran it with no arguments, it will set up the dev environment. It will then run a script to ensure that you're in a virtualenv. If not, the script will end, to prevent you from accidentally installing your Python dependencies globally. Install project dependencies ( install ) Python dependencies are installed into the virtualenv via pip. Dependencies vary slightly depending on whether we're in dev, test, or prod.","title":"Installation"},{"location":"installation/#installation-and-configuration-for-cfgov-refresh","text":"","title":"Installation and configuration for cfgov-refresh"},{"location":"installation/#clone-the-repository","text":"Using the console, navigate to the root directory in which your projects live and clone this project's repository: git clone git@github.com:cfpb/cfgov-refresh.git cd cfgov-refresh You may also wish to fork the repository on GitHub and clone the resultant personal fork. This is advised if you are going to be doing development on cfgov-refresh and contributing to the project. There are two ways to install cfgov-refresh: Stand-alone installation Docker-compose installation","title":"Clone the repository"},{"location":"installation/#stand-alone-installation","text":"These instructions are somewhat specific to developing on macOS, but if you're familiar with other Unix-based systems, it should be fairly easy to adapt them to your needs.","title":"Stand-alone installation"},{"location":"installation/#install-system-level-requirements","text":"","title":"Install system-level requirements"},{"location":"installation/#virtualenv-virtualenvwrapper-python-modules","text":"Install virtualenv and virtualenvwrapper to be able to create a local environment for your server: pip install virtualenv virtualenvwrapper","title":"virtualenv &amp; virtualenvwrapper Python modules"},{"location":"installation/#autoenv-module","text":"This project uses a large number of environment variables. To automatically define environment variables and launch the virtualenv upon cd ing to the project folder, install Autoenv . We recommend using Homebrew : brew install autoenv After installation, Homebrew will output instructions similar to: To finish the installation, source activate.sh in your shell: source /Users/[YOUR USERNAME]/homebrew/opt/autoenv/activate.sh Run that now for your initial setup. Any time you run the project you\u2019ll need to run that last line, so if you\u2019ll be working with the project consistently, we suggest adding it to your Bash profile by running: echo 'source /Users/[YOUR USERNAME]/homebrew/opt/autoenv/activate.sh' ~/.bash_profile If you need to find this info again later, you can run: brew info autoenv Note If you use Zsh you\u2019ll need to use zsh-autoenv , but we can\u2019t provide support for issues that may arise.","title":"Autoenv module"},{"location":"installation/#front-end-dependencies","text":"The cfgov-refresh front end currently uses the following frameworks / tools: Gulp : task management for pulling in assets, linting and concatenating code, etc. Less : CSS pre-processor. Capital Framework : User interface pattern-library produced by the CFPB. Note If you\u2019re new to Capital Framework, we encourage you to start here . Install Node.js however you\u2019d like. We recommend using nvm , though. Note This project requires Node.js v8.5 or higher, and npm v5.2 or higher. Install yarn . We recommend using Homebrew : # Use --ignore-dependencies to use your system installed node version brew install yarn --ignore-dependencies","title":"Front-end dependencies"},{"location":"installation/#webfonts","text":"The site uses a proprietary licensed font, Avenir. If you want to pull this from a content delivery network (CDN), you can set the @use-font-cdn to true and rebuild the assets with yarn run gulp build . If you want to install self-hosted fonts locally, you can place the font files in static.in/cfgov-fonts/fonts/ and restart the local web server. If you are a CFPB employee, you can perform this step with: cd static.in/ git clone https://[GHE]/CFGOV/cfgov-fonts/ Where [GHE] is our GitHub Enterprise URL.","title":"Webfonts"},{"location":"installation/#set-up-your-environment","text":"The Django app relies on environment variables defined in a .env file. If this is your first time setting up the project, copy .env_SAMPLE to .env and then source that file: cp -a .env_SAMPLE .env source .env Each time you start a new session for working on this project, you'll need to get those environment variables and get your virtualenv running again. If you setup Autoenv earlier, this will happen for you automatically when you cd into the project directory. If you prefer not to use Autoenv, just be sure to source .env every time you start a new session of work on the project.","title":"Set up your environment"},{"location":"installation/#install-postgres","text":"If you're on a Mac and use Homebrew, you can easily install Postgres: brew install postgresql Once it's installed, you can configure it to run as a service: brew services start postgresql Then create the database and associated user: dropdb --if-exists cfgov dropuser --if-exists cfpb createuser cfpb createdb -O cfpb cfgov If you absolutely need to use SQLite, you'll need to update your .env file to comment out the line that specifies Postgres as the db: # export DATABASE_URL=postgres://cfpb@localhost/cfgov And then uncomment the line that tells Django to use SQLite: export DATABASE_URL=sqlite:///db.sqlite3","title":"Install Postgres"},{"location":"installation/#run-the-setup-script","text":"At this point, your machine should have everything it needs to automate the rest of the setup process. If you haven't cloned this repo yet, clone it to a local folder. Because related projects will need to be installed as siblings to this project, we recommend putting them all in their own folder, e.g., ~/Projects/cf.gov . Once cloned, from the project root ( ~/Projects/cf.gov/cfgov-refresh/ ), run this command to complete the setup process: ./setup.sh This will take several minutes, going through the steps in these scripts: frontend.sh backend.sh Once complete, you should have a fully functioning local environment, ready for you to develop against! There are some optional setup steps that you may want to perform before continuing. Want to know more about what the setup scripts are doing? Read the detailed rundown. Continue following the usage instructions .","title":"Run the setup script"},{"location":"installation/#docker-compose-installation","text":"","title":"Docker-compose installation"},{"location":"installation/#tools-we-use-for-developing-with-docker","text":"Docker : You may not need to interact directly with Docker, but you should know that it's a client/server application for managing containers (a way of running software in an isolated environment) and images (a snapshot of all of the files neccessary to run a container). Docker Compose : Compose allows you to configure and run a collection of connected containers (like a web application and its database). Docker Machine : In environments where Docker Engine is not available, Docker Machine can be used to create and manage Docker hosts on virtual machines. For more information on Docker Machine, see the 'How do I use Docker Machine' section in the usage guide .","title":"Tools we use for developing with Docker"},{"location":"installation/#1-setup-your-docker-environment","text":"If you have never installed Docker before, follow the instructions here or from your operating system vendor. If you are on a mac and are unable to install the official \"Docker for Mac\" package, the quickstart instructions below might help. If you are on a machine that is already set up to run Linux docker containers, please install Docker Compose . If docker-compose ps runs without error, you can can go to step 2.","title":"1. Setup your Docker environment"},{"location":"installation/#copy-the-env_sample-file-over","text":"The Docker Compose setup (see docker-compose.yml ) provides the environment variables defined in .env to the container running the Django app. If this is your first time setting up the project, copy .env_SAMPLE to .env : cp -a .env_SAMPLE .env","title":"Copy the .env_SAMPLE file over"},{"location":"installation/#mac-homebrew-docker-machine-virtualbox-quickstart","text":"Starting assumptions : You already have Homebrew and VirtualBox installed. You can run brew search docker without error. Install Docker, Docker Machine, and Docker Compose: brew install docker docker-compose docker-machine Then run source mac-virtualbox-init.sh to initialize your Docker Machine setup. At this point, docker-compose ps should run without error.","title":"Mac + Homebrew + Docker Machine + VirtualBox quickstart"},{"location":"installation/#2-setup-your-frontend-environment","text":"Refer to the front-end dependencies described above in the standalone installation instructions .","title":"2. Setup your frontend environment"},{"location":"installation/#3-run-setup","text":"./setup.sh docker This will install and build the frontend and set up the docker environment.","title":"3. Run setup"},{"location":"installation/#4-run-docker-compose-for-the-first-time","text":"docker-compose up This will download and/or build images, and then start the containers, as described in the docker-compose.yml file. This will take a few minutes, or longer if you are on a slow internet connection. When it's all done, you should be able to load http://localhost:8000 in your browser, and see a database error.","title":"4. Run Docker Compose for the first time"},{"location":"installation/#3-setup-the-database","text":"Run ./shell.sh . This opens a bash shell inside your Python container. You can either load initial data per the instructions below, or load a database dump. You could save some time and effort later (if you have access to the CFPB network), by configuring a URL for database dumps in the .env file. CFGOV_PROD_DB_LOCATION=http://(rest of the URL) You can get that URL at [GHE]/CFGOV/platform/wiki/Database-downloads#resources-available-via-s3 With CFGOV_PROD_DB_LOCATION in .env you should be able to run: ./refresh-data.sh Otherwise, the instructions to load a database dump below should be enough to get you started. Once you have a database loaded, you should have a functioning copy of site working at http://localhost:8000","title":"3. Setup the database"},{"location":"installation/#4-next-steps","text":"See the Docker section of the usage page to continue after that.","title":"4. Next Steps"},{"location":"installation/#optional-steps","text":"","title":"Optional steps"},{"location":"installation/#load-initial-data-into-database","text":"The initial-data.sh script can be used to initialize a new database to make it easy to get started working on Wagtail. This script first ensures that all migrations are applied to the database, and then does the following: Creates an admin superuser with password admin . If it doesn't already exist, creates a new Wagtail home page named CFGOV , with a slug of cfgov . Updates the default Wagtail site to use the port defined by the DJANGO_HTTP_PORT environment variable, if defined; otherwise this port is set to 80. If it doesn't already exist, creates a new wagtail-sharing SharingSite with a hostname and port defined by the DJANGO_STAGING_HOSTNAME and DJANGO_HTTP_PORT environment variables.","title":"Load initial data into database"},{"location":"installation/#load-a-database-dump","text":"If you're installing this fresh, the initial data you receive will not be as extensive as you'd probably like it to be. You can get a database dump by: Going to [GHE]/CFGOV/platform/wiki/Database-downloads Selecting one of the extractions and downloading the production_django.sql.gz file Run: ./refresh-data.sh /path/to/dump.sql.gz The refresh-data.sh script will apply the same changes as the initial-data.sh script described above (including setting up the admin superuser), but will not apply migrations. To apply any unapplied migrations to a database created from a dump, run: python cfgov/manage.py migrate","title":"Load a database dump"},{"location":"installation/#set-variables-for-working-with-the-govdelivery-api","text":"Uncomment and set the GovDelivery environment variables in your .env file. Note GovDelivery is a third-party web service that powers our emails. The API is used by subscribe forms on our website. Users may decide to swap this tool out for another third-party service.","title":"Set variables for working with the GovDelivery API"},{"location":"installation/#install-gnu-gettext-for-django-translation-support","text":"In order to generate Django translations as documented here , you'll need to install the GNU gettext library. On MacOS, GNU gettext is available via Homebrew: brew install gettext but it gets installed as \"keg-only\" due to conflicts with the default installation of BSD gettext. This means that GNU gettext won't be loaded in your PATH by default. To fix this, you can run brew link --force gettext to force its installation, or see brew info gettext for an alternate solution. If installed locally, you should be able to run this command successfully: $ gettext --version GNU gettext is also required to run our translation-related unit tests locally.","title":"Install GNU gettext for Django translation support"},{"location":"installation/#curious-about-what-the-setup-scripts-are-doing","text":"Here's a rundown of each of the scripts called by setup.sh and what they do.","title":"Curious about what the setup scripts are doing?"},{"location":"installation/#1-frontendsh","text":"Initialize project dependency directories ( init ) This script first checks for an argument passed from the command line that can trigger different options for different environments. Since you ran it with no arguments, it will set up the dev environment. It then creates a checksum for package-lock.json (if it exists) and package.json . This will be used later to determine if dependencies need to be installed. It will then set some env vars for the Node dependency directories. 1. Clean and install project dependencies ( clean_and_install ) The script will now compare the checksums to see if it needs to install dependencies, or if they are already up-to-date. If the checksums do not match, the script will empty out all installed dependencies ( clean ) so the new installation can start fresh, then install the latest requested dependencies ( install ). The devDependencies from package.json are not installed if the environment is production, and if it's the dev or test environment, it checks to see if Protractor is globally installed. Finally, it creates a new checksum for future comparisons. 1. Run tasks to build the project for distribution ( build ) Finally, the script runs yarn run gulp build to rebuild the front-end assets. It no longer cleans first, because the gulp-changed plugin prevents rebuilding assets that haven't changed since the last build. If this is the production environment, it also triggers style and script builds for ondemand and nemo , which aren't part of a standard gulp build .","title":"1. frontend.sh"},{"location":"installation/#2-backendsh","text":"Note backend.sh is not used for our Docker setup. Confirm environment ( init ) This script first checks for an argument passed from the command line that can trigger different options for different environments. Since you ran it with no arguments, it will set up the dev environment. It will then run a script to ensure that you're in a virtualenv. If not, the script will end, to prevent you from accidentally installing your Python dependencies globally. Install project dependencies ( install ) Python dependencies are installed into the virtualenv via pip. Dependencies vary slightly depending on whether we're in dev, test, or prod.","title":"2. backend.sh"},{"location":"javascript-unit-tests/","text":"JavaScript Unit Tests \u00b6 This page provides instructions for writing and running JavaScript (JS) unit tests in cfgov-refresh. Jest is the framework we use for writing and running JavaScript unit tests. If you\u2019re not familiar with it, it would be a good idea to peruse their docs before diving in here. Table of contents \u00b6 Running unit tests Running a single test file Run a directory of unit tests Run all unit tests Where to find tests Test-driven development Setting up tests New test file from sample Folder structure (where to put your JavaScript and tests) First test run File structure (basic layout of a test file) Providing test data Common test patterns Testing a basic function Testing DOM manipulation Testing browser state Testing user interaction Running unit tests \u00b6 Run a single test file \u00b6 To run a single test file, use the --specs flag to specify a file path: gulp test:unit --specs=js/organisms/Footer-spec.js The above command tests the code at cfgov/unprocessed/js/organisms/Footer.js . Run a directory of unit tests \u00b6 A directory of unit tests can be run with: gulp test:unit --specs=js/molecules/ Run all unit tests \u00b6 To run all of the unit tests: gulp test:unit Where to find tests \u00b6 The following links list out the main directories containing tests (as of January 2019, this page\u2019s initial publication date). All unit tests Tests for individual apps Tests for regular modules Tests for molecules Tests for organisms Test-driven development \u00b6 We recommend using test-driven development (TDD) when coding new JavaScript. The general concept is to start by writing your test first , with the expected behavior and functionality well-described, then you write the code that makes the test pass. A good pithy summary is: Write only enough of a unit test to fail. Write only enough production code to make the failing unit test pass. Then repeat that process until you have written all of the code you need. Read this primer on test-driven development to learn more about how it differs from the typical approach to programming and unit tests. Setting up tests \u00b6 New test file from sample \u00b6 For this guide, we\u2019ll use very basic sample code files to illustrate how to use the test framework in cfgov-refresh and how to test very common code patterns. Another common approach is to look for existing tests that are testing something similar to what you are writing now. Feel free to do so and copy from an existing module and its tests instead of copying the sample files referenced below. For links to existing tests, refer back to the \u201cWhere to find tests\u201d section . Now, let\u2019s begin! Let\u2019s make a new unit test fail, then we will make it pass, following the principles of TDD. Copy the sample test file to a new location by running this command from the root of cfgov-refresh: cp docs/samples/sample-spec.js test/unit_tests/js/modules/ Copy the sample module file to a new location by running this command: cp docs/samples/sample.js cfgov/unprocessed/js/modules/ Test file names should always match what they are testing, with the addition of a -spec suffix. Folder structure (where to put your JavaScript and tests) \u00b6 JavaScript unit test files belong in the test/unit_tests/ directory. The folder structure of the test files mirrors the structure of the project JavaScript in cfgov/unprocessed/js/ . When considering exactly where to place JavaScript in these directories, it might be helpful to review the documentation about atomic components in cfgov-refresh . JavaScript corresponding to atomic elements should go into the appropriate subfolder for the type of element being implemented. In our case, sample.js and sample-spec.js don\u2019t relate to atomic elements, so they can be placed into the uncategorized modules subfolders: cfgov/unprocessed/js/modules and test/unit_tests/modules , respectively. Child apps If you\u2019re working on something in a child app, put it in test/unit_test/appname/js/ . Otherwise, if you\u2019re working on something that belongs to cfgov-refresh generally, it should go in the corresponding folder under test/unit_test/js/ . First test run \u00b6 Now that you have your sample JS and test files in the right places, let\u2019s try running them and see what happens! I\u2019ll refer to sample-spec.js and sample.js in the instructions below, but you should work in your own new test file and JavaScript file to save and commit your changes. Edit line 6 of your spec file and remove the call to the .skip method. The line should now read: it( 'should return a string with expected value', () = { \u2026 } ); Run your sample test using gulp test:unit --specs=js/modules/sample-spec.js (substituting your own filename). You should see output like this: The test should fail \u2013 this is expected. Remember, when doing TDD, we want to write our test to fail first, then write the corresponding JavaScript that will make the test pass. Make the test pass by changing your script\u2019s line 7 ( see sample.js ) to the following: return 'Shredder'; Run the test again to confirm the test now passes. You should see output like this: Doesn\u2019t it feel good? Refer back to the \u201cRunning unit tests\u201d section for additional commands to run tests. File structure (basic layout of a test file) \u00b6 In order to make the sample-spec.js more meaningful to your own use case, you\u2019ll need to know how to structure a unit test using Jest methods. Let\u2019s take a look at the structure of our very basic sample test file. Loading dependencies \u00b6 Line 1 of any spec file will use an import statement to include the JavaScript file that you are testing. Additional dependencies should be added in the same manner. import sample from '../../../../cfgov/unprocessed/js/modules/sample.js'; Some test files use const declarations to require scripts instead of import , because those files were written before import was available. We prefer to use import because it allows for tree shaking in Webpack , meaning if two modules are importing the same module it should only be included in the bundle once, whereas with require it would be included twice. A consequence is that variables can\u2019t be used in the import path, as they prevent Webpack from figuring out which modules are duplicates. For example, this snippet shows how a require statement should be converted to an import statement, but without including the BASE_JS_PATH variable in the file path: // This works, but could duplicate footer-button.js, if other files also require it. const FooterButton = require( BASE_JS_PATH + 'modules/footer-button.js' ); // This doesn't work and the build will fail. import * as FooterButton from BASE_JS_PATH + 'modules/footer-button.js'; // This is ugly, but it works and supports tree shaking. import * as FooterButton from '../../../../cfgov/unprocessed/js/modules/footer-button.js'; import also provides a benefit in that you can choose specific parts of a module so that you only import the dependencies you need. For testing purposes, we will typically import the whole module to make sure we have full test coverage. Read the import reference guide on MDN on how to implement import for different use cases. The describe function \u00b6 In Jest (whose syntax is based on Jasmine), describe blocks serve as organizational structures that you can use to outline the methods you need in your JS module. The root describe method is where we put the name of the JavaScript module we are testing. For the sample, the name of our module is sample , so we set this up on line 4 of sample-spec.js : describe( 'sample', () = { \u2026 } ); This module name will appear in your test output in the console when the test is running: More complex tests will have additional describe blocks \u2013 children of the root describe block \u2013 that should correspond to a particular method in the module. For example, if we want to add more functionality to our sample JS, we could start by writing these tests in sample-spec.js : describe( 'sample', () = { describe( 'gimmeString()', () = { it( 'should return a string with expected value', () = { const sampleString = 'Shredder'; expect( sample.gimmeString() ).toBe( sampleString ); } ); } ); describe( 'gimmeObject()', () = { it( 'should return an object with expected value', () = { const sampleObject = { image: 'https://vignette.wikia.nocookie.net/tmnt/images/0/00/Krangnobody.png', caption: 'Krang portrait' }; expect( sample.gimmeObject() ).toBe( sampleObject ); } ); } ); } ); (We\u2019ll talk more about writing the individual tests in the next section.) And then we would create the gimmeString and gimmeObject methods in our sample.js file. Another example is breakpoint-state-spec.js , which tests 2 methods, get and isInDesktop , on the module breakpoint-state . When using TDD, you may prefer to add describe blocks later, during the refactor stage of writing code. Individual tests \u00b6 Within a describe block, individual tests are encapsulated by it methods, which is an alias of Jest\u2019s test method . Each test must include one or more assertions (usually only one) that confirm that the result of executing some code is what you expected. These are called \u201cmatchers\u201d in Jest parlance, and they all follow this format: expect( someValue ).someKindOfComparisonWith( someOtherValue ); For example, let\u2019s take another look at the sample tests we wrote above: describe( 'sample', () = { describe( 'gimmeString()', () = { it( 'should return a string with expected value', () = { const sampleString = 'Shredder'; expect( sample.gimmeString() ).toBe( sampleString ); } ); } ); describe( 'gimmeObject()', () = { it( 'should return an object with expected value', () = { const sampleObject = { image: 'https://vignette.wikia.nocookie.net/tmnt/images/0/00/Krangnobody.png', caption: 'Krang portrait' }; expect( sample.gimmeObject() ).toBe( sampleObject ); } ); } ); } ); In these tests, we check ( expect ) that the string returned by sample.gimmeString() and the object returned by sample.gimmeObject() match ( toBe ) the sampleString and sampleObject that we defined in the tests. There are many kinds of assertions you can use besides the simple equality comparison of toBe . See the Jest Using Matchers guide for a primer on them and the Jest expect API docs for the full list of its matchers. Providing test data \u00b6 The first principle of test data for unit tests is to keep test data as simple as possible \u2013 use the minimum needed to test the code. Direct definition of test data \u00b6 The simplest way to set up test data is to declare it as variables within each test, e.g., the tests in strings-spec.js . This can include HTML markup for DOM manipulation tests, if each test requires different markup. Setup and teardown methods \u00b6 If you will need to leverage the same test data across different tests, Jest has setup and teardown methods, such as beforeEach and afterEach , or beforeAll and afterAll , which can be used to performing actions that are needed before and after running all tests or each test in a suite. For example, the tests in Analytics-spec.js use both beforeAll and beforeEach inside the root describe block to do a variable definition for all tests at the beginning of the suite and reset the dataLayer before each test, respectively. Check out the Jest documentation on \u201cSetup and teardown\u201d methods. A common structure when the DOM is involved is to create a constant representing an HTML snippet to test, then \u2013 in a beforeEach or beforeAll (depending on whether the tests modify the markup or not) \u2013 set document.body.innerHTML to that snippet. Use beforeAll to attach HTML markup that is unaffected by the tests, e.g., the tests in footer-button-spec.js . Use beforeEach to reset manipulated markup between tests, e.g., the tests in Notification-spec.js . See \u201cTesting DOM manipulation\u201d in the \u201cCommon test patterns\u201d section of this page for a more in-depth discussion of this scenario. Common test patterns \u00b6 Testing a basic function \u00b6 Testing simple functions is pretty straightforward. Each function in a module should have tests set up as a child describe within the module\u2019s own describe . Then, write a number of it statements in prose that describe how the function should respond to various kinds of input. Inside each it , invoke the function with the input described in the it statement and use expect to check that you receive the desired result. Here is a simple example from our array helpers module ( cfgov/unprocessed/js/modules/util/array-helpers.js ): function indexOfObject( array, key, val ) { let match = -1; if ( !array.length 0 ) { return match; } array.forEach( function( item, index ) { if ( item[key] === val ) { match = index; } } ); return match; } Tests for that function, from test/unit_tests/js/modules/util/array-helpers-spec.js : describe( 'indexOfObject()', () = { it( 'should return -1 if the array is empty', () = { array = []; index = arrayHelpers.indexOfObject( array, 'foo' ); expect( index ).toBe( -1 ); } ); it( 'should return -1 if there is no match', () = { array = [ { value: 'bar' }, { value: 'baz' } ]; index = arrayHelpers.indexOfObject( array, 'value', 'foo' ); expect( index ).toBe( -1 ); } ); it( 'should return the matched index', () = { array = [ { value: 'foo' }, { value: 'bar' }, { value: 'baz' } ]; index = arrayHelpers.indexOfObject( array, 'value', 'foo' ); expect( index ).toBe( 0 ); } ); } ); Testing DOM manipulation \u00b6 Jest , the JavaScript testing framework we use, includes jsdom , which simulates a DOM environment as if you were in the browser. This means that we can call any DOM API in our test code and observe it in the same way as we do in the module code itself, which acts on the browser\u2019s DOM. As an example, let\u2019s look at our Notification component. The Notification component uses a common set of markup with different classes and SVG icon code to style it as a particular kind of notification (success, warning, etc.). In the component JS , we have this function that sets the type of a notification before displaying it: function _setType( type ) { // If type hasn't changed, return. if ( _currentType === type ) { return this; } // Remove existing type class const classList = _dom.classList; classList.remove( `${ BASE_CLASS }__${ _currentType }` ); if ( type === SUCCESS || type === WARNING || type === ERROR ) { // Add new type class and update the value of _currentType classList.add( `${ BASE_CLASS }__${ type }` ); _currentType = type; // Replace svg element with contents of type_ICON const currentIcon = _dom.querySelector( '.cf-icon-svg' ); const newIconSetup = document.createElement( 'div' ); newIconSetup.innerHTML = ICON[type]; const newIcon = newIconSetup.firstChild; _dom.replaceChild( newIcon, currentIcon ); } else { throw new Error( `${ type } is not a supported notification type!` ); } return this; } This function would be invoked by an instance of the Notification class. _dom is the DOM node for the Notification. As you can see from the code comments above, it has a few different steps that modify the DOM node. Now let\u2019s look at the tests. Here are the first 22 lines of the spec file that tests this component: import Notification from '../../../../cfgov/unprocessed/js/molecules/Notification'; const BASE_CLASS = 'm-notification'; const HTML_SNIPPET = ` div class= m-notification m-notification__default svg xmlns= http://www.w3.org/2000/svg viewBox= 0 0 1000 1200 class= cf-icon-svg /svg div class= m-notification_content div class= h4 m-notification_message Notification content /div /div /div `; describe( 'Notification', () = { let notificationElem; let notification; let thisNotification; beforeEach( () = { document.body.innerHTML = HTML_SNIPPET; notificationElem = document.querySelector( `.${ BASE_CLASS }` ); notification = new Notification( notificationElem, BASE_CLASS, {} ); } ); \u2026 } ); The main things to note here at the beginning of the file are the addition of the HTML_SNIPPET definition, containing the markup we will used for testing as it would be rendered for this component, and the beforeEach function that (1) uses jsdom to add that snippet to the test environment and assigns the component node to the notificationElem variable, and (2) creates a new instance of the Notification class. A word about HTML_SNIPPET s Right now it\u2019s possible to update a component\u2019s Jinja template, forget to update the corresponding JavaScript, and the unit tests would still pass, because they're using their own HTML_SNIPPET . It would be preferable if we had a canonical component markup template that is pulled in by the application, the unit tests, and the docs. We haven\u2019t yet figured out how to do this, since our component templates contain Jinja tags that the tests would have to reconcile into a complete, finished chunk of markup. For now, just be aware of this when editing a Wagtail component that includes JavaScript . Further down, here are some of the tests that cover the _setType function (by way of the setTypeAndContent function that wraps both _setType and _setContent ): describe( 'setTypeAndContent()', () = { it( 'should update the notification type for the success state', () = { notification.init(); notification.setTypeAndContent( notification.SUCCESS, '' ); expect( notificationElem.classList ).toContain( 'm-notification__success' ); } ); it( 'should update the notification type for the warning state', () = { notification.init(); notification.setTypeAndContent( notification.WARNING, '' ); expect( notificationElem.classList ).toContain( 'm-notification__warning' ); } ); \u2026 } ); This part mostly works like testing any other function. The notable distinction here is that the test invokes the function using the DOM nodes and class set up in beforeEach . Testing browser state \u00b6 Another common thing to test is code that interacts with the state of the browser itself, e.g., fragment identifiers, query strings, or other things in the URL; the window object; session storage; page history; etc. One way of doing this is to create a spy (a special kind of mocked function) that watches for browser API calls to be made a certain number of times or with a specific payload. One example is found in the tests for our full-table-row-linking code . In the module code ( o-table-row-links.js ), if an event listener detects a click anywhere on one of these special table rows, it invokes window.location to send the browser to the href of the first link in that row: window.location.assign( target.querySelector( 'a' ).getAttribute( 'href' ) ); To test this, in the aforementioned o-table-row-links-spec.js file, we first set up a standard Jest mock for window.location.assign , and then create our spy to watch it: describe( 'o-table-row-links', () = { beforeEach( () = { window.location.assign = jest.fn(); locationSpy = jest.spyOn( window.location, 'assign' ); \u2026 } ); \u2026 } ); A little further down (after finishing the DOM setup and initializing the module we\u2019re testing), we have three tests that simulate clicks and then assert things that the spy can answer for us: whether it was called with a particular location parameter, and that it was called a specific number of times (zero). it( 'should navigate to new location when link row cell clicked', () = { simulateEvent( 'click', linkRowCellDom ); expect( locationSpy ).toBeCalledWith( 'https://www.example.com' ); } ); it( 'should not set window location when link is clicked', () = { simulateEvent( 'click', linkDom ); expect( locationSpy ).toHaveBeenCalledTimes( 0 ); } ); it( 'should not navigate to new location when non link row cell clicked', () = { simulateEvent( 'click', nonLinkRowCellDom ); expect( locationSpy ).toHaveBeenCalledTimes( 0 ); } ); Testing user interaction \u00b6 Testing user interaction with simulated pointer events, keystrokes, or form submissions is best handled via browser tests, not unit tests. User interaction in a unit test could falsely pass if the component wasn\u2019t visible on the page, for instance. Read more about how we run browser tests.","title":"JavaScript Unit Testing"},{"location":"javascript-unit-tests/#javascript-unit-tests","text":"This page provides instructions for writing and running JavaScript (JS) unit tests in cfgov-refresh. Jest is the framework we use for writing and running JavaScript unit tests. If you\u2019re not familiar with it, it would be a good idea to peruse their docs before diving in here.","title":"JavaScript Unit Tests"},{"location":"javascript-unit-tests/#table-of-contents","text":"Running unit tests Running a single test file Run a directory of unit tests Run all unit tests Where to find tests Test-driven development Setting up tests New test file from sample Folder structure (where to put your JavaScript and tests) First test run File structure (basic layout of a test file) Providing test data Common test patterns Testing a basic function Testing DOM manipulation Testing browser state Testing user interaction","title":"Table of contents"},{"location":"javascript-unit-tests/#running-unit-tests","text":"","title":"Running unit tests"},{"location":"javascript-unit-tests/#run-a-single-test-file","text":"To run a single test file, use the --specs flag to specify a file path: gulp test:unit --specs=js/organisms/Footer-spec.js The above command tests the code at cfgov/unprocessed/js/organisms/Footer.js .","title":"Run a single test file"},{"location":"javascript-unit-tests/#run-a-directory-of-unit-tests","text":"A directory of unit tests can be run with: gulp test:unit --specs=js/molecules/","title":"Run a directory of unit tests"},{"location":"javascript-unit-tests/#run-all-unit-tests","text":"To run all of the unit tests: gulp test:unit","title":"Run all unit tests"},{"location":"javascript-unit-tests/#where-to-find-tests","text":"The following links list out the main directories containing tests (as of January 2019, this page\u2019s initial publication date). All unit tests Tests for individual apps Tests for regular modules Tests for molecules Tests for organisms","title":"Where to find tests"},{"location":"javascript-unit-tests/#test-driven-development","text":"We recommend using test-driven development (TDD) when coding new JavaScript. The general concept is to start by writing your test first , with the expected behavior and functionality well-described, then you write the code that makes the test pass. A good pithy summary is: Write only enough of a unit test to fail. Write only enough production code to make the failing unit test pass. Then repeat that process until you have written all of the code you need. Read this primer on test-driven development to learn more about how it differs from the typical approach to programming and unit tests.","title":"Test-driven development"},{"location":"javascript-unit-tests/#setting-up-tests","text":"","title":"Setting up tests"},{"location":"javascript-unit-tests/#new-test-file-from-sample","text":"For this guide, we\u2019ll use very basic sample code files to illustrate how to use the test framework in cfgov-refresh and how to test very common code patterns. Another common approach is to look for existing tests that are testing something similar to what you are writing now. Feel free to do so and copy from an existing module and its tests instead of copying the sample files referenced below. For links to existing tests, refer back to the \u201cWhere to find tests\u201d section . Now, let\u2019s begin! Let\u2019s make a new unit test fail, then we will make it pass, following the principles of TDD. Copy the sample test file to a new location by running this command from the root of cfgov-refresh: cp docs/samples/sample-spec.js test/unit_tests/js/modules/ Copy the sample module file to a new location by running this command: cp docs/samples/sample.js cfgov/unprocessed/js/modules/ Test file names should always match what they are testing, with the addition of a -spec suffix.","title":"New test file from sample"},{"location":"javascript-unit-tests/#folder-structure-where-to-put-your-javascript-and-tests","text":"JavaScript unit test files belong in the test/unit_tests/ directory. The folder structure of the test files mirrors the structure of the project JavaScript in cfgov/unprocessed/js/ . When considering exactly where to place JavaScript in these directories, it might be helpful to review the documentation about atomic components in cfgov-refresh . JavaScript corresponding to atomic elements should go into the appropriate subfolder for the type of element being implemented. In our case, sample.js and sample-spec.js don\u2019t relate to atomic elements, so they can be placed into the uncategorized modules subfolders: cfgov/unprocessed/js/modules and test/unit_tests/modules , respectively. Child apps If you\u2019re working on something in a child app, put it in test/unit_test/appname/js/ . Otherwise, if you\u2019re working on something that belongs to cfgov-refresh generally, it should go in the corresponding folder under test/unit_test/js/ .","title":"Folder structure (where to put your JavaScript and tests)"},{"location":"javascript-unit-tests/#first-test-run","text":"Now that you have your sample JS and test files in the right places, let\u2019s try running them and see what happens! I\u2019ll refer to sample-spec.js and sample.js in the instructions below, but you should work in your own new test file and JavaScript file to save and commit your changes. Edit line 6 of your spec file and remove the call to the .skip method. The line should now read: it( 'should return a string with expected value', () = { \u2026 } ); Run your sample test using gulp test:unit --specs=js/modules/sample-spec.js (substituting your own filename). You should see output like this: The test should fail \u2013 this is expected. Remember, when doing TDD, we want to write our test to fail first, then write the corresponding JavaScript that will make the test pass. Make the test pass by changing your script\u2019s line 7 ( see sample.js ) to the following: return 'Shredder'; Run the test again to confirm the test now passes. You should see output like this: Doesn\u2019t it feel good? Refer back to the \u201cRunning unit tests\u201d section for additional commands to run tests.","title":"First test run"},{"location":"javascript-unit-tests/#file-structure-basic-layout-of-a-test-file","text":"In order to make the sample-spec.js more meaningful to your own use case, you\u2019ll need to know how to structure a unit test using Jest methods. Let\u2019s take a look at the structure of our very basic sample test file.","title":"File structure (basic layout of a test file)"},{"location":"javascript-unit-tests/#loading-dependencies","text":"Line 1 of any spec file will use an import statement to include the JavaScript file that you are testing. Additional dependencies should be added in the same manner. import sample from '../../../../cfgov/unprocessed/js/modules/sample.js'; Some test files use const declarations to require scripts instead of import , because those files were written before import was available. We prefer to use import because it allows for tree shaking in Webpack , meaning if two modules are importing the same module it should only be included in the bundle once, whereas with require it would be included twice. A consequence is that variables can\u2019t be used in the import path, as they prevent Webpack from figuring out which modules are duplicates. For example, this snippet shows how a require statement should be converted to an import statement, but without including the BASE_JS_PATH variable in the file path: // This works, but could duplicate footer-button.js, if other files also require it. const FooterButton = require( BASE_JS_PATH + 'modules/footer-button.js' ); // This doesn't work and the build will fail. import * as FooterButton from BASE_JS_PATH + 'modules/footer-button.js'; // This is ugly, but it works and supports tree shaking. import * as FooterButton from '../../../../cfgov/unprocessed/js/modules/footer-button.js'; import also provides a benefit in that you can choose specific parts of a module so that you only import the dependencies you need. For testing purposes, we will typically import the whole module to make sure we have full test coverage. Read the import reference guide on MDN on how to implement import for different use cases.","title":"Loading dependencies"},{"location":"javascript-unit-tests/#the-describe-function","text":"In Jest (whose syntax is based on Jasmine), describe blocks serve as organizational structures that you can use to outline the methods you need in your JS module. The root describe method is where we put the name of the JavaScript module we are testing. For the sample, the name of our module is sample , so we set this up on line 4 of sample-spec.js : describe( 'sample', () = { \u2026 } ); This module name will appear in your test output in the console when the test is running: More complex tests will have additional describe blocks \u2013 children of the root describe block \u2013 that should correspond to a particular method in the module. For example, if we want to add more functionality to our sample JS, we could start by writing these tests in sample-spec.js : describe( 'sample', () = { describe( 'gimmeString()', () = { it( 'should return a string with expected value', () = { const sampleString = 'Shredder'; expect( sample.gimmeString() ).toBe( sampleString ); } ); } ); describe( 'gimmeObject()', () = { it( 'should return an object with expected value', () = { const sampleObject = { image: 'https://vignette.wikia.nocookie.net/tmnt/images/0/00/Krangnobody.png', caption: 'Krang portrait' }; expect( sample.gimmeObject() ).toBe( sampleObject ); } ); } ); } ); (We\u2019ll talk more about writing the individual tests in the next section.) And then we would create the gimmeString and gimmeObject methods in our sample.js file. Another example is breakpoint-state-spec.js , which tests 2 methods, get and isInDesktop , on the module breakpoint-state . When using TDD, you may prefer to add describe blocks later, during the refactor stage of writing code.","title":"The describe function"},{"location":"javascript-unit-tests/#individual-tests","text":"Within a describe block, individual tests are encapsulated by it methods, which is an alias of Jest\u2019s test method . Each test must include one or more assertions (usually only one) that confirm that the result of executing some code is what you expected. These are called \u201cmatchers\u201d in Jest parlance, and they all follow this format: expect( someValue ).someKindOfComparisonWith( someOtherValue ); For example, let\u2019s take another look at the sample tests we wrote above: describe( 'sample', () = { describe( 'gimmeString()', () = { it( 'should return a string with expected value', () = { const sampleString = 'Shredder'; expect( sample.gimmeString() ).toBe( sampleString ); } ); } ); describe( 'gimmeObject()', () = { it( 'should return an object with expected value', () = { const sampleObject = { image: 'https://vignette.wikia.nocookie.net/tmnt/images/0/00/Krangnobody.png', caption: 'Krang portrait' }; expect( sample.gimmeObject() ).toBe( sampleObject ); } ); } ); } ); In these tests, we check ( expect ) that the string returned by sample.gimmeString() and the object returned by sample.gimmeObject() match ( toBe ) the sampleString and sampleObject that we defined in the tests. There are many kinds of assertions you can use besides the simple equality comparison of toBe . See the Jest Using Matchers guide for a primer on them and the Jest expect API docs for the full list of its matchers.","title":"Individual tests"},{"location":"javascript-unit-tests/#providing-test-data","text":"The first principle of test data for unit tests is to keep test data as simple as possible \u2013 use the minimum needed to test the code.","title":"Providing test data"},{"location":"javascript-unit-tests/#direct-definition-of-test-data","text":"The simplest way to set up test data is to declare it as variables within each test, e.g., the tests in strings-spec.js . This can include HTML markup for DOM manipulation tests, if each test requires different markup.","title":"Direct definition of test data"},{"location":"javascript-unit-tests/#setup-and-teardown-methods","text":"If you will need to leverage the same test data across different tests, Jest has setup and teardown methods, such as beforeEach and afterEach , or beforeAll and afterAll , which can be used to performing actions that are needed before and after running all tests or each test in a suite. For example, the tests in Analytics-spec.js use both beforeAll and beforeEach inside the root describe block to do a variable definition for all tests at the beginning of the suite and reset the dataLayer before each test, respectively. Check out the Jest documentation on \u201cSetup and teardown\u201d methods. A common structure when the DOM is involved is to create a constant representing an HTML snippet to test, then \u2013 in a beforeEach or beforeAll (depending on whether the tests modify the markup or not) \u2013 set document.body.innerHTML to that snippet. Use beforeAll to attach HTML markup that is unaffected by the tests, e.g., the tests in footer-button-spec.js . Use beforeEach to reset manipulated markup between tests, e.g., the tests in Notification-spec.js . See \u201cTesting DOM manipulation\u201d in the \u201cCommon test patterns\u201d section of this page for a more in-depth discussion of this scenario.","title":"Setup and teardown methods"},{"location":"javascript-unit-tests/#common-test-patterns","text":"","title":"Common test patterns"},{"location":"javascript-unit-tests/#testing-a-basic-function","text":"Testing simple functions is pretty straightforward. Each function in a module should have tests set up as a child describe within the module\u2019s own describe . Then, write a number of it statements in prose that describe how the function should respond to various kinds of input. Inside each it , invoke the function with the input described in the it statement and use expect to check that you receive the desired result. Here is a simple example from our array helpers module ( cfgov/unprocessed/js/modules/util/array-helpers.js ): function indexOfObject( array, key, val ) { let match = -1; if ( !array.length 0 ) { return match; } array.forEach( function( item, index ) { if ( item[key] === val ) { match = index; } } ); return match; } Tests for that function, from test/unit_tests/js/modules/util/array-helpers-spec.js : describe( 'indexOfObject()', () = { it( 'should return -1 if the array is empty', () = { array = []; index = arrayHelpers.indexOfObject( array, 'foo' ); expect( index ).toBe( -1 ); } ); it( 'should return -1 if there is no match', () = { array = [ { value: 'bar' }, { value: 'baz' } ]; index = arrayHelpers.indexOfObject( array, 'value', 'foo' ); expect( index ).toBe( -1 ); } ); it( 'should return the matched index', () = { array = [ { value: 'foo' }, { value: 'bar' }, { value: 'baz' } ]; index = arrayHelpers.indexOfObject( array, 'value', 'foo' ); expect( index ).toBe( 0 ); } ); } );","title":"Testing a basic function"},{"location":"javascript-unit-tests/#testing-dom-manipulation","text":"Jest , the JavaScript testing framework we use, includes jsdom , which simulates a DOM environment as if you were in the browser. This means that we can call any DOM API in our test code and observe it in the same way as we do in the module code itself, which acts on the browser\u2019s DOM. As an example, let\u2019s look at our Notification component. The Notification component uses a common set of markup with different classes and SVG icon code to style it as a particular kind of notification (success, warning, etc.). In the component JS , we have this function that sets the type of a notification before displaying it: function _setType( type ) { // If type hasn't changed, return. if ( _currentType === type ) { return this; } // Remove existing type class const classList = _dom.classList; classList.remove( `${ BASE_CLASS }__${ _currentType }` ); if ( type === SUCCESS || type === WARNING || type === ERROR ) { // Add new type class and update the value of _currentType classList.add( `${ BASE_CLASS }__${ type }` ); _currentType = type; // Replace svg element with contents of type_ICON const currentIcon = _dom.querySelector( '.cf-icon-svg' ); const newIconSetup = document.createElement( 'div' ); newIconSetup.innerHTML = ICON[type]; const newIcon = newIconSetup.firstChild; _dom.replaceChild( newIcon, currentIcon ); } else { throw new Error( `${ type } is not a supported notification type!` ); } return this; } This function would be invoked by an instance of the Notification class. _dom is the DOM node for the Notification. As you can see from the code comments above, it has a few different steps that modify the DOM node. Now let\u2019s look at the tests. Here are the first 22 lines of the spec file that tests this component: import Notification from '../../../../cfgov/unprocessed/js/molecules/Notification'; const BASE_CLASS = 'm-notification'; const HTML_SNIPPET = ` div class= m-notification m-notification__default svg xmlns= http://www.w3.org/2000/svg viewBox= 0 0 1000 1200 class= cf-icon-svg /svg div class= m-notification_content div class= h4 m-notification_message Notification content /div /div /div `; describe( 'Notification', () = { let notificationElem; let notification; let thisNotification; beforeEach( () = { document.body.innerHTML = HTML_SNIPPET; notificationElem = document.querySelector( `.${ BASE_CLASS }` ); notification = new Notification( notificationElem, BASE_CLASS, {} ); } ); \u2026 } ); The main things to note here at the beginning of the file are the addition of the HTML_SNIPPET definition, containing the markup we will used for testing as it would be rendered for this component, and the beforeEach function that (1) uses jsdom to add that snippet to the test environment and assigns the component node to the notificationElem variable, and (2) creates a new instance of the Notification class. A word about HTML_SNIPPET s Right now it\u2019s possible to update a component\u2019s Jinja template, forget to update the corresponding JavaScript, and the unit tests would still pass, because they're using their own HTML_SNIPPET . It would be preferable if we had a canonical component markup template that is pulled in by the application, the unit tests, and the docs. We haven\u2019t yet figured out how to do this, since our component templates contain Jinja tags that the tests would have to reconcile into a complete, finished chunk of markup. For now, just be aware of this when editing a Wagtail component that includes JavaScript . Further down, here are some of the tests that cover the _setType function (by way of the setTypeAndContent function that wraps both _setType and _setContent ): describe( 'setTypeAndContent()', () = { it( 'should update the notification type for the success state', () = { notification.init(); notification.setTypeAndContent( notification.SUCCESS, '' ); expect( notificationElem.classList ).toContain( 'm-notification__success' ); } ); it( 'should update the notification type for the warning state', () = { notification.init(); notification.setTypeAndContent( notification.WARNING, '' ); expect( notificationElem.classList ).toContain( 'm-notification__warning' ); } ); \u2026 } ); This part mostly works like testing any other function. The notable distinction here is that the test invokes the function using the DOM nodes and class set up in beforeEach .","title":"Testing DOM manipulation"},{"location":"javascript-unit-tests/#testing-browser-state","text":"Another common thing to test is code that interacts with the state of the browser itself, e.g., fragment identifiers, query strings, or other things in the URL; the window object; session storage; page history; etc. One way of doing this is to create a spy (a special kind of mocked function) that watches for browser API calls to be made a certain number of times or with a specific payload. One example is found in the tests for our full-table-row-linking code . In the module code ( o-table-row-links.js ), if an event listener detects a click anywhere on one of these special table rows, it invokes window.location to send the browser to the href of the first link in that row: window.location.assign( target.querySelector( 'a' ).getAttribute( 'href' ) ); To test this, in the aforementioned o-table-row-links-spec.js file, we first set up a standard Jest mock for window.location.assign , and then create our spy to watch it: describe( 'o-table-row-links', () = { beforeEach( () = { window.location.assign = jest.fn(); locationSpy = jest.spyOn( window.location, 'assign' ); \u2026 } ); \u2026 } ); A little further down (after finishing the DOM setup and initializing the module we\u2019re testing), we have three tests that simulate clicks and then assert things that the spy can answer for us: whether it was called with a particular location parameter, and that it was called a specific number of times (zero). it( 'should navigate to new location when link row cell clicked', () = { simulateEvent( 'click', linkRowCellDom ); expect( locationSpy ).toBeCalledWith( 'https://www.example.com' ); } ); it( 'should not set window location when link is clicked', () = { simulateEvent( 'click', linkDom ); expect( locationSpy ).toHaveBeenCalledTimes( 0 ); } ); it( 'should not navigate to new location when non link row cell clicked', () = { simulateEvent( 'click', nonLinkRowCellDom ); expect( locationSpy ).toHaveBeenCalledTimes( 0 ); } );","title":"Testing browser state"},{"location":"javascript-unit-tests/#testing-user-interaction","text":"Testing user interaction with simulated pointer events, keystrokes, or form submissions is best handled via browser tests, not unit tests. User interaction in a unit test could falsely pass if the component wasn\u2019t visible on the page, for instance. Read more about how we run browser tests.","title":"Testing user interaction"},{"location":"mega-menu/","text":"How to Edit the Mega Menu \u00b6 Page contents \u00b6 Accessing the Mega Menu in Wagtail Editing Mega Menu items Editing Menu Item general information Editing navigation links and featured content Editing the Menu Item footer Mega Menu draft state Block draft state Link draft state Akamai cache Clearing the cache Previewing production changes Accessing the Mega Menu in Wagtail \u00b6 If you have permission to edit the menu in Wagtail, a Mega Menu link will appear in the sidebar nav when you are logged into the Wagtail admin. Clicking it will bring up a screen where you can add or edit Menu Items. You can find more information about permissions in Wagtail on the \u201cWagtail accounts and permissions (production)\u201d page on the Platform wiki). Editing Menu Items \u00b6 Each Menu Item represents a section (aka \"vertical\") in the mega menu, like the Consumer Tools vertical below: Editing Menu Item general information \u00b6 The name of the vertical (\u201cMenu label\u201d), its Wagtail overview page (\u201cOverview page link\u201d), and the order in which it appears in the cf.gov header (\u201cOrder\u201d) can be edited in the Menu Information and Order panels. Editing navigation links and featured content \u00b6 A Menu Item has four column panels, one for each of its possible columns. The first three columns can only contain groups of navigation links ( Menu Items Group ), while the fourth also includes a Featured Image Item option. Navigation links \u2013 the Menu Items Group \u00b6 The Menu Items Group block lets you manage a column\u2019s navigation links. Links are added to the \u201cMenu items\u201d list (which is not to be confused with the top-level Menu Item that represents the main sections, or verticals, of the site). Each link has a text field and options to specify either a Wagtail page or the direct URL of a page outside of Wagtail. See the main \u201cMega Menu\u201d page on Hubcap for editorial guidelines on writing links, link and character count guidance, and specific guidelines for the Consumer Tools and Practitioner Resources section links. The optional \u201cChild items\u201d list lets you create a secondary level of navigation for a link that will only be shown at screen sizes under 900px. For example, in the Policy Compliance vertical, each of the five main links has child items that can be accessed in the small-screen version of the menu, as indicated by the right arrows in the screenshot below. The optional \u201cColumn title\u201d field lets you specify a title that will be displayed above the column\u2019s links. Column titles should be brief and fit on one line at max screen width. If two adjacent columns share the same title, enter the title in both columns, but check the \u201cHide column title\u201d box in the second column so the title will only be displayed once (as in the Money Topics columns below). Featured content \u2013 the Featured Image Item \u00b6 The Featured Image Item block lets you feature a link to a single resource in the last column of the menu vertical. (This component used to be called \u201cfeatured menu content\u201d.) It provides fields for the text of the link, its Wagtail page or external URL, a short paragraph of supporting text, and an image. Content guidelines for this block can be found on the \u201cFeatured Menu Content\u201d and \u201cMega Menu\u201d pages on Hubcap. The Featured Image Item will be the only contents of its column: Editing the Menu Item footer \u00b6 The optional Nav Footer panel of a Menu Item can be used to display links to related resources with optional additional text. A Menu Footer block consists of a single \u201cContent\u201d rich text field that only accepts paragraphs and links. Footer items should be no longer than one line at max screen width, and can either consist entirely of a link or be a brief sentence that ends in a link. If a footer item contains non-link text, the full text of the item will be automatically added as an aria-label attribute on its link for users navigating the site via screen reader, so it should be written to be comprehensible when read aloud. Additional content guidance for footer links can be found on the main \u201cMega Menu\u201d page on Hubcap. Mega Menu draft state \u00b6 Unlike Wagtail pages, Menu Items don\u2019t have a Save draft option, but if you have a sharing site set up with wagtail-sharing , you can specify draft state for a particular block or link. Blocks or links that are marked as draft will only be shown on the sharing site and hidden on the live site. When you add a block or link with draft state, you can verify that it is only showing up on the sharing site by checking the production servers directly. See the \u201cPreviewing production changes\u201d section below for more information. Block draft state \u00b6 The Menu Items Group, Featured Image Item, and Menu Footer blocks all have a \u201cMark block as draft\u201d checkbox: If you want to test different column or footer content (like a new Featured Image Item or updated Menu Footer), add another block to the section and check its \u201cMark block as draft\u201d field. The new block will only be displayed on the sharing site, while the non-draft block will continue to be displayed on the live site. When you\u2019re ready to publish the new block, uncheck the draft box on the new block. You can now either delete the old block or move the new block up so it is the first block in its section. When there are multiple blocks in a section, the code that generates the production version of the menu looks for the first block that isn\u2019t in draft state, so either way, your new block will now be displayed in production once the cache is cleared. Since we don\u2019t save revisions of Menu Items, it might be safer to temporarily keep the old block until the new one is successfully deployed to production. Link draft state \u00b6 Instead of a \u201cMark as draft\u201d checkbox, links within a Menu Items Group have a \u201cState\u201d field with three options: If you want to test a new link (or a new version of an existing link), click the Add another button to create a new link and set it to \u201cShow on Content only\u201d. The new link will now only appear on the sharing site. If you are testing a new version of an existing link, you can either set the original link to \u201cShow on Production only\u201d or leave it as \u201cShow always\u201d, depending on whether you want both versions to show up on the sharing site. Anything marked as \u201cShow always\u201d (the default value of the State field) will show on both the sharing and live sites. Akamai cache \u00b6 Clearing the cache \u00b6 Since the mega menu is cached on all pages, the Akamai cache must be flushed to make changes to the menu on the live site. See the \u201cFlushing the Akamai cache\u201d page on Hubcap for instructions. Previewing production changes \u00b6 You can preview changes to the production version of the menu before clearing the Akamai cache by going to the internal hostname of a production server. By hitting the server directly, the Akamai cache is bypassed, so it will show the updated menu as soon as a change is saved to a Menu Item. This is a good way to make sure that changes are displaying correctly before generally pushing them out to to the live site by flushing the cache.","title":"Editing the Mega Menu"},{"location":"mega-menu/#how-to-edit-the-mega-menu","text":"","title":"How to Edit the Mega Menu"},{"location":"mega-menu/#page-contents","text":"Accessing the Mega Menu in Wagtail Editing Mega Menu items Editing Menu Item general information Editing navigation links and featured content Editing the Menu Item footer Mega Menu draft state Block draft state Link draft state Akamai cache Clearing the cache Previewing production changes","title":"Page contents"},{"location":"mega-menu/#accessing-the-mega-menu-in-wagtail","text":"If you have permission to edit the menu in Wagtail, a Mega Menu link will appear in the sidebar nav when you are logged into the Wagtail admin. Clicking it will bring up a screen where you can add or edit Menu Items. You can find more information about permissions in Wagtail on the \u201cWagtail accounts and permissions (production)\u201d page on the Platform wiki).","title":"Accessing the Mega Menu in Wagtail"},{"location":"mega-menu/#editing-menu-items","text":"Each Menu Item represents a section (aka \"vertical\") in the mega menu, like the Consumer Tools vertical below:","title":"Editing Menu Items"},{"location":"mega-menu/#editing-menu-item-general-information","text":"The name of the vertical (\u201cMenu label\u201d), its Wagtail overview page (\u201cOverview page link\u201d), and the order in which it appears in the cf.gov header (\u201cOrder\u201d) can be edited in the Menu Information and Order panels.","title":"Editing Menu Item general information"},{"location":"mega-menu/#editing-navigation-links-and-featured-content","text":"A Menu Item has four column panels, one for each of its possible columns. The first three columns can only contain groups of navigation links ( Menu Items Group ), while the fourth also includes a Featured Image Item option.","title":"Editing navigation links and featured content"},{"location":"mega-menu/#navigation-links-the-menu-items-group","text":"The Menu Items Group block lets you manage a column\u2019s navigation links. Links are added to the \u201cMenu items\u201d list (which is not to be confused with the top-level Menu Item that represents the main sections, or verticals, of the site). Each link has a text field and options to specify either a Wagtail page or the direct URL of a page outside of Wagtail. See the main \u201cMega Menu\u201d page on Hubcap for editorial guidelines on writing links, link and character count guidance, and specific guidelines for the Consumer Tools and Practitioner Resources section links. The optional \u201cChild items\u201d list lets you create a secondary level of navigation for a link that will only be shown at screen sizes under 900px. For example, in the Policy Compliance vertical, each of the five main links has child items that can be accessed in the small-screen version of the menu, as indicated by the right arrows in the screenshot below. The optional \u201cColumn title\u201d field lets you specify a title that will be displayed above the column\u2019s links. Column titles should be brief and fit on one line at max screen width. If two adjacent columns share the same title, enter the title in both columns, but check the \u201cHide column title\u201d box in the second column so the title will only be displayed once (as in the Money Topics columns below).","title":"Navigation links \u2013\u00a0the Menu Items Group"},{"location":"mega-menu/#featured-content-the-featured-image-item","text":"The Featured Image Item block lets you feature a link to a single resource in the last column of the menu vertical. (This component used to be called \u201cfeatured menu content\u201d.) It provides fields for the text of the link, its Wagtail page or external URL, a short paragraph of supporting text, and an image. Content guidelines for this block can be found on the \u201cFeatured Menu Content\u201d and \u201cMega Menu\u201d pages on Hubcap. The Featured Image Item will be the only contents of its column:","title":"Featured content \u2013 the Featured Image Item"},{"location":"mega-menu/#editing-the-menu-item-footer","text":"The optional Nav Footer panel of a Menu Item can be used to display links to related resources with optional additional text. A Menu Footer block consists of a single \u201cContent\u201d rich text field that only accepts paragraphs and links. Footer items should be no longer than one line at max screen width, and can either consist entirely of a link or be a brief sentence that ends in a link. If a footer item contains non-link text, the full text of the item will be automatically added as an aria-label attribute on its link for users navigating the site via screen reader, so it should be written to be comprehensible when read aloud. Additional content guidance for footer links can be found on the main \u201cMega Menu\u201d page on Hubcap.","title":"Editing the Menu Item footer"},{"location":"mega-menu/#mega-menu-draft-state","text":"Unlike Wagtail pages, Menu Items don\u2019t have a Save draft option, but if you have a sharing site set up with wagtail-sharing , you can specify draft state for a particular block or link. Blocks or links that are marked as draft will only be shown on the sharing site and hidden on the live site. When you add a block or link with draft state, you can verify that it is only showing up on the sharing site by checking the production servers directly. See the \u201cPreviewing production changes\u201d section below for more information.","title":"Mega Menu draft state"},{"location":"mega-menu/#block-draft-state","text":"The Menu Items Group, Featured Image Item, and Menu Footer blocks all have a \u201cMark block as draft\u201d checkbox: If you want to test different column or footer content (like a new Featured Image Item or updated Menu Footer), add another block to the section and check its \u201cMark block as draft\u201d field. The new block will only be displayed on the sharing site, while the non-draft block will continue to be displayed on the live site. When you\u2019re ready to publish the new block, uncheck the draft box on the new block. You can now either delete the old block or move the new block up so it is the first block in its section. When there are multiple blocks in a section, the code that generates the production version of the menu looks for the first block that isn\u2019t in draft state, so either way, your new block will now be displayed in production once the cache is cleared. Since we don\u2019t save revisions of Menu Items, it might be safer to temporarily keep the old block until the new one is successfully deployed to production.","title":"Block draft state"},{"location":"mega-menu/#link-draft-state","text":"Instead of a \u201cMark as draft\u201d checkbox, links within a Menu Items Group have a \u201cState\u201d field with three options: If you want to test a new link (or a new version of an existing link), click the Add another button to create a new link and set it to \u201cShow on Content only\u201d. The new link will now only appear on the sharing site. If you are testing a new version of an existing link, you can either set the original link to \u201cShow on Production only\u201d or leave it as \u201cShow always\u201d, depending on whether you want both versions to show up on the sharing site. Anything marked as \u201cShow always\u201d (the default value of the State field) will show on both the sharing and live sites.","title":"Link draft state"},{"location":"mega-menu/#akamai-cache","text":"","title":"Akamai cache"},{"location":"mega-menu/#clearing-the-cache","text":"Since the mega menu is cached on all pages, the Akamai cache must be flushed to make changes to the menu on the live site. See the \u201cFlushing the Akamai cache\u201d page on Hubcap for instructions.","title":"Clearing the cache"},{"location":"mega-menu/#previewing-production-changes","text":"You can preview changes to the production version of the menu before clearing the Akamai cache by going to the internal hostname of a production server. By hitting the server directly, the Akamai cache is bypassed, so it will show the updated menu as soon as a change is saved to a Menu Item. This is a good way to make sure that changes are displaying correctly before generally pushing them out to to the live site by flushing the cache.","title":"Previewing production changes"},{"location":"migrations/","text":"Django and Wagtail Migrations \u00b6 Adding or changing fields on Django models, Wagtail page models (which are a particular kind of Django model), or StreamField block classes will always require a new Django schema migration ; additionally, changing field names or types on an existing block will require a Django data migration . Table of contents \u00b6 Reference material Schema migrations Data migrations Wagtail-specific consideration Utility functions Recreating migrations Reference material \u00b6 The following links may be useful for setting context or diving deeper into the concepts presented throughout this page: Django migrations documentation Django data migrations documentation Wagtail Streamfield migrations documentation Schema migrations \u00b6 Any time you add or change a field on a Django model, Wagtail page model (which are a particular kind of Django model), or StreamField block class, a Django schema migration will be required. This includes changes as small as modifying the help_text string. To automatically generate a schema migration, run the following, editing it to give your migration a name that briefly describes the change(s) you're making: ./cfgov/manage.py makemigrations -n description_of_changes For examples of good migration names, look through some of our existing migration files . Note Some changes will generate multiple migration files. If you change a block that is used in pages defined in different sub-apps, you will see a migration file for each of those sub-apps. Migration numbering and conflicts \u00b6 When a migration file is generated, it will automatically be given a unique four-digit number at the beginning of its filename. These numbers are assigned in sequence, and they end up being a regular source of conflicts between pull requests that are in flight at the same time. If a PR with a migration gets merged between the time you create your migration and the time that your PR is ready for merging, you will have to update your branch as normal to be current with master and then re-create your migration. Also note that our back-end tests that run in Travis will fail if a required schema migration is missing or if migrations are in conflict with one another. Data migrations \u00b6 Data migrations are required any time you: rename an existing field change the type of an existing field delete an existing field rename a block within a StreamField delete a block if you do not want to lose any data already stored in that field or block. In other words, if an existing field or block is changing, any data stored in that field or block has to be migrated to a different place, unless you're OK with jettisoning it. There is no automatic generation mechanism like there is for schema migrations. You must write the script by hand that automates the transfer of data from old fields to new fields. To generate an empty migration file for your data migration, run: ./cfgov/manage.py makemigrations --empty yourappname You can also copy the code below to get started with forward() and backward() functions to migrate your model's data: from django.db import migrations def forwards(apps, schema_editor): MyModel = apps.get_model('yourappname', 'MyModel') for obj in MyModel.objects.all(): # Make forward changes to the object pass def backwards(apps, schema_editor): MyModel = apps.get_model('yourappname', 'MyModel') for obj in MyModel.objects.all(): # Make backward changes to the object pass class Migration(migrations.Migration): dependencies = [] operations = [ migrations.RunPython(forwards, backwards), ] The forwards() and backwards() functions are where any changes that need to happen to a model's data are made. Wagtail-specific considerations \u00b6 Django data migrations with Wagtail can be challenging because programmatic editing of Wagtail pages is difficult , and pages have both revisions and StreamFields. This section describes ways we try to address these challenges in cfgov-refresh. The data migration needs to modify both the existing Wagtail pages that correspond to the changed model and all revisions of that page. It also needs to be able to manipulate the contents of StreamFields. As described in the editing a component guide , it's a three-step process to modify a field without losing data: Create the new field with an automatic schema migration Use a handwritten data migration script to move data from the old field to the new field Delete the old field with an automatic schema migration We've written some utility functions in cfgov-refresh that make writing data migrations for StreamFields easier. Using these utilities, a Django data migration that modifies a StreamField would use the following format: from django.db import migrations from v1.util.migrations import migrate_page_types_and_fields def forward_mapper(page_or_revision, data): # Manipulate the stream block data forwards return data def backward_mapper(page_or_revision, data): # Manipulate the stream block data backwards return data def forwards(apps, schema_editor): page_types_and_fields = [ ('myapp', 'MyPage', 'field_name', 'block_name'), ] migrate_page_types_and_fields( apps, page_types_and_fields, forward_mapper ) def backwards(apps, schema_editor): page_types_and_fields = [ ('myapp', 'MyPage', 'field_name', 'block_name'), ] migrate_page_types_and_fields( apps, page_types_and_fields, backward_mapper ) class Migration(migrations.Migration): dependencies = [] operations = [ migrations.RunPython(forwards, backwards), ] field_name is the name of the StreamField on the Page model that contains the blocks to migrate. block_name is the name of the block within a StreamField that contains the data to be migrated. StreamBlocks can themselves also contain child blocks. The block name can be given as a list of block names that form the \"path\" to the block that needs to be migrated. For example : def forwards(apps, schema_editor): page_types_and_fields = [ ('myapp', 'MyPage', 'field_name', ['parent_block', 'child_block']), ] migrate_page_types_and_fields( apps, page_types_and_fields, forward_mapper ) In this example, a block with the name child_block that is inside a block named parent_block will be passed to the forward_mapper function. The data that gets passed to the forward_mapper or backward_mapper is a JSON-compatible Python dict that corresponds to the block's schema. Utility functions \u00b6 These functions, defined in v1.util.migrations , are used in the above data migration example. They reduce the amount of boilerplate required to work with Wagtail StreamField data in data migrations. migrate_page_types_and_fields(apps, page_types_and_fields, mapper) \u00b6 Migrate the fields of a Wagtail page type using the given mapper function. page_types_and_fields should be a list of 4-tuples providing ('app', 'PageType', 'field_name', 'block_name') or ('app', 'PageType', 'field_name', ['parent_block_name', 'child_block_name']) . field_name is the name of the StreamField on the Page model. block_name is the name of the StreamBlock within the StreamField to migrate. The mapper function should take page_or_revision and the stream block's value as a dict . This function calls migrate_stream_field() . migrate_stream_field(page_or_revision, field_name, block_path, mapper) \u00b6 Migrate all occurrences of the block name contained within the block_path list belonging to the page or revision using the mapper function. The mapper function should take page_or_revision and the stream block's value as a dict . This function calls migrate_stream_data() . migrate_stream_data(page_or_revision, block_path, stream_data, mapper) \u00b6 Migrate all occurrences of the block name contained within the block_path list within the stream_data dict using the given mapper function. The mapper function should take page_or_revision and the stream block's value as a dict . get_stream_data(page_or_revision, field_name) \u00b6 Get the StreamField data for a given field name on a page or a revision. This function will return a list of dict -like objects containing the blocks within the given StreamField. set_stream_data(page_or_revision, field_name, stream_data, commit=True) \u00b6 Set the StreamField data for a given field name on a page or a revision. If commit is True (default), save() is called on the page_or_revision object. stream_data must be a list of dict -like objects containing the blocks within the given StreamField. Recreating migrations \u00b6 As described above , each time a Django model's definition changes it requires the generation of a new Django migration. Over time, the number of migrations in our apps can grow very large, slowing down testing and the migrate command. For this reason it may be desirable to periodically delete and recreate the migration files, so that instead of a series of files detailing every change over time we have a smaller set that just describes the current state of models in the code. Django does provide an automated squashing process for migrations, but this is often not optimal when migrations contain manual RunPython blocks that we don't necessarily care about keeping around. Instead, we delete all existing migration files and then run manage.py makemigrations to create new ones. This will generate the smallest number of migration files needed to describe the state of models in the code; typically one per app although sometimes multiple are needed due to app dependencies. This process does have these critical side effects: Databases that exist at some migration state before the one at the point of the recreation will no longer be able to be migrated to the current state, as the intermediate changes will have been lost. This means that those databases will need to be recreated. This also means that historical database archives will require a bit more work to resurrect; they'll need to first be migrated to the point just before the recreation, and then updated to code at or after that point. For example, say a database dump exists at a point where N migrations occur. At such time as N + 1 migrations occur, we decide to go through the recreation process. Now we have a new migration numbered N + 2 that represents the equivalent of all (1..N+1) migrations that it replaces. If you try to load and migrate the dump at point N, Django no longer has the code necessary to go from N- N+1 only -- it only has the ability to go from 0- N+2. To recover such a dump, you'll need to check out the code at the point before the recreation was done, migrate from N- N+1, and then check out latest and migrate forwards. Any open pull requests at the time of the recreation that reference or depend on some of the existing migrations will need to be modified to instead refer to the new migration files. Migrations can be recreated with this process: Remove all existing migration files: rm -f -v cfgov/*/migrations/0* Create new migration files from the state of model Python code: cfgov/manage.py makemigrations --noinput As it happens this creates new initial migrations ( 0001_initial ) for all apps, plus some subsequent migrations ( 0002_something ) for apps that depend on other apps (for example, ask_cfpb has its own initial migration and then some changes that rely on v1 ). Rename the created migration files so that they follow in sequence the migration files that used to exist. For example, if at the time of recreation there are 101 v1 migrations, the first new migration should be numbered 102. Manually alter all new migration files to indicate that they replace the old migration files. This involves adding lines to these files like: includes = [('app_name', '0002_foo'), ('app_name', '0003_bar'), ...] This tells Django that these new files replace the old files, so that when migrations are run again, it doesn't need to do anything. Also manually update any new subsequent migration files so that they properly refer to each other. For example, if an app has two new migrations 102 and 103, the 103 file needs to properly depend on 102. To apply these new migration files to an existing database, you can simply run: cfgov/manage.py migrate --noinput You'll see that there are no changes to apply, as the new files should exactly describe the current model state in the same way that the old migrations did. See cfgov-refresh#3770 for an example of when this was done.","title":"Django and Wagtail Migrations"},{"location":"migrations/#django-and-wagtail-migrations","text":"Adding or changing fields on Django models, Wagtail page models (which are a particular kind of Django model), or StreamField block classes will always require a new Django schema migration ; additionally, changing field names or types on an existing block will require a Django data migration .","title":"Django and Wagtail Migrations"},{"location":"migrations/#table-of-contents","text":"Reference material Schema migrations Data migrations Wagtail-specific consideration Utility functions Recreating migrations","title":"Table of contents"},{"location":"migrations/#reference-material","text":"The following links may be useful for setting context or diving deeper into the concepts presented throughout this page: Django migrations documentation Django data migrations documentation Wagtail Streamfield migrations documentation","title":"Reference material"},{"location":"migrations/#schema-migrations","text":"Any time you add or change a field on a Django model, Wagtail page model (which are a particular kind of Django model), or StreamField block class, a Django schema migration will be required. This includes changes as small as modifying the help_text string. To automatically generate a schema migration, run the following, editing it to give your migration a name that briefly describes the change(s) you're making: ./cfgov/manage.py makemigrations -n description_of_changes For examples of good migration names, look through some of our existing migration files . Note Some changes will generate multiple migration files. If you change a block that is used in pages defined in different sub-apps, you will see a migration file for each of those sub-apps.","title":"Schema migrations"},{"location":"migrations/#migration-numbering-and-conflicts","text":"When a migration file is generated, it will automatically be given a unique four-digit number at the beginning of its filename. These numbers are assigned in sequence, and they end up being a regular source of conflicts between pull requests that are in flight at the same time. If a PR with a migration gets merged between the time you create your migration and the time that your PR is ready for merging, you will have to update your branch as normal to be current with master and then re-create your migration. Also note that our back-end tests that run in Travis will fail if a required schema migration is missing or if migrations are in conflict with one another.","title":"Migration numbering and conflicts"},{"location":"migrations/#data-migrations","text":"Data migrations are required any time you: rename an existing field change the type of an existing field delete an existing field rename a block within a StreamField delete a block if you do not want to lose any data already stored in that field or block. In other words, if an existing field or block is changing, any data stored in that field or block has to be migrated to a different place, unless you're OK with jettisoning it. There is no automatic generation mechanism like there is for schema migrations. You must write the script by hand that automates the transfer of data from old fields to new fields. To generate an empty migration file for your data migration, run: ./cfgov/manage.py makemigrations --empty yourappname You can also copy the code below to get started with forward() and backward() functions to migrate your model's data: from django.db import migrations def forwards(apps, schema_editor): MyModel = apps.get_model('yourappname', 'MyModel') for obj in MyModel.objects.all(): # Make forward changes to the object pass def backwards(apps, schema_editor): MyModel = apps.get_model('yourappname', 'MyModel') for obj in MyModel.objects.all(): # Make backward changes to the object pass class Migration(migrations.Migration): dependencies = [] operations = [ migrations.RunPython(forwards, backwards), ] The forwards() and backwards() functions are where any changes that need to happen to a model's data are made.","title":"Data migrations"},{"location":"migrations/#wagtail-specific-considerations","text":"Django data migrations with Wagtail can be challenging because programmatic editing of Wagtail pages is difficult , and pages have both revisions and StreamFields. This section describes ways we try to address these challenges in cfgov-refresh. The data migration needs to modify both the existing Wagtail pages that correspond to the changed model and all revisions of that page. It also needs to be able to manipulate the contents of StreamFields. As described in the editing a component guide , it's a three-step process to modify a field without losing data: Create the new field with an automatic schema migration Use a handwritten data migration script to move data from the old field to the new field Delete the old field with an automatic schema migration We've written some utility functions in cfgov-refresh that make writing data migrations for StreamFields easier. Using these utilities, a Django data migration that modifies a StreamField would use the following format: from django.db import migrations from v1.util.migrations import migrate_page_types_and_fields def forward_mapper(page_or_revision, data): # Manipulate the stream block data forwards return data def backward_mapper(page_or_revision, data): # Manipulate the stream block data backwards return data def forwards(apps, schema_editor): page_types_and_fields = [ ('myapp', 'MyPage', 'field_name', 'block_name'), ] migrate_page_types_and_fields( apps, page_types_and_fields, forward_mapper ) def backwards(apps, schema_editor): page_types_and_fields = [ ('myapp', 'MyPage', 'field_name', 'block_name'), ] migrate_page_types_and_fields( apps, page_types_and_fields, backward_mapper ) class Migration(migrations.Migration): dependencies = [] operations = [ migrations.RunPython(forwards, backwards), ] field_name is the name of the StreamField on the Page model that contains the blocks to migrate. block_name is the name of the block within a StreamField that contains the data to be migrated. StreamBlocks can themselves also contain child blocks. The block name can be given as a list of block names that form the \"path\" to the block that needs to be migrated. For example : def forwards(apps, schema_editor): page_types_and_fields = [ ('myapp', 'MyPage', 'field_name', ['parent_block', 'child_block']), ] migrate_page_types_and_fields( apps, page_types_and_fields, forward_mapper ) In this example, a block with the name child_block that is inside a block named parent_block will be passed to the forward_mapper function. The data that gets passed to the forward_mapper or backward_mapper is a JSON-compatible Python dict that corresponds to the block's schema.","title":"Wagtail-specific considerations"},{"location":"migrations/#utility-functions","text":"These functions, defined in v1.util.migrations , are used in the above data migration example. They reduce the amount of boilerplate required to work with Wagtail StreamField data in data migrations.","title":"Utility functions"},{"location":"migrations/#migrate_page_types_and_fieldsapps-page_types_and_fields-mapper","text":"Migrate the fields of a Wagtail page type using the given mapper function. page_types_and_fields should be a list of 4-tuples providing ('app', 'PageType', 'field_name', 'block_name') or ('app', 'PageType', 'field_name', ['parent_block_name', 'child_block_name']) . field_name is the name of the StreamField on the Page model. block_name is the name of the StreamBlock within the StreamField to migrate. The mapper function should take page_or_revision and the stream block's value as a dict . This function calls migrate_stream_field() .","title":"migrate_page_types_and_fields(apps, page_types_and_fields, mapper)"},{"location":"migrations/#migrate_stream_fieldpage_or_revision-field_name-block_path-mapper","text":"Migrate all occurrences of the block name contained within the block_path list belonging to the page or revision using the mapper function. The mapper function should take page_or_revision and the stream block's value as a dict . This function calls migrate_stream_data() .","title":"migrate_stream_field(page_or_revision, field_name, block_path, mapper)"},{"location":"migrations/#migrate_stream_datapage_or_revision-block_path-stream_data-mapper","text":"Migrate all occurrences of the block name contained within the block_path list within the stream_data dict using the given mapper function. The mapper function should take page_or_revision and the stream block's value as a dict .","title":"migrate_stream_data(page_or_revision, block_path, stream_data, mapper)"},{"location":"migrations/#get_stream_datapage_or_revision-field_name","text":"Get the StreamField data for a given field name on a page or a revision. This function will return a list of dict -like objects containing the blocks within the given StreamField.","title":"get_stream_data(page_or_revision, field_name)"},{"location":"migrations/#set_stream_datapage_or_revision-field_name-stream_data-committrue","text":"Set the StreamField data for a given field name on a page or a revision. If commit is True (default), save() is called on the page_or_revision object. stream_data must be a list of dict -like objects containing the blocks within the given StreamField.","title":"set_stream_data(page_or_revision, field_name, stream_data, commit=True)"},{"location":"migrations/#recreating-migrations","text":"As described above , each time a Django model's definition changes it requires the generation of a new Django migration. Over time, the number of migrations in our apps can grow very large, slowing down testing and the migrate command. For this reason it may be desirable to periodically delete and recreate the migration files, so that instead of a series of files detailing every change over time we have a smaller set that just describes the current state of models in the code. Django does provide an automated squashing process for migrations, but this is often not optimal when migrations contain manual RunPython blocks that we don't necessarily care about keeping around. Instead, we delete all existing migration files and then run manage.py makemigrations to create new ones. This will generate the smallest number of migration files needed to describe the state of models in the code; typically one per app although sometimes multiple are needed due to app dependencies. This process does have these critical side effects: Databases that exist at some migration state before the one at the point of the recreation will no longer be able to be migrated to the current state, as the intermediate changes will have been lost. This means that those databases will need to be recreated. This also means that historical database archives will require a bit more work to resurrect; they'll need to first be migrated to the point just before the recreation, and then updated to code at or after that point. For example, say a database dump exists at a point where N migrations occur. At such time as N + 1 migrations occur, we decide to go through the recreation process. Now we have a new migration numbered N + 2 that represents the equivalent of all (1..N+1) migrations that it replaces. If you try to load and migrate the dump at point N, Django no longer has the code necessary to go from N- N+1 only -- it only has the ability to go from 0- N+2. To recover such a dump, you'll need to check out the code at the point before the recreation was done, migrate from N- N+1, and then check out latest and migrate forwards. Any open pull requests at the time of the recreation that reference or depend on some of the existing migrations will need to be modified to instead refer to the new migration files. Migrations can be recreated with this process: Remove all existing migration files: rm -f -v cfgov/*/migrations/0* Create new migration files from the state of model Python code: cfgov/manage.py makemigrations --noinput As it happens this creates new initial migrations ( 0001_initial ) for all apps, plus some subsequent migrations ( 0002_something ) for apps that depend on other apps (for example, ask_cfpb has its own initial migration and then some changes that rely on v1 ). Rename the created migration files so that they follow in sequence the migration files that used to exist. For example, if at the time of recreation there are 101 v1 migrations, the first new migration should be numbered 102. Manually alter all new migration files to indicate that they replace the old migration files. This involves adding lines to these files like: includes = [('app_name', '0002_foo'), ('app_name', '0003_bar'), ...] This tells Django that these new files replace the old files, so that when migrations are run again, it doesn't need to do anything. Also manually update any new subsequent migration files so that they properly refer to each other. For example, if an app has two new migrations 102 and 103, the 103 file needs to properly depend on 102. To apply these new migration files to an existing database, you can simply run: cfgov/manage.py migrate --noinput You'll see that there are no changes to apply, as the new files should exactly describe the current model state in the same way that the old migrations did. See cfgov-refresh#3770 for an example of when this was done.","title":"Recreating migrations"},{"location":"python-unit-tests/","text":"Backend testing \u00b6 Writing tests \u00b6 We have multiple resources for writing new unit tests for Django, Wagtial, and Python code: CFPB Django and Wagtail unit testing documentation The Django testing documentation The Wagtail testing documentation Real Python's \"Testing in Django\" Running tests \u00b6 To run the the full suite of Python tests using Tox, make sure you are in the cfgov-refresh root and then run: tox This will run linting tests and unit tests with migrations. This is the same as running: tox -e lint -e unittest-py27-dj111-wag113-slow By default this uses a local SQLite database for tests. To override this, you can set the DATABASE_URL environment variable to a database connection sring as supported by dj-database-url . If you haven't changed any Python dependencies and you don't need to test all migrations, you can run a much faster Python code test using: tox -e fast To test against Python 3 without migrations: tox -e unittest-py36-dj111-wag113-fast If you would like to run only a specific test, or the tests for a specific app, you can provide a dotted path to the test as the final argument to any of the above calls to tox : tox -e unittest-py36-dj111-wag113-fast regulations3k.tests.test_regdown Linting \u00b6 We use the flake8 and isort tools to ensure compliance with PEP8 style guide , Django coding style guidelines , and the CFPB Python style guide . Both flake8 and isort can be run using the Tox lint environment: tox -e lint This will run isort in check-only mode and it will print diffs for imports that need to be fixed. To automatically fix import sort issues, run: isort --recursive cfgov/ From the root of cfgov-refresh . Coverage \u00b6 To see Python code coverage information, run coverage report -m To see coverage for a limited number of files, use the --include argument to coverage and provide a path to the files you wish to see: coverage report -m --include=./cfgov/regulations3k/* Test output \u00b6 Python tests should avoid writing to stdout as part of their normal execution. To enforce this convention, the tests can be run using a custom Django test runner that fails if anything is written to stdout. This test runner is at cfgov.test.StdoutCapturingTestRunner and can be enabled with the TEST_RUNNER environment variable: TEST_RUNNER=cfgov.test.StdoutCapturingTestRunner tox -e fast This test runner is enabled when tests are run automatically on Travis CI , but is not used by default when running tests locally. GovDelivery \u00b6 If you write Python code that interacts with the GovDelivery subscription API, you can use the functionality provided in core.govdelivery.MockGovDelivery as a mock interface to avoid the use of patch in unit tests. This object behaves similarly to the real govdelivery.api.GovDelivery class in that it handles all requests and returns a valid (200) requests.Response instance. Conveniently for unit testing, all calls are stored in a class-level list that can be retrieved at MockGovDelivery.calls . This allows for testing of code that interacts with GovDelivery by checking the contents of this list to ensure that the right methods were called. This pattern is modeled after Django's django.core.mail.outbox which provides similar functionality for testing sending of emails. The related classes ExceptionMockGovDelivery and ServerErrorMockGovDelivery can similarly be used in unit tests to test for cases where a call to the GovDelivery API raises an exception and returns an HTTP status code of 500, respectively.","title":"Python Unit Testing"},{"location":"python-unit-tests/#backend-testing","text":"","title":"Backend testing"},{"location":"python-unit-tests/#writing-tests","text":"We have multiple resources for writing new unit tests for Django, Wagtial, and Python code: CFPB Django and Wagtail unit testing documentation The Django testing documentation The Wagtail testing documentation Real Python's \"Testing in Django\"","title":"Writing tests"},{"location":"python-unit-tests/#running-tests","text":"To run the the full suite of Python tests using Tox, make sure you are in the cfgov-refresh root and then run: tox This will run linting tests and unit tests with migrations. This is the same as running: tox -e lint -e unittest-py27-dj111-wag113-slow By default this uses a local SQLite database for tests. To override this, you can set the DATABASE_URL environment variable to a database connection sring as supported by dj-database-url . If you haven't changed any Python dependencies and you don't need to test all migrations, you can run a much faster Python code test using: tox -e fast To test against Python 3 without migrations: tox -e unittest-py36-dj111-wag113-fast If you would like to run only a specific test, or the tests for a specific app, you can provide a dotted path to the test as the final argument to any of the above calls to tox : tox -e unittest-py36-dj111-wag113-fast regulations3k.tests.test_regdown","title":"Running tests"},{"location":"python-unit-tests/#linting","text":"We use the flake8 and isort tools to ensure compliance with PEP8 style guide , Django coding style guidelines , and the CFPB Python style guide . Both flake8 and isort can be run using the Tox lint environment: tox -e lint This will run isort in check-only mode and it will print diffs for imports that need to be fixed. To automatically fix import sort issues, run: isort --recursive cfgov/ From the root of cfgov-refresh .","title":"Linting"},{"location":"python-unit-tests/#coverage","text":"To see Python code coverage information, run coverage report -m To see coverage for a limited number of files, use the --include argument to coverage and provide a path to the files you wish to see: coverage report -m --include=./cfgov/regulations3k/*","title":"Coverage"},{"location":"python-unit-tests/#test-output","text":"Python tests should avoid writing to stdout as part of their normal execution. To enforce this convention, the tests can be run using a custom Django test runner that fails if anything is written to stdout. This test runner is at cfgov.test.StdoutCapturingTestRunner and can be enabled with the TEST_RUNNER environment variable: TEST_RUNNER=cfgov.test.StdoutCapturingTestRunner tox -e fast This test runner is enabled when tests are run automatically on Travis CI , but is not used by default when running tests locally.","title":"Test output"},{"location":"python-unit-tests/#govdelivery","text":"If you write Python code that interacts with the GovDelivery subscription API, you can use the functionality provided in core.govdelivery.MockGovDelivery as a mock interface to avoid the use of patch in unit tests. This object behaves similarly to the real govdelivery.api.GovDelivery class in that it handles all requests and returns a valid (200) requests.Response instance. Conveniently for unit testing, all calls are stored in a class-level list that can be retrieved at MockGovDelivery.calls . This allows for testing of code that interacts with GovDelivery by checking the contents of this list to ensure that the right methods were called. This pattern is modeled after Django's django.core.mail.outbox which provides similar functionality for testing sending of emails. The related classes ExceptionMockGovDelivery and ServerErrorMockGovDelivery can similarly be used in unit tests to test for cases where a call to the GovDelivery API raises an exception and returns an HTTP status code of 500, respectively.","title":"GovDelivery"},{"location":"satellite-repos/","text":"Satellite repos \u00b6 Satellite repositories provide functionality or site content that is optionally included as part of the website, but not included in this repository. Some satellite repos exist to allow certain functionality to be run or tested without needing to set up the full website. Thinking about making a new satellite repo? Satellite projects were originally built to be imported into the consumerfinance.gov website before we started using Wagtail to manage site content. We now prefer to build projects as apps inside the cfgov-refresh repo. For more info, refer to the \"Setting up new project code for consumerfinance.gov\" page on the CFGOV/platform wiki on GHE. We have seven satellite repos that are maintained outside of the cfgov-refresh codebase: ccdb5_api ccdb5_ui college_costs comparisontool owning_a_home_api retirement teachers_digital_platform We import these satellite projects into cfgov-refresh by specifying wheel files for each in requirements/optional-public.txt .","title":"Satellite Repos"},{"location":"satellite-repos/#satellite-repos","text":"Satellite repositories provide functionality or site content that is optionally included as part of the website, but not included in this repository. Some satellite repos exist to allow certain functionality to be run or tested without needing to set up the full website. Thinking about making a new satellite repo? Satellite projects were originally built to be imported into the consumerfinance.gov website before we started using Wagtail to manage site content. We now prefer to build projects as apps inside the cfgov-refresh repo. For more info, refer to the \"Setting up new project code for consumerfinance.gov\" page on the CFGOV/platform wiki on GHE. We have seven satellite repos that are maintained outside of the cfgov-refresh codebase: ccdb5_api ccdb5_ui college_costs comparisontool owning_a_home_api retirement teachers_digital_platform We import these satellite projects into cfgov-refresh by specifying wheel files for each in requirements/optional-public.txt .","title":"Satellite repos"},{"location":"split-testing/","text":"Setting up a split testing experiment \u00b6 One thing we like to do from time to time is test changes on specific groups of pages and see how they perform. We call this \"split testing\". In contrast to traditional A/B testing, where the change being tested is shown to a portion of the audience , in the case of split testing, the change is shown to everyone, but only on a subset of similar pages (e.g., a group of related Ask CFPB answers). We call this a subset of pages a \"cluster\". So you've been asked to set up a split testing experiment \u00b6 Good news! The infrastructure has been put in place for you to do it easily. It's set up so that the code being tested is isolated by a feature flag. If you're not familiar with how our flags are set up, read the concepts overview in the django-flags concepts . The flag condition \u00b6 In order to check whether or not a page is part of a split testing cluster, we registered a new flag condition in cfgov/core/feature_flags.py . @conditions.register('in split testing cluster') def in_split_testing_cluster(cluster_group, page, clusters=CLUSTERS, **kwargs): cluster_group = clusters[cluster_group] lookup_value = getattr(page, 'split_test_id', page.id) return lookup_value in chain(*cluster_group.values()) This condition takes the following arguments: - cluster_group : A string that identifies which group of clusters we want to check - page : An instance of a Wagtail page that we want to look for in the cluster group - clusters : A dictionary containing one or more named cluster_group s \u2013 by default, this points to a dictionary defined in our master cluster definition file (see \"Defining clusters\" below) If the page 's id property is found in the cluster_group , the condition returns True . If the page has a split_test_id property defined, it will check that instead of page.id . (More on that later.) You do not need to modify anything in this file to set up a new experiment. Defining clusters \u00b6 To run a split test we first have to know what pages we're testing. Split testing clusters are typically defined in our master cluster definition file at cfgov/core/split_testing_clusters.py . Create a new dictionary with a meaningful name and add the page IDs for the pages in your cluster(s) to it. For example: ASK_CFPB_H1 = { 18: [ 103, 160, 187, 337, 731, 767, 951, 1157, 1161, 1403, 1405, 1439, 1447, 1567, 1695, ], 34: [ 104, 105, 130, 205, 747, 841, 1423, 1791, ], 38: [ 143, 184, 329, 331, 1165, 1637, 1787, 1789, 1797, 1921, 1923, ], 49: [ 136, 161, 163, 164, 172, 176, 179, 180, 181, 188, 192, 336, 817, 1699, 1983, 1989, 1995, 1997, 2001, ], 93: [ 146, 226, 237, 318, 338, 545, 633, 811, 1215, 1463, 1507, ], } In the above example, there are five separate clusters, all of which will be part of the experiment. The number being used for the keys doesn't matter to Python, but it may be useful for internal tracking purposes. As noted earlier, normally the page IDs inside the lists correspond to Wagtail's page ID (most easily findable in the edit URL for a page), but in this particular example, they are actually Ask CFPB answer IDs . For Ask CFPB answer pages, we use the answer ID, because that ID is frequently used in internal communications for shorthand references to Ask answers. All other page types use their Wagtail page ID value. If you have cause to use something else for the lookup values, you can define the split_test_id property in a page class. For example: # cfgov/ask_cfpb/models/pages.py class AnswerPage(CFGOVPage): \u2026 # Overrides the default of page.id for comparing against split testing # clusters. See: core.feature_flags.in_split_testing_cluster @property def split_test_id(self): return self.answer_base.id \u2026 If you do this, remember to add a simple test: # cfgov/ask_cfpb/tests/models/test_pages.py def test_answer_split_testing_id(self): Confirm AnswerPage's split_testing_id is set to its answer_base.id, which is checked by the core.feature_flags.in_split_testing_cluster flag condition when doing split testing on Ask CFPB answer pages. answer = self.answer1234 page = answer.english_page self.assertEqual(page.split_test_id, answer.id) You will also need to add a reference to your new dictionary to the CLUSTERS dictionary at the bottom of the file: CLUSTERS = { 'ASK_CFPB_H1': ASK_CFPB_H1, } Creating and using the flag \u00b6 The new \"in split testing cluster\" condition described earlier can be added to flags and used like any other feature flag. For the example we started above, we first create the flag in our settings, giving it the default condition with a parameter corresponding to the cluster group name we created above: # cfgov/settings/base.py \u2026 # Feature flags # All feature flags must be listed here with a dict of any hard-coded # conditions or an empty dict. If the conditions dict is empty the flag will # only be enabled if database conditions are added. FLAGS = { \u2026 # SPLIT TESTING FLAGS # Ask CFPB page titles as H1s instead of H2s 'ASK_CFPB_H1': { 'in split testing cluster': 'ASK_CFPB_H1' }, \u2026 } \u2026 And then in our template we check the flag, passing in the current page , and if the page's answer ID is found in a cluster, flag_enabled() is True , and we see the h1 instead of the h2 . !-- cfgov/jinja2/v1/ask-cfpb/answer-page.html -- \u2026 {% if flag_enabled('ASK_CFPB_H1', page=page) %} h1 {{ page.question | striptags }} /h1 {% else %} h2 {{ page.question | striptags }} /h2 {% endif %} \u2026 Setting the flag up with the default condition of in_split_testing_cluster means that as soon as the code is deployed, the flag is acting on pages in the cluster. One the results of the test are known, you can set a new boolean flag condition in Wagtail to have the flag always be True , if the test was successful, or always be False , if the test was not successful. This will make (or not make) the change on all pages touched by that flag until you take the time to remove the flag entirely.","title":"Split Testing"},{"location":"split-testing/#setting-up-a-split-testing-experiment","text":"One thing we like to do from time to time is test changes on specific groups of pages and see how they perform. We call this \"split testing\". In contrast to traditional A/B testing, where the change being tested is shown to a portion of the audience , in the case of split testing, the change is shown to everyone, but only on a subset of similar pages (e.g., a group of related Ask CFPB answers). We call this a subset of pages a \"cluster\".","title":"Setting up a split testing experiment"},{"location":"split-testing/#so-youve-been-asked-to-set-up-a-split-testing-experiment","text":"Good news! The infrastructure has been put in place for you to do it easily. It's set up so that the code being tested is isolated by a feature flag. If you're not familiar with how our flags are set up, read the concepts overview in the django-flags concepts .","title":"So you've been asked to set up a split testing experiment"},{"location":"split-testing/#the-flag-condition","text":"In order to check whether or not a page is part of a split testing cluster, we registered a new flag condition in cfgov/core/feature_flags.py . @conditions.register('in split testing cluster') def in_split_testing_cluster(cluster_group, page, clusters=CLUSTERS, **kwargs): cluster_group = clusters[cluster_group] lookup_value = getattr(page, 'split_test_id', page.id) return lookup_value in chain(*cluster_group.values()) This condition takes the following arguments: - cluster_group : A string that identifies which group of clusters we want to check - page : An instance of a Wagtail page that we want to look for in the cluster group - clusters : A dictionary containing one or more named cluster_group s \u2013 by default, this points to a dictionary defined in our master cluster definition file (see \"Defining clusters\" below) If the page 's id property is found in the cluster_group , the condition returns True . If the page has a split_test_id property defined, it will check that instead of page.id . (More on that later.) You do not need to modify anything in this file to set up a new experiment.","title":"The flag condition"},{"location":"split-testing/#defining-clusters","text":"To run a split test we first have to know what pages we're testing. Split testing clusters are typically defined in our master cluster definition file at cfgov/core/split_testing_clusters.py . Create a new dictionary with a meaningful name and add the page IDs for the pages in your cluster(s) to it. For example: ASK_CFPB_H1 = { 18: [ 103, 160, 187, 337, 731, 767, 951, 1157, 1161, 1403, 1405, 1439, 1447, 1567, 1695, ], 34: [ 104, 105, 130, 205, 747, 841, 1423, 1791, ], 38: [ 143, 184, 329, 331, 1165, 1637, 1787, 1789, 1797, 1921, 1923, ], 49: [ 136, 161, 163, 164, 172, 176, 179, 180, 181, 188, 192, 336, 817, 1699, 1983, 1989, 1995, 1997, 2001, ], 93: [ 146, 226, 237, 318, 338, 545, 633, 811, 1215, 1463, 1507, ], } In the above example, there are five separate clusters, all of which will be part of the experiment. The number being used for the keys doesn't matter to Python, but it may be useful for internal tracking purposes. As noted earlier, normally the page IDs inside the lists correspond to Wagtail's page ID (most easily findable in the edit URL for a page), but in this particular example, they are actually Ask CFPB answer IDs . For Ask CFPB answer pages, we use the answer ID, because that ID is frequently used in internal communications for shorthand references to Ask answers. All other page types use their Wagtail page ID value. If you have cause to use something else for the lookup values, you can define the split_test_id property in a page class. For example: # cfgov/ask_cfpb/models/pages.py class AnswerPage(CFGOVPage): \u2026 # Overrides the default of page.id for comparing against split testing # clusters. See: core.feature_flags.in_split_testing_cluster @property def split_test_id(self): return self.answer_base.id \u2026 If you do this, remember to add a simple test: # cfgov/ask_cfpb/tests/models/test_pages.py def test_answer_split_testing_id(self): Confirm AnswerPage's split_testing_id is set to its answer_base.id, which is checked by the core.feature_flags.in_split_testing_cluster flag condition when doing split testing on Ask CFPB answer pages. answer = self.answer1234 page = answer.english_page self.assertEqual(page.split_test_id, answer.id) You will also need to add a reference to your new dictionary to the CLUSTERS dictionary at the bottom of the file: CLUSTERS = { 'ASK_CFPB_H1': ASK_CFPB_H1, }","title":"Defining clusters"},{"location":"split-testing/#creating-and-using-the-flag","text":"The new \"in split testing cluster\" condition described earlier can be added to flags and used like any other feature flag. For the example we started above, we first create the flag in our settings, giving it the default condition with a parameter corresponding to the cluster group name we created above: # cfgov/settings/base.py \u2026 # Feature flags # All feature flags must be listed here with a dict of any hard-coded # conditions or an empty dict. If the conditions dict is empty the flag will # only be enabled if database conditions are added. FLAGS = { \u2026 # SPLIT TESTING FLAGS # Ask CFPB page titles as H1s instead of H2s 'ASK_CFPB_H1': { 'in split testing cluster': 'ASK_CFPB_H1' }, \u2026 } \u2026 And then in our template we check the flag, passing in the current page , and if the page's answer ID is found in a cluster, flag_enabled() is True , and we see the h1 instead of the h2 . !-- cfgov/jinja2/v1/ask-cfpb/answer-page.html -- \u2026 {% if flag_enabled('ASK_CFPB_H1', page=page) %} h1 {{ page.question | striptags }} /h1 {% else %} h2 {{ page.question | striptags }} /h2 {% endif %} \u2026 Setting the flag up with the default condition of in_split_testing_cluster means that as soon as the code is deployed, the flag is acting on pages in the cluster. One the results of the test are known, you can set a new boolean flag condition in Wagtail to have the flag always be True , if the test was successful, or always be False , if the test was not successful. This will make (or not make) the change on all pages touched by that flag until you take the time to remove the flag entirely.","title":"Creating and using the flag"},{"location":"translation/","text":"Translation \u00b6 As cfgov-refresh is a Django project, the Django translation documentation is a good place to start . What follows is a brief introduction to translations with the particular tools cfgov-refresh uses (like Jinja2 templates) and the conventions we use. Overview \u00b6 Django translations use GNU gettext (see the installation instructions ). By convention, translations are usually performed in code by wrapping a string to be translated in a function that is either named or aliased with an underscore. For example: _( This is a translatable string. ) These strings are collected into portable object ( .po ) files for each supported language. These files map the original string ( msgid ) to a matching translated string ( msgstr ). For example: msgid This is a translatable string. msgstr Esta es una cadena traducible. These portable object files are compiled into machine object files ( .mo ) that the translation system uses when looking up the original string. By convention the .po and .mo files live inside a locale/[LANGUAGE]/LC_MESSAGES/ folder structure, for example, cfgov/locale/es/LC_MESSAGES/django.po for the Spanish language portable object file for all of our cfgov-refresh messages. How to translate text in cfgov-refresh \u00b6 This brief howto will guide you through adding translatable text to cfgov-refresh. 1. Add the translation function around the string \u00b6 In Jinja2 templates: {{ _('Hello World!') }} In Django templates: {% load i18n %} {% trans Hello World! %} In Python code: from django.utils.translation import ugettext as _ mystring = _('Hello World!') The string in the call to the translation function will be the msgid in the portable object file below. 2. Run the makemessages management command to add the string to the portable object file \u00b6 The makemessages management command will look through all Python, Django, and Jinja2 template files to find strings that are wrapped in a translation function call and add them to the portable object file for a particular language. The language is specified with -l . The command also must be called from the root of the Django app tree, not the project root. To generate or update the portable object file for Spanish: cd cfgov django-admin.py makemessages -l es 3. Edit the portable object file to add a translation for the string \u00b6 The portable object files are stored in cfgov/locale/[LANGUAGE]/LC_MESSAGES/ . For the Spanish portable object file, edit cfgov/locale/es/LC_MESSAGES/django.po and add the Spanish translation as the msgstr for your new msgid msgid Hello World! msgstr Hola Mundo! 4. Run the compilemessages management command to compile the machine object file \u00b6 cd cfgov django-admin.py compilemessages Wagtail Considerations \u00b6 All of our Wagtail pages include a language-selection dropdown under its Configuration tab: The selected language will force translation of all translatable strings in templates and code for that page.","title":"Translation"},{"location":"translation/#translation","text":"As cfgov-refresh is a Django project, the Django translation documentation is a good place to start . What follows is a brief introduction to translations with the particular tools cfgov-refresh uses (like Jinja2 templates) and the conventions we use.","title":"Translation"},{"location":"translation/#overview","text":"Django translations use GNU gettext (see the installation instructions ). By convention, translations are usually performed in code by wrapping a string to be translated in a function that is either named or aliased with an underscore. For example: _( This is a translatable string. ) These strings are collected into portable object ( .po ) files for each supported language. These files map the original string ( msgid ) to a matching translated string ( msgstr ). For example: msgid This is a translatable string. msgstr Esta es una cadena traducible. These portable object files are compiled into machine object files ( .mo ) that the translation system uses when looking up the original string. By convention the .po and .mo files live inside a locale/[LANGUAGE]/LC_MESSAGES/ folder structure, for example, cfgov/locale/es/LC_MESSAGES/django.po for the Spanish language portable object file for all of our cfgov-refresh messages.","title":"Overview"},{"location":"translation/#how-to-translate-text-in-cfgov-refresh","text":"This brief howto will guide you through adding translatable text to cfgov-refresh.","title":"How to translate text in cfgov-refresh"},{"location":"translation/#1-add-the-translation-function-around-the-string","text":"In Jinja2 templates: {{ _('Hello World!') }} In Django templates: {% load i18n %} {% trans Hello World! %} In Python code: from django.utils.translation import ugettext as _ mystring = _('Hello World!') The string in the call to the translation function will be the msgid in the portable object file below.","title":"1. Add the translation function around the string"},{"location":"translation/#2-run-the-makemessages-management-command-to-add-the-string-to-the-portable-object-file","text":"The makemessages management command will look through all Python, Django, and Jinja2 template files to find strings that are wrapped in a translation function call and add them to the portable object file for a particular language. The language is specified with -l . The command also must be called from the root of the Django app tree, not the project root. To generate or update the portable object file for Spanish: cd cfgov django-admin.py makemessages -l es","title":"2. Run the makemessages management command to add the string to the portable object file"},{"location":"translation/#3-edit-the-portable-object-file-to-add-a-translation-for-the-string","text":"The portable object files are stored in cfgov/locale/[LANGUAGE]/LC_MESSAGES/ . For the Spanish portable object file, edit cfgov/locale/es/LC_MESSAGES/django.po and add the Spanish translation as the msgstr for your new msgid msgid Hello World! msgstr Hola Mundo!","title":"3. Edit the portable object file to add a translation for the string"},{"location":"translation/#4-run-the-compilemessages-management-command-to-compile-the-machine-object-file","text":"cd cfgov django-admin.py compilemessages","title":"4. Run the compilemessages management command to compile the machine object file"},{"location":"translation/#wagtail-considerations","text":"All of our Wagtail pages include a language-selection dropdown under its Configuration tab: The selected language will force translation of all translatable strings in templates and code for that page.","title":"Wagtail Considerations"},{"location":"travis/","text":"How we use Travis CI \u00b6 What Travis does \u00b6 We use Travis CI on cfgov-refresh to perform the following tasks: Run automated unit tests and acceptance tests. Deploy this documentation website to GitHub on the gh-pages branch . How Travis is configured \u00b6 We use the following constraints to optimize Travis builds for speed and utility: Travis runs tests on pull requests only, including subsequent pushes to a pull request's branch. Tests are not run on the master branch. Travis deploys documentation on merges and pushes to the master branch only, and not on pull requests. We do not run Travis builds of any kind on any other branches that are not master , and that are not pull request branches. To customize Travis to fit the above constraints, we use a combination of: - Build conditionals and build stages in our .travis.yml file - \"Settings\" in the Travis UI at https://travis-ci.org An extra task for satellite repositories \u00b6 For our satellite apps , Travis is also used to build and attach a deployment wheel file to every release. An example is the .whl file on this release of the retirement app .","title":"How We Use Travis CI"},{"location":"travis/#how-we-use-travis-ci","text":"","title":"How we use Travis CI"},{"location":"travis/#what-travis-does","text":"We use Travis CI on cfgov-refresh to perform the following tasks: Run automated unit tests and acceptance tests. Deploy this documentation website to GitHub on the gh-pages branch .","title":"What Travis does"},{"location":"travis/#how-travis-is-configured","text":"We use the following constraints to optimize Travis builds for speed and utility: Travis runs tests on pull requests only, including subsequent pushes to a pull request's branch. Tests are not run on the master branch. Travis deploys documentation on merges and pushes to the master branch only, and not on pull requests. We do not run Travis builds of any kind on any other branches that are not master , and that are not pull request branches. To customize Travis to fit the above constraints, we use a combination of: - Build conditionals and build stages in our .travis.yml file - \"Settings\" in the Travis UI at https://travis-ci.org","title":"How Travis is configured"},{"location":"travis/#an-extra-task-for-satellite-repositories","text":"For our satellite apps , Travis is also used to build and attach a deployment wheel file to every release. An example is the .whl file on this release of the retirement app .","title":"An extra task for satellite repositories"},{"location":"usage/","text":"Usage: Stand Alone \u00b6 You will generally have three tabs (or windows) open in your terminal, which will be used for: Git operations . Perform Git operations and general development in the repository, such as git checkout master . Elasticsearch . Run an Elasticsearch (ES) instance. See instructions below . Django server . Start and stop the web server. Server is started with ./runserver.sh , but see more details below . What follows are the specific steps for each of these tabs. 1. Git operations \u00b6 From this tab you can do Git operations, such as checking out our master branch: git checkout master Updating all dependencies \u00b6 Each time you fetch from the upstream repository (this repo), run ./setup.sh . This setup script will remove and reinstall the project dependencies and rebuild the site's JavaScript and CSS assets. Note You may also run ./backend.sh or ./frontend.sh if you only want to re-build the backend or front-end, respectively. Setting environments \u00b6 The NODE_ENV environment variable can be set in your .env file to either development or production , which will affect how the build is made and what gulp tasks are available. To install dependencies of one environment or the other run ./frontend.sh (dependencies and devDependencies) or ./frontend.sh production (dependencies but not devDependencies). 2. Run Elasticsearch (optional) \u00b6 Elasticsearch is needed for certain pieces of this project but is not a requirement for basic functionality. If Elasticsearch is installed via Homebrew , you can see instructions for running manually or as a background service using: brew info elasticsearch Typically to run Elasticsearch as a background service you can run: brew services start elasticsearch 3. Launch Site \u00b6 First, move into the cfgov-refresh project directory and ready your environment: # Use the cfgov-refresh virtualenv. workon cfgov-refresh # cd into this directory (if you aren't already there) cd cfgov-refresh From the project root, start the Django server: ./runserver.sh Note If prompted to migrate database changes, stop the server with ctrl + c and run these commands: python cfgov/manage.py migrate ./initial-data.sh ./runserver.sh To view the site browse to: http://localhost:8000 Using a different port If you want to run the server at a port other than 8000 use python cfgov/manage.py runserver port number Specify an alternate port number, e.g. 8001 . To view the Wagtail admin login, browse to http://localhost:8000/admin and login with username admin and password admin (created in initial-data.sh above; note that this password will expire after 60 days). Using HTTPS locally To access a local server using HTTPS use ./runserver.sh ssl You'll need to ignore any browser certificate errors. Available Gulp Tasks \u00b6 There are a number of important gulp tasks, particularly build and test , which will build the project and test it, respectively. Tasks are invoked via an yarn run command so that the local gulp-cli can be used. Using the yarn run gulp -- --tasks command you can view all available tasks. The important ones are listed below: yarn run gulp build # Concatenate, optimize, and copy source files to the production /dist/ directory. yarn run gulp clean # Remove the contents of the production /dist/ directory. yarn run gulp lint # Lint the scripts and build files. yarn run gulp docs # Generate JSDocs from the scripts. yarn run gulp test # Run linting, unit and acceptance tests (see below). yarn run gulp test:unit # Run only unit tests on source code. yarn run gulp test:acceptance # Run only acceptance (in-browser) tests on production code. yarn run gulp audit # Run code quality audits. Usage: Docker \u00b6 Much of the guidance above for the \"stand-alone\" set-up still stands, and it is worth reviewing in full. Here are some things that might be different: docker-compose takes care of running Elasticsearch for you, and all Elasticsearch, Postgres, and Python output will be shown in a single Terminal window or tab. (wherever you run docker-compose up ) manage.py commands can only be run after you've opened up a terminal in the Python container, which you can do with ./shell.sh There is not yet a good way to use SSL/HTTPS, but that is in the works You won't ever need to use backend.sh or runserver.sh How do I... \u00b6 Use Docker Machine \u00b6 If you used mac-virtualbox-init.sh , then we used Docker Machine to create a VirtualBox VM, running the Docker server. Here are some useful docker-machine commands: Start and stop the VM with docker-machine start and docker-machine stop get the current machine IP with docker-machine ip if for some reason you want to start over, docker-machine rm default , and source mac-virtualbox-init.sh To enable Docker and Docker Compose commands, you'll always first need to run this command in any new shell: eval $(docker-machine env) It may be helpful to run docker-machine env by itself, so you understand what's happening. Those variables are what allows docker-compose and the docker command line tool, running natively on your Mac, to connect to the Docker server running inside VirtualBox. If you use autoenv (described in the stand-alone intructions) or something similar, you might consider adding eval $(docker-machine env) to your .env file. You could also achieve the same results (and start the VM if it's not running yet) with source mac-virtualbox-init.sh Any further Docker documentation will assume you are either in a shell where you have already run eval $(docker-machine env) , or you are in an environment where that's not neccessary. Run manage.py commands like migrate, shell, and dbshell, and shell scripts like refresh-data.sh \u00b6 run ./shell.sh to open up a shell inside the Python container. From there, commands like cfgov/manage.py migrate should run as expected. The same goes for scripts like ./refresh-data.sh and ./initial-data.sh \u2013 they will work as expected once you're inside the container. In addition you can run single commands by passing them as arguments to shell.sh , for example: ./shell.sh cfgov/manage.py migrate Use PDB \u00b6 Run ./attach.sh to connect to the TTY session where manage.py runserver is running. If the app is paused at a PDB prompt, this is where you can access it. Handle updates to Python requirements \u00b6 If Compose is running, stop it with CTRL-C. Run: docker-compose build python This will update your Python image. The next time you run docker-compose up , the new requirements will be in place. Set environment variables \u00b6 Environment variables from your .env file are sourced when the python container starts and when you access a running container with ./shell.sh Your shell environment variables, however, are not visible to applications running in Docker. To add new environment variables, simply add them to the .env file, stop compose with ctrl-c, and start it again with docker-compose up . Get familiar with Docker Compose, and our configuration \u00b6 docker-compose.yml contains a sort of \"recipe\" for running the site. Each entry in the Compose file describes a component of our application stack (Postgres, Elasticsearch, and Python), and either points to a public image on Dockerhub, or to a Dockerfile in cfgov-refresh. You can learn a lot more about Compose files in the docs Similarly, a Dockerfile contains instructions for transforming some base image, to one that suits our needs. The Dockerfile sitting in the top level of cfgov-refresh is probably the most interesting. It starts with the public CentOS:7 image , and installs everything else neccessary to run our Python dependencies and the Django app itself. This file will only be executed: the first time you run docker-compose up (or the first time after you re-create the Docker Machine VM) any time you run docker-compose build That's why you need to run docker-compose build after any changes to /requirements/ There are other compose subcommands you might be interested in. Consider learning about build , restarts , logs , ps , top , and the -d option for up . Develop satellite apps \u00b6 Check out any apps you are developing into the develop-apps directory. These will automatically be added to the PYTHONPATH , and apps contained within will be importable from Python running in the container. For example, if your app is called 'foobar', in a repo called foobar-project, you could clone foobar-project in to develop apps: git clone https://github.com/myorg/foobar-project ... which will create a directory at develop-apps/foobar-project. Assuming 'foobar' is at the top-level of 'foobar-project', you should be able to import it from your python code: import foobar runserver has crashed! How do I start it again \u00b6 In a separate terminal window or tab, running docker-compose up python should restart the server.","title":"Usage"},{"location":"usage/#usage-stand-alone","text":"You will generally have three tabs (or windows) open in your terminal, which will be used for: Git operations . Perform Git operations and general development in the repository, such as git checkout master . Elasticsearch . Run an Elasticsearch (ES) instance. See instructions below . Django server . Start and stop the web server. Server is started with ./runserver.sh , but see more details below . What follows are the specific steps for each of these tabs.","title":"Usage: Stand Alone"},{"location":"usage/#1-git-operations","text":"From this tab you can do Git operations, such as checking out our master branch: git checkout master","title":"1. Git operations"},{"location":"usage/#updating-all-dependencies","text":"Each time you fetch from the upstream repository (this repo), run ./setup.sh . This setup script will remove and reinstall the project dependencies and rebuild the site's JavaScript and CSS assets. Note You may also run ./backend.sh or ./frontend.sh if you only want to re-build the backend or front-end, respectively.","title":"Updating all dependencies"},{"location":"usage/#setting-environments","text":"The NODE_ENV environment variable can be set in your .env file to either development or production , which will affect how the build is made and what gulp tasks are available. To install dependencies of one environment or the other run ./frontend.sh (dependencies and devDependencies) or ./frontend.sh production (dependencies but not devDependencies).","title":"Setting environments"},{"location":"usage/#2-run-elasticsearch-optional","text":"Elasticsearch is needed for certain pieces of this project but is not a requirement for basic functionality. If Elasticsearch is installed via Homebrew , you can see instructions for running manually or as a background service using: brew info elasticsearch Typically to run Elasticsearch as a background service you can run: brew services start elasticsearch","title":"2. Run Elasticsearch (optional)"},{"location":"usage/#3-launch-site","text":"First, move into the cfgov-refresh project directory and ready your environment: # Use the cfgov-refresh virtualenv. workon cfgov-refresh # cd into this directory (if you aren't already there) cd cfgov-refresh From the project root, start the Django server: ./runserver.sh Note If prompted to migrate database changes, stop the server with ctrl + c and run these commands: python cfgov/manage.py migrate ./initial-data.sh ./runserver.sh To view the site browse to: http://localhost:8000 Using a different port If you want to run the server at a port other than 8000 use python cfgov/manage.py runserver port number Specify an alternate port number, e.g. 8001 . To view the Wagtail admin login, browse to http://localhost:8000/admin and login with username admin and password admin (created in initial-data.sh above; note that this password will expire after 60 days). Using HTTPS locally To access a local server using HTTPS use ./runserver.sh ssl You'll need to ignore any browser certificate errors.","title":"3. Launch Site"},{"location":"usage/#available-gulp-tasks","text":"There are a number of important gulp tasks, particularly build and test , which will build the project and test it, respectively. Tasks are invoked via an yarn run command so that the local gulp-cli can be used. Using the yarn run gulp -- --tasks command you can view all available tasks. The important ones are listed below: yarn run gulp build # Concatenate, optimize, and copy source files to the production /dist/ directory. yarn run gulp clean # Remove the contents of the production /dist/ directory. yarn run gulp lint # Lint the scripts and build files. yarn run gulp docs # Generate JSDocs from the scripts. yarn run gulp test # Run linting, unit and acceptance tests (see below). yarn run gulp test:unit # Run only unit tests on source code. yarn run gulp test:acceptance # Run only acceptance (in-browser) tests on production code. yarn run gulp audit # Run code quality audits.","title":"Available Gulp Tasks"},{"location":"usage/#usage-docker","text":"Much of the guidance above for the \"stand-alone\" set-up still stands, and it is worth reviewing in full. Here are some things that might be different: docker-compose takes care of running Elasticsearch for you, and all Elasticsearch, Postgres, and Python output will be shown in a single Terminal window or tab. (wherever you run docker-compose up ) manage.py commands can only be run after you've opened up a terminal in the Python container, which you can do with ./shell.sh There is not yet a good way to use SSL/HTTPS, but that is in the works You won't ever need to use backend.sh or runserver.sh","title":"Usage: Docker"},{"location":"usage/#how-do-i","text":"","title":"How do I..."},{"location":"usage/#use-docker-machine","text":"If you used mac-virtualbox-init.sh , then we used Docker Machine to create a VirtualBox VM, running the Docker server. Here are some useful docker-machine commands: Start and stop the VM with docker-machine start and docker-machine stop get the current machine IP with docker-machine ip if for some reason you want to start over, docker-machine rm default , and source mac-virtualbox-init.sh To enable Docker and Docker Compose commands, you'll always first need to run this command in any new shell: eval $(docker-machine env) It may be helpful to run docker-machine env by itself, so you understand what's happening. Those variables are what allows docker-compose and the docker command line tool, running natively on your Mac, to connect to the Docker server running inside VirtualBox. If you use autoenv (described in the stand-alone intructions) or something similar, you might consider adding eval $(docker-machine env) to your .env file. You could also achieve the same results (and start the VM if it's not running yet) with source mac-virtualbox-init.sh Any further Docker documentation will assume you are either in a shell where you have already run eval $(docker-machine env) , or you are in an environment where that's not neccessary.","title":"Use Docker Machine"},{"location":"usage/#run-managepy-commands-like-migrate-shell-and-dbshell-and-shell-scripts-like-refresh-datash","text":"run ./shell.sh to open up a shell inside the Python container. From there, commands like cfgov/manage.py migrate should run as expected. The same goes for scripts like ./refresh-data.sh and ./initial-data.sh \u2013 they will work as expected once you're inside the container. In addition you can run single commands by passing them as arguments to shell.sh , for example: ./shell.sh cfgov/manage.py migrate","title":"Run manage.py commands like migrate, shell, and dbshell, and shell scripts like refresh-data.sh"},{"location":"usage/#use-pdb","text":"Run ./attach.sh to connect to the TTY session where manage.py runserver is running. If the app is paused at a PDB prompt, this is where you can access it.","title":"Use PDB"},{"location":"usage/#handle-updates-to-python-requirements","text":"If Compose is running, stop it with CTRL-C. Run: docker-compose build python This will update your Python image. The next time you run docker-compose up , the new requirements will be in place.","title":"Handle updates to Python requirements"},{"location":"usage/#set-environment-variables","text":"Environment variables from your .env file are sourced when the python container starts and when you access a running container with ./shell.sh Your shell environment variables, however, are not visible to applications running in Docker. To add new environment variables, simply add them to the .env file, stop compose with ctrl-c, and start it again with docker-compose up .","title":"Set environment variables"},{"location":"usage/#get-familiar-with-docker-compose-and-our-configuration","text":"docker-compose.yml contains a sort of \"recipe\" for running the site. Each entry in the Compose file describes a component of our application stack (Postgres, Elasticsearch, and Python), and either points to a public image on Dockerhub, or to a Dockerfile in cfgov-refresh. You can learn a lot more about Compose files in the docs Similarly, a Dockerfile contains instructions for transforming some base image, to one that suits our needs. The Dockerfile sitting in the top level of cfgov-refresh is probably the most interesting. It starts with the public CentOS:7 image , and installs everything else neccessary to run our Python dependencies and the Django app itself. This file will only be executed: the first time you run docker-compose up (or the first time after you re-create the Docker Machine VM) any time you run docker-compose build That's why you need to run docker-compose build after any changes to /requirements/ There are other compose subcommands you might be interested in. Consider learning about build , restarts , logs , ps , top , and the -d option for up .","title":"Get familiar with Docker Compose, and our configuration"},{"location":"usage/#develop-satellite-apps","text":"Check out any apps you are developing into the develop-apps directory. These will automatically be added to the PYTHONPATH , and apps contained within will be importable from Python running in the container. For example, if your app is called 'foobar', in a repo called foobar-project, you could clone foobar-project in to develop apps: git clone https://github.com/myorg/foobar-project ... which will create a directory at develop-apps/foobar-project. Assuming 'foobar' is at the top-level of 'foobar-project', you should be able to import it from your python code: import foobar","title":"Develop satellite apps"},{"location":"usage/#runserver-has-crashed-how-do-i-start-it-again","text":"In a separate terminal window or tab, running docker-compose up python should restart the server.","title":"runserver has crashed! How do I start it again"},{"location":"wagtail-pages/","text":"Wagtail pages \u00b6 Wagtail pages are Django models that are constructed of fields , StreamFields , and panels that are rendered in a standard way. All CFPB Wagtail pages should inherit from the v1.models.base.CFGOVPage class . Note Before creating a new Wagtail page type please consider whether one of our existing page types can meet your needs. Talk to the consumerfinance.gov product owners if your content is significantly different from anything else on the site or a specific maintenance efficiency will be gained from a new page type. There are types of information defined on a new Wagtail page model: basic database fields (like any Django model), specialized database fields called StreamFields that allow for freeform page content, and editor panels that present these fields to content editors. Fields \u00b6 Database fields in Wagtail pages work exactly the same as in Django models , and Wagtail pages can use any Django model field . For example, our BrowsePage includes a standard Django BooleanField that allows content editors to toggle secondary navigation sibling pages: from django.db import models from v1.models.base import CFGOVPage class BrowsePage(CFGOVPage): secondary_nav_exclude_sibling_pages = models.BooleanField(default=False) StreamFields \u00b6 StreamFields are special Django model fields provided by Wagtail for freeform page content . They allow a content editor to pick any number number of optional components and place them in any order within their StreamField. In practice, this provides the flexibility of a large rich text field, with the structure of individual components . For example, our LandingPage page model includes a header StreamField that can have a hero and/or a text introduction: from wagtail.wagtailcore.fields import StreamField from v1.atomic_elements import molecules from v1.models import CFGOVPage class LandingPage(CFGOVPage): header = StreamField([ ('hero', molecules.Hero()), ('text_introduction', molecules.TextIntroduction()), ], blank=True) The specifics of StreamField block components can be found in Creating and Editing Wagtail Components . Panels \u00b6 Editor panels define how the page's fields and StreamFields will be organized for content editors; they correspond to the tabs that appear across the top of the edit view for a page in the Wagtail admin. The base Wagtail Page class and the CFGOVPage subclass of it define specific sets of panels to which all fields should be added: content_panels : For page body content. These fields appear on the \"General Content\" tab when editing a page. sidefoot_panels : For page sidebar or footer content. These fields appear on the \"Sidebar\" tab when editing a page. settings_panels : Page configuration such as the categories, tags, scheduled publishing, etc. Appears on the \"Configuration\" tab when editing a page. Most fields will simply require a FieldPanel to be added to one of the sets of panels above. StreamFields will require a StreamFieldPanel . See the Wagtail documentation for additional, more complex panel options . For example, in our BrowsePage (used in the database fields example above ), the secondary_nav_exclude_sibling_pages BooleanField is added to the sidefoot_panels as a FieldPanel : from django.db import models from wagtail.wagtailadmin.edit_handlers import FieldPanel from v1.models.base import CFGOVPage class BrowsePage(CFGOVPage): secondary_nav_exclude_sibling_pages = models.BooleanField(default=False) # \u2026 sidefoot_panels = CFGOVPage.sidefoot_panels + [ FieldPanel('secondary_nav_exclude_sibling_pages'), ] Because secondary_nav_exclude_sibling_pages is a boolean field, this creates a checkbox on the \"Sidebar/Footer\" tab when editing a page. Checking or unchecking that checkbox will set the value of secondary_nav_exclude_sibling_pages when the page is saved. In our LandingPage (used in the StreamFields example above ), the header StreamField is added to the content_panels as a StreamFieldPanel : from wagtail.wagtailadmin.edit_handlers import StreamFieldPanel from wagtail.wagtailcore.fields import StreamField from v1.atomic_elements import molecules from v1.models import CFGOVPage class LandingPage(CFGOVPage): header = StreamField([ ('hero', molecules.Hero()), ('text_introduction', molecules.TextIntroduction()), ], blank=True) # \u2026 content_panels = CFGOVPage.content_panels + [ StreamFieldPanel('header'), # \u2026 ] Parent / child page relationships \u00b6 Wagtail provides two attributes to page models that enable restricting the types of subpages or parent pages a particular page model can have. On any page model: parent_page_types limits which page types this type can be created under. subpage_types limits which page types can be created under this type. For example, in our interactive regulations page models we have a RegulationLandingPage that can be created anywhere in the page tree. RegulationLandingPage , however, can only have two types of pages created within it: RegulationPage and RegulationSearchPage . This parent/child relationship is expressed by setting subpage_types on RegulationLandingPage and parent_page_types on RegulationPage and RegulationSearchPage to a model name in the form 'app_label.ModelName': from v1.models import CFGOVPage class RegulationLandingPage(CFGOVPage): subpage_types = ['regulations3k.RegulationPage', 'regulations3k.RegulationsSearchPage'] class RegulationsSearchPage(CFGOVPage): parent_page_types = ['regulations3k.RegulationLandingPage'] subpage_types = [] class RegulationPage(CFGOVPage): parent_page_types = ['regulations3k.RegulationLandingPage'] subpage_types = [] Note We prevent child pages from being added to RegulationPage and RegulationSearchPage by setting subpage_types to an empty list. Template rendering \u00b6 New Wagtail page types will usually need to make customizations to their base template when rendering the page . This is done by overriding the template attribute on the page model. For example, the interactive regulations landing page includes a customized list of recently issued notices that gets loaded dynamically from the Federal Register. To do this it provides its own template that inherits from our base templates and overrides the content_sidebar block to include a seperate recent_notices template: from v1.models import CFGOVPage class RegulationLandingPage(CFGOVPage): template = 'regulations3k/landing-page.html' And in regulations3k/landing-page.html : {% extends 'layout-2-1-bleedbar.html' %} {% import 'recent-notices.html' as recent_notices with context %} {% block content_sidebar scoped -%} {{ recent_notices }} {%- endblock %}","title":"Wagtail Pages"},{"location":"wagtail-pages/#wagtail-pages","text":"Wagtail pages are Django models that are constructed of fields , StreamFields , and panels that are rendered in a standard way. All CFPB Wagtail pages should inherit from the v1.models.base.CFGOVPage class . Note Before creating a new Wagtail page type please consider whether one of our existing page types can meet your needs. Talk to the consumerfinance.gov product owners if your content is significantly different from anything else on the site or a specific maintenance efficiency will be gained from a new page type. There are types of information defined on a new Wagtail page model: basic database fields (like any Django model), specialized database fields called StreamFields that allow for freeform page content, and editor panels that present these fields to content editors.","title":"Wagtail pages"},{"location":"wagtail-pages/#fields","text":"Database fields in Wagtail pages work exactly the same as in Django models , and Wagtail pages can use any Django model field . For example, our BrowsePage includes a standard Django BooleanField that allows content editors to toggle secondary navigation sibling pages: from django.db import models from v1.models.base import CFGOVPage class BrowsePage(CFGOVPage): secondary_nav_exclude_sibling_pages = models.BooleanField(default=False)","title":"Fields"},{"location":"wagtail-pages/#streamfields","text":"StreamFields are special Django model fields provided by Wagtail for freeform page content . They allow a content editor to pick any number number of optional components and place them in any order within their StreamField. In practice, this provides the flexibility of a large rich text field, with the structure of individual components . For example, our LandingPage page model includes a header StreamField that can have a hero and/or a text introduction: from wagtail.wagtailcore.fields import StreamField from v1.atomic_elements import molecules from v1.models import CFGOVPage class LandingPage(CFGOVPage): header = StreamField([ ('hero', molecules.Hero()), ('text_introduction', molecules.TextIntroduction()), ], blank=True) The specifics of StreamField block components can be found in Creating and Editing Wagtail Components .","title":"StreamFields"},{"location":"wagtail-pages/#panels","text":"Editor panels define how the page's fields and StreamFields will be organized for content editors; they correspond to the tabs that appear across the top of the edit view for a page in the Wagtail admin. The base Wagtail Page class and the CFGOVPage subclass of it define specific sets of panels to which all fields should be added: content_panels : For page body content. These fields appear on the \"General Content\" tab when editing a page. sidefoot_panels : For page sidebar or footer content. These fields appear on the \"Sidebar\" tab when editing a page. settings_panels : Page configuration such as the categories, tags, scheduled publishing, etc. Appears on the \"Configuration\" tab when editing a page. Most fields will simply require a FieldPanel to be added to one of the sets of panels above. StreamFields will require a StreamFieldPanel . See the Wagtail documentation for additional, more complex panel options . For example, in our BrowsePage (used in the database fields example above ), the secondary_nav_exclude_sibling_pages BooleanField is added to the sidefoot_panels as a FieldPanel : from django.db import models from wagtail.wagtailadmin.edit_handlers import FieldPanel from v1.models.base import CFGOVPage class BrowsePage(CFGOVPage): secondary_nav_exclude_sibling_pages = models.BooleanField(default=False) # \u2026 sidefoot_panels = CFGOVPage.sidefoot_panels + [ FieldPanel('secondary_nav_exclude_sibling_pages'), ] Because secondary_nav_exclude_sibling_pages is a boolean field, this creates a checkbox on the \"Sidebar/Footer\" tab when editing a page. Checking or unchecking that checkbox will set the value of secondary_nav_exclude_sibling_pages when the page is saved. In our LandingPage (used in the StreamFields example above ), the header StreamField is added to the content_panels as a StreamFieldPanel : from wagtail.wagtailadmin.edit_handlers import StreamFieldPanel from wagtail.wagtailcore.fields import StreamField from v1.atomic_elements import molecules from v1.models import CFGOVPage class LandingPage(CFGOVPage): header = StreamField([ ('hero', molecules.Hero()), ('text_introduction', molecules.TextIntroduction()), ], blank=True) # \u2026 content_panels = CFGOVPage.content_panels + [ StreamFieldPanel('header'), # \u2026 ]","title":"Panels"},{"location":"wagtail-pages/#parent-child-page-relationships","text":"Wagtail provides two attributes to page models that enable restricting the types of subpages or parent pages a particular page model can have. On any page model: parent_page_types limits which page types this type can be created under. subpage_types limits which page types can be created under this type. For example, in our interactive regulations page models we have a RegulationLandingPage that can be created anywhere in the page tree. RegulationLandingPage , however, can only have two types of pages created within it: RegulationPage and RegulationSearchPage . This parent/child relationship is expressed by setting subpage_types on RegulationLandingPage and parent_page_types on RegulationPage and RegulationSearchPage to a model name in the form 'app_label.ModelName': from v1.models import CFGOVPage class RegulationLandingPage(CFGOVPage): subpage_types = ['regulations3k.RegulationPage', 'regulations3k.RegulationsSearchPage'] class RegulationsSearchPage(CFGOVPage): parent_page_types = ['regulations3k.RegulationLandingPage'] subpage_types = [] class RegulationPage(CFGOVPage): parent_page_types = ['regulations3k.RegulationLandingPage'] subpage_types = [] Note We prevent child pages from being added to RegulationPage and RegulationSearchPage by setting subpage_types to an empty list.","title":"Parent / child page relationships"},{"location":"wagtail-pages/#template-rendering","text":"New Wagtail page types will usually need to make customizations to their base template when rendering the page . This is done by overriding the template attribute on the page model. For example, the interactive regulations landing page includes a customized list of recently issued notices that gets loaded dynamically from the Federal Register. To do this it provides its own template that inherits from our base templates and overrides the content_sidebar block to include a seperate recent_notices template: from v1.models import CFGOVPage class RegulationLandingPage(CFGOVPage): template = 'regulations3k/landing-page.html' And in regulations3k/landing-page.html : {% extends 'layout-2-1-bleedbar.html' %} {% import 'recent-notices.html' as recent_notices with context %} {% block content_sidebar scoped -%} {{ recent_notices }} {%- endblock %}","title":"Template rendering"}]}